[
{
	"uri": "https://filipecn.github.io/aprendaopengl/introducao/",
	"title": "Introdução",
	"tags": [],
	"description": "",
	"content": "Post Original\nJá que veio aqui então você provavelmente quer aprender como Computação Gráfica funciona por debaixo dos panos e fazer sozinho todas as coisas super legais que a galera faz. Fazer as coisas por você mesmo é extremamente divertido, engenhoso e te dá um ótimo entendimento de programação gráfica. Entretanto, têm alguns itens que precisam ser considerados antes de começar sua grande jornada.\nPré-requisitos Já que a OpenGL é uma API (biblioteca) gráfica e não uma plataforma por si só, ela requer uma linguagem de programação para que possa operar e a linguagem escolhida é C++. Portanto um conhecimento razoável de C++ é necessário para os capítulos a seguir. Tentarei explicar quase todos os conceitos utilizados, incluindo tópicos avançados de C++ então não é preciso ser nenhum expert em C++, porém você deveria ser capaz de escrever mais do que um programa \u0026ldquo;Hello World\u0026rdquo;. Caso não tenha muita experiencia com C++ eu recomendo alguns tutoriais de graça.\nAlém disso, vamos utilizar um pouco de matemática (algebra linear, geometria e trigonometria) durante o caminho e tentarei explicar todos conceitos necessários. Entretanto, não sou um matemático então, mesmo que minhas explicações sejam fáceis de entender, elas provavelmente serão incompletas. Apontareis boas fontes que explicam o material de forma mais completa. Não se assuste sobre o conhecimento matemático necessário antes de começar sua jornada com a OpenGL; quase todos conceitos podem ser entendidos com um conhecimento básico de matemática e tentarei manter a matemática num nível mínimo a medida que for possível. A maior parte da funcionalidade nem sequer exige que você entenda toda matemática contando que você saiba como usá-la.\nEstrutura O AprendaOpenGL é dividido vários capítulos gerais. Cada seção contem várias seções das quais cada uma explica diferentes conceitos em maiores detalhes. Cada uma das seções pode ser encontrada no menu a sua esquerda. Os conceitos são ensinados de forma linear (então é recomendado começar do topo até o final, a menos que seja instruído o contrário), onde cada seção explica a base teórica e os aspectos práticos.\nA fim de tornar os conceitos mais fáceis de seguir, e estruturá-los um pouco mais, o texto contém caixas, blocos de código, dicas coloridas e referência de funções.\nCaixas  Caixas verdes contém algumas notas ou dicas úteis sobre OpenGL ou sobre o assunto sendo discutido.\n Caixas vermelhas contém avisos ou outras informações das quais você deve tomar um cuidado extra.\n Código Você encontrará vários pedacinhos de código neste website que estão localizados em caixas escuras contendo código com sintaxe colorida, como pode ver abaixo:\n// esta caixa contem código Como exibem apenas pedaços de código, sempre que necessário oferecerei um link para o código fonte inteiro respectivo ao seu assunto.\nColor hints Algumas palavras são mostradas com uma cor diferente para deixar mais claro que carregam um significado especial:\n Definição  : palavras verdes especificam uma definição, isto é, um aspecto/nome importante de alguma coisa que você provavelmente encontrará mais vezes. Estrutura  de  Programa  : palavras vermelhas especificam nomes de funções ou classes. Variáveis : palavras azuis especificam variáveis incluindo todas constantes da OpenGL.  Referencias de Funções OpenGL Uma característica particularmente apreciada do LearnOpenGL é a habilidade de visitar a maioria das funções OpenGL toda vez que aparecem no texto. Toda vez que uma função é encontrada no conteúdo que é documentado no website, a função irá aparecer sublinhada. Você pode passar o mouse em cima e depois de um breve intervalo, uma janela irá mostrar informações relevantes sobre esta função incluindo um bom resumo sobre o que a função de fato faz. Passe seu mouse em cima de glEnable  para ver a mágica acontecer.\nEsse site não tem tanta magia quanto o original (nenhuma pra dizer a verdade), mas quem sabe um dia né :). Por enquanto colocarei um link para documentação oficial no lugar. Desse jeito: glEnable.\n Agora que você sentiu um pouco da estrutura do site, pule para o próximo capitulo e comece sua grande jornada em OpengGL!\n"
},
{
	"uri": "https://filipecn.github.io/aprendaopengl/ponto_de_partida/",
	"title": "Ponto de Partida",
	"tags": [],
	"description": "",
	"content": "Capítulo 2 Ponto de Partida "
},
{
	"uri": "https://filipecn.github.io/aprendaopengl/iluminacao/",
	"title": "Iluminação",
	"tags": [],
	"description": "",
	"content": "Capítulo 3 Iluminação "
},
{
	"uri": "https://filipecn.github.io/aprendaopengl/iluminacao_avancada/sombras/",
	"title": "Sombras",
	"tags": [],
	"description": "",
	"content": "Chapter X Some Chapter title Lorem Ipsum.\n"
},
{
	"uri": "https://filipecn.github.io/aprendaopengl/ponto_de_partida/opengl/",
	"title": "OpenGL",
	"tags": [],
	"description": "",
	"content": "Post Original\nAntes de começar nossa jornada devemos primeiro definir o que de fato é a OpenGL. A OpenGL é sobretudo considerada uma API (Application  Programming  Interface  ) que provê uma variedade de funções que podemos usar para manipular gráficos e imagens. Porém, a OpenGL por si só não é uma API, mas meramente uma especificação, desenvolvida e mantida pelo Grupo Khronos.\nA especificação da OpenGL define exatamente o que o resultado/saída de cada função deveria ser e como deveria funcionar. Fica então para os desenvolvedores implementar essa especificação e pensar em uma solução em como as funções deveria operar. Como a especificação da OpenGL não nos dá os detalhes de implementação, as versões da OpenGL que são de fato desenvolvidas têm permissão de ter diferentes implementações, desde que seus resultados concordem com a especificação (e são portanto os mesmos para o usuário).\nAs pessoas que desenvolvem as bibliotecas da OpenGL são normalmente os fabricantes de placas gráficas. Cada placa gráfica que você compra suporta versões específicas da OpenGL, que são desenvolvidas especificamente para aquela (série) placa. Quando se usa um sistema Apple a biblioteca da OpenGL é mantida pela própria Apple e sob o Linux existe uma combinação de fornecedores de versões adaptações de hobistas dessas bibliotecas. Isso significa que sempre que a OpenGL mostrar um comportamento estranho, provavelmente a culpa é dos fabricantes de placas gráficas (ou quem quer que tenha desenvolvido/mantido a biblioteca).\nJá que a maioria das implementações é feita por fabricantes de placas gráficas, sempre que existir um erro de implementação o problema é normalmente resolvido ao atualizar os drivers da placa de vídeo; esses drivers incluem as versões mais novas da OpenGL que a placa suporta. Esta é uma das razões pela qual é recomendável ocasionalmente atualizar os drivers gráficos.\n O Khronos hospeda publicamente todos documentos com a especificação de todas versões da OpenGL. O leitor interessado pode encontrar as especificações da versão 3.3 da OpenGL (que é a versão que vamos utilizar) aqui, que é uma boa leitura se você quiser mergulhar nos detalhes da OpenGL (note como eles descrevem mais os resultados do que as implementações). As especificações também dão uma boa referencia para encontrar o funcionamento exato das suas funções.\nCore-profile vs Immediate mode (modo imediato) Antigamente, usar a OpenGL significava desenvolver em modo  imediato  (normalmente referido como o pipeline de funções fixas (the fixed  function  pipeline  )), o qual era um método fácil-de-usar para desenhar gráficos. A maior parte da funcionalidade da OpenGL era escondida dentro da biblioteca e os desenvolvedores não tinham muito controle sobre como a OpenGL faz seus cálculos. Os desenvolvedores eventualmente ficaram famintos por mais flexibilidade e com o tempo as especificações se tornaram mais flexíveis como resultado; os desenvolvedores ganharam mais controle sobre seus gráficos. O modo imediato é realmente fácil de usar e entender, mas é extremamente ineficiente. Por essa razão a especificação começou a descontinuar a funcionalidade de modo imediato da versão 3.2 em diante e começou a motivar os programadores a desenvolver no modo core-profile  da OpenGL, que é uma divisão da especificação da OpenGL que removeu toda funcionalidade antiga e descontinuada.\nQuando se usa o core-profile da OpenGL, a OpenGL nos força a seguir praticas modernas. Sempre que tentarmos utilizar uma função descontinuada, a OpenGL cospe um erro e para de desenhar. A vantagem de aprender a abordagem moderna é que ela é muito flexível e eficiente. Porém, é também mais difícil de aprender. O modo imediato abstraiu demais muitas das operações executadas pela OpenGL e, enquanto era fácil de aprender, era mais difícil de entender como a OpenGL realmente funciona. A abordagem moderna requer que o programador verdadeiramente entenda a OpenGL, e enquanto isso é meio difícil, permite muito mais flexibilidade, mais eficiência e o mais importante: um entendimento muito melhor de programação de gráficos.\nEssa é também a razão do porque esse livro é engrenado no core-profile da versão 3.3 da OpenGL. Embora seja mais difícil, vale muito a pena o esforço.\nAtualmente, versões mais altas da OpenGL estão disponíveis sobre as quais você pode perguntar: porque eu quero aprender OpenGL 3.3 se existe a 4.6? A resposta para esta questão é relativamente simples. Todas versões posteriores a 3.3 adicionam funcionalidades extras a OpenGL sem mudar a mecânica do núcleo da OpenGL; as versões mais novas introduzem modos mais eficientes ou mais úteis para completar as mesmas tarefas. O resultado é que todos conceitos e técnicas permanecem os mesmos pelas versões da OpenGL moderna, então é perfeitamente válido aprender a OpenGL 3.3. Sempre que estiver pronto e/ou mais experiente, você pode facilmente usar uma funcionalidade especifica de versões da OpenGL mais recentes.\nQuando se utiliza a versão mais recente da OpenGL, apenas as placas gráficas mais modernas terão capacidade de rodar sua aplicação. Por isso a maioria dos desenvolvedores geralmente visa versões mais baixas da OpenGL e opcionalmente habilitam funcionalidades de versões mais altas.\n Em alguns capítulos você encontrará recursos mais modernos que são anotados como tais.\nExtensões Um recurso muito bom da OpenGL é seu suporte a extensões. Sempre que uma empresa de gráficos aparece com uma técnica nova ou uma otimização importante para renderização, estas novidades são normalmente encontradas em uma extensão  implementada nos drivers. Se o hardware suporta uma extensão em particular, o programador pode usar a sua funcionalidade em busca de aplicações gráficas mais avançadas e eficientes. Deste modo, um desenvolvedor de gráficos pode ainda usar estas técnicas novas de renderização sem ter que esperar pela inclusão da funcionalidade nas versões futuras da OpenGL, tendo apenas que checar se a extensão é suportada pela placa gráfica. Frequentemente, quando uma extensão é popular ou muito útil ela eventualmente é incorporada a versões futuras da OpenGL.\nO programador deve inquerir se algumas dessas extensões estáo disponíveis antes de utilizá-las (ou usar uma biblioteca de extensão da OpenGL). Isto permite que o programador faça coisas de um jeito melhor ou mais eficiente, baseado se uma extensão pode ser usada:\nif(GL_ARB_extension_name) { // Fazer coisas maneiras e modernas suportadas pelo hardware } else { // Extensao nao eh suportada: fazer isso do jeito antigo } Com a versão 3.3 da OpenGL nós raramente precisamos de uma extensão para maioria das técnicas, mas sempre que for necessário instruções serão dadas.\nMáquina de Estados A OpenGL é uma grande máquina de estados: uma coleção de variáveis que definem como a OpenGL deveria se comportar. O estado da OpenGL é comumente referido como o contexto da OpenGL. Quando utilizamos a OpenGL, normalmente mudamos seu estado através da configuração de algumas opções, manipulação de alguns buffers e então renderizamos o contexto corrente.\nSempre que dizemos a OpenGL que agora queremos desenhar linhas ao invés de triângulos por exemplo, mudamos o estado da OpenGL através de algumas variáveis de contexto que definem como a OpenGL deveria desenhar. Assim que mudamos o contexto ao dizer a OpenGL que esta deveria desenhar linhas, os próximos comandos de desenho irão desenhar linhas ao invés de triângulos.\nAo trabalhar com a OpenGL iremos esbarrar com várias funções de mudança de estado que mudam o contexto e várias funções que dependem do estado corrente da OpenGL. Contando que você mantenha na sua mente que a OpenGL é basicamente uma grande máquina de estados, a maioria de suas funcionalidades farão mais sentido.\nObjetos As bibliotecas da OpenGL são escritas em C e permitem muitas derivações em outras linguagens, mas no seu núcleo ela permanece uma biblioteca em C. Como várias construções da linguagem C não traduzem bem para outras linguagens de alto nível, a OpenGL foi desenvolvida com uma variedade de abstrações em mente. Uma dessas abstrações são os objetos  em OpenGL.\nUm objeto na OpenGL é uma coleção de opções que representam um subconjunto do estado da OpenGL. Por exemplo, poderíamos ter um objeto que representa as configurações da janela de desenho; nós poderíamos então definir seu tamanho, quantas cores ela suporta e assim em diante. Alguém poderia visualizar um objeto como uma estrutura do C:\nstruct object_name { float option1; int option2; char[] name; };} Sempre que quisermos usar objetos, geralmente vão se parecer com isso (com o contexto da OpenGL visualizado como uma grande estrutura):\n// The State of OpenGL struct OpenGL_Context { ... object_name* object_Window_Target; ... }; // create object unsigned int objectId = 0; glGenObject(1, \u0026amp;objectId); // bind/assign object to context glBindObject(GL_WINDOW_TARGET, objectId); // set options of object currently bound to GL_WINDOW_TARGET glSetObjectOption(GL_WINDOW_TARGET, GL_OPTION_WINDOW_WIDTH, 800); glSetObjectOption(GL_WINDOW_TARGET, GL_OPTION_WINDOW_HEIGHT, 600); // set context target back to default glBindObject(GL_WINDOW_TARGET, 0); Este pedacinho de código é uma rotina que você irá encontrar frequentemente quando estiver trabalhando com a OpenGL. Nós primeiro criamos um objeto e armazenamos uma referencia para ele como um id (o dado real do objeto é armazenado nos bastidores). Então nós ligamos (bind) o objeto (usando seu id) ao uma localização alvo do contexto (a localização do exemplo é definida como GL_WINDOW_TARGET ). Em seguida, definimos as opções da janela e finalmente desconectamos o objeto ao setar o id do objeto corrente da janela alvo para 0. As opções que definimos são armazenadas no objeto referenciado por objectId e restauradas assim que ligamos o objeto de volta a GL_WINDOW_TARGET .\nOs códigos mostrados até agora são apenas aproximações de como a OpenGL opera; ao longo deste livro você encontrará exemplos de verdade o suficiente.\n O lado bom de utilizar esses objetos é que podemos definir mais de um objeto em nossa aplicação, definir suas opções e sempre que começarmos uma operação que usa o estado da OpenGL, podemos conectar o objeto com nossas configurações. Existem objetos por exemplo, que agem como objetos recipientes para dados de modelos 3D (uma casa ou um personagem) e sempre que quisermos desenhar um deles, nós ligamos o objeto contendo os dados do modelo que queremos desenhar (dado que primeiramente criamos e setamos suas opções). Ter vários objetos nos permite especificar vários modelos e sempre que quisermos desenhar um modelo específico, basta conectar o objeto correspondente antes de desenhar sem ter que configurar todas opções de novo.\nVamos começar Você aprendeu um pouco sobre a OpenGL como uma especificação e uma biblioteca, como aproximadamente a OpenGL opera por debaixo dos panos e alguns truques customizados que a OpenGL utiliza. Não se preocupe se não entendeu tudo; ao longo do livro vamos passar por cada passo e você verá exemplos o suficientes para realmente pegar o jeito da OpenGL.\nRecursos Adicionais  opengl.org: website oficial da OpenGL OpenGL registry: hospeda as especificações e extensões de todas versões da OpenGL.  "
},
{
	"uri": "https://filipecn.github.io/aprendaopengl/iluminacao_avancada/",
	"title": "Iluminação Avançada",
	"tags": [],
	"description": "",
	"content": "Capítulo 6 Iluminação Avançada Lorem Ipsum.\n"
},
{
	"uri": "https://filipecn.github.io/aprendaopengl/pbr/",
	"title": "PBR",
	"tags": [],
	"description": "",
	"content": "Capítulo 7 PBR "
},
{
	"uri": "https://filipecn.github.io/aprendaopengl/iluminacao_avancada/iluminacao_avancada/",
	"title": "Iluminação Avançada",
	"tags": [],
	"description": "",
	"content": "Nos capítulos de iluminação, apresentamos brevemente o modelo de iluminação Phong para trazer um mínimo de realismo às nossas cenas. O modelo Phong parece bom, mas tem algumas nuances nas quais nos concentraremos neste capítulo.\nBlinn-Phong A iluminação Phong é uma aproximação excelente e muito eficiente da iluminação, mas seus reflexos especulares falham em certas condições, especificamente quando a propriedade de brilho é baixa, resultando em uma grande área especular (áspera). A imagem abaixo mostra o que acontece quando usamos um expoente de brilho especular de 1.0 em um plano com textura:\n\nVocê pode ver nas bordas que a área especular é imediatamente cortada. Isso acontece porque o ângulo entre o vetor da câmera e o vetor de reflexão não ultrapassa 90 graus. Se o ângulo for maior que 90 graus, o produto escalar resultante torna-se negativo e isso resulta em um expoente especular de 0.0. Você provavelmente está pensando que isso não será um problema, já que não devemos obter nenhuma luz com ângulos superiores a 90 graus, certo?\nErrado, isso só se aplica a componente difusa, onde um ângulo superior a 90 graus entre a normal da superfície e a fonte de luz significa que a fonte de luz está abaixo da superfície iluminada e, portanto, a contribuição difusa da luz deve ser igual a 0.0. No entanto, com a iluminação especular, não medimos o ângulo entre a fonte de luz e a normal da superfície, mas entre o vetor da câmera e o vetor de reflexão. Dê uma olhada nas duas imagens a seguir:\n\nAqui, o problema deve se tornar aparente. A imagem à esquerda mostra os reflexos Phong como de costume, com $\\theta$ sendo inferior a 90 graus. Na imagem da direita podemos ver que o ângulo $\\theta$ entre o vetor da câmera e o vetor de reflexão é maior do que 90 graus, o que anula a contribuição especular. Isso geralmente não é um problema, uma vez que a direção da câmera está longe da direção da reflexão, mas se usarmos um expoente especular baixo, o raio especular é grande o suficiente para ter uma contribuição nessas condições. Como anulamos essa contribuição em ângulos maiores do que 90 graus, obtemos o artefato conforme visto na primeira imagem.\nEm 1977, o modelo de shading Blinn-Phong  foi introduzido por James F. Blinn como uma extensão do shading de Phong que usamos até agora. O modelo Blinn-Phong é muito semelhante, mas se aproxima do modelo especular um pouco diferente de tal forma que resolve nosso problema. Em vez de depender de um vetor de reflexão, estamos usando o chamado vetor intermediário  ( halfway vector  ), que é um vetor unitário exatamente no meio do caminho entre a direção da câmera e a direção da luz. Quanto mais próximo este vetor intermediário estiver alinhado com o vetor normal da superfície, maior será a contribuição especular.\n\nQuando a direção da câmera está perfeitamente alinhada com o vetor de reflexão (agora imaginário), o vetor intermediário se alinha perfeitamente com o vetor normal. Quanto mais próxima a direção da câmera estiver da direção de reflexão original, mais forte será a luminosidade especular.\nAqui você pode ver que, seja qual for a direção de onde o observador olha, o ângulo entre o vetor intermediário e a normal da superfície nunca excede 90 graus (a menos que a luz esteja muito abaixo da superfície, é claro). Os resultados são ligeiramente diferentes das reflexões de Phong, mas geralmente mais plausíveis visualmente, especialmente com expoentes especulares baixos. O modelo de shading Blinn-Phong também é exatamente o modelo de shading usado no pipeline de função fixa anterior da OpenGL.\nObter o vetor intermediário é fácil, adicionamos o vetor de direção da luz e o vetor da câmera e normalizamos o resultado:\n\\[\\bar{H} = \\frac{\\bar{L} + \\bar{V}}{\\parallel \\bar{L} + \\bar{V} \\parallel}\\]\nIsso se traduz em código GLSL da seguinte maneira:\nvec3 lightDir = normalize(lightPos - FragPos); vec3 viewDir = normalize(viewPos - FragPos); vec3 halfwayDir = normalize(lightDir + viewDir);  Então, o cálculo real do termo especular torna-se um produto escalar limitado entre a normal da superfície e o vetor intermediário para obter o ângulo do cosseno entre eles que novamente elevamos a um expoente de brilho especular:\nfloat spec = pow(max(dot(normal, halfwayDir), 0.0), shininess); vec3 specular = lightColor * spec;  E não há nada mais para Blinn-Phong do que o que acabamos de descrever. A única diferença entre a reflexão especular de Blinn-Phong e Phong é que agora medimos o ângulo entre o vetor normal e intermediário, em vez do ângulo entre o vetor da câmera e o vetor de reflexão.\nCom a introdução do vetor intermediário, não deveríamos mais ter o problema de corte especular do shading de Phong. A imagem abaixo mostra a área especular de ambos os métodos com um expoente especular de 0.5:\n\nOutra diferença sutil entre o shading de Phong e Blinn-Phong é que o ângulo entre o vetor intermediário e a normal da superfície costuma ser menor do que o ângulo entre os vetores da câmera e reflexão. Como resultado, para obter visuais semelhantes ao shading de Phong, o expoente de brilho especular deve ser definido um pouco mais alto. Uma regra geral é configurá-lo entre 2 e 4 vezes o expoente de brilho Phong.\nAbaixo está uma comparação entre os dois modelos de reflexão especular com o expoente Phong definido como 8.0 e o componente Blinn-Phong definido como 32.0:\n\nVocê pode ver que o expoente especular de Blinn-Phong é um pouco mais nítido em comparação com Phong. Geralmente, requer alguns ajustes para obter resultados semelhantes aos obtidos anteriormente com o shading de Phong. Mas vale a pena, pois o shading de Blinn-Phong é geralmente mais realista em comparação ao shading de Phong padrão.\nAqui, usamos um shader de fragmento simples que alterna entre reflexões Phong regulares e reflexões Blinn-Phong:\nvoid main() { [...] float spec = 0.0; if(blinn) { vec3 halfwayDir = normalize(lightDir + viewDir); spec = pow(max(dot(normal, halfwayDir), 0.0), 16.0); } else { vec3 reflectDir = reflect(-lightDir, normal); spec = pow(max(dot(viewDir, reflectDir), 0.0), 8.0); }  Você pode encontrar o código-fonte para a demonstração simples aqui. Ao pressionar a tecla b, a demonstração muda de Phong para iluminação Blinn-Phong e vice-versa.\n"
},
{
	"uri": "https://filipecn.github.io/aprendaopengl/ponto_de_partida/criando_uma_janela/",
	"title": "Criando uma Janela",
	"tags": [],
	"description": "",
	"content": "Post Original\nA primeira coisa que precisamos fazer antes de começar a criar gráficos incríveis é criar um contexto OpenGL e uma janela da aplicação na qual desenhar. No entanto, essas operações são específicas para cada sistema operacional e a OpenGL propositalmente tenta se abstrair dessas operações. Isso significa que temos que criar uma janela, definir um contexto e lidar com a entrada do usuário por conta própria.\nFelizmente, existem algumas bibliotecas por aí que fornecem a funcionalidade que buscamos, algumas voltadas especificamente para OpenGL. Essas bibliotecas nos poupam todo o trabalho específico do sistema operacional e nos fornecem uma janela e um contexto OpenGL para renderizar. Algumas das bibliotecas mais populares são GLUT, SDL, SFML e GLFW. No AprendaOpenGL estaremos usando a GLFW. Sinta-se à vontade para usar qualquer uma das outras bibliotecas, a configuração para a maioria é semelhante à configuração da GLFW.\nGLFW A GLFW é uma biblioteca, escrita em C, voltada especificamente para OpenGL. A GLFW nos dá o as funções básicas necessárias para renderizar objetos na tela. Ela nos permite criar um contexto OpenGL, definir parâmetros de janela e lidar com a entrada do usuário, o que é suficiente para nossos propósitos.\nO foco desta e da próxima seção é colocar a GLFW em funcionamento, certificando-se de que ela crie corretamente um contexto OpenGL e exiba uma janela simples para podermos nos divertir. Este capítulo apresenta uma abordagem passo a passo para baixar, compilar e linkar a biblioteca GLFW. Usaremos o Microsoft Visual Studio 2019 no momento desta escrita (observe que o processo é o mesmo nas versões mais recentes do Visual Studio). Se você não estiver usando o Visual Studio (ou uma versão mais antiga), não se preocupe, o processo será semelhante na maioria das outras IDEs.\nCompilando a GLFW A GLFW pode ser obtida na sua própria página de download. A GLFW já tem binários pré-compilados e arquivos de cabeçalho para Visual Studio 2012 até 2019, mas para fins de completude, iremos compilar a GLFW a partir do código-fonte. Isso é para lhe dar uma ideia do processo de compilar bibliotecas de código aberto você mesmo, pois nem todas as bibliotecas terão binários pré-compilados disponíveis. Então, vamos baixar o pacote de código-fonte.\nEstaremos compilando todas bibliotecas como binários de 64-bits, então tenha certeza de pegar as versões de 64-bits caso vá usar os binários pré-compilados.\n Depois de baixar o pacote, extraia-o e abra seu conteúdo. Estamos interessados ​​apenas em alguns itens:\n* A biblioteca resultante da compilação. * A pasta **include**.  Compilar a partir do código-fonte garante que a biblioteca resultante seja perfeitamente ajustada para sua CPU/OS, um luxo que os binários pré-compilados nem sempre oferecem (às vezes, binários pré-compilados nem estão disponíveis para seu sistema). O problema de fornecer código-fonte para o mundo todo, entretanto, é que nem todo mundo usa a mesma IDE ou sistema de compilação para desenvolver suas aplicações, o que significa que os arquivos de projeto/solução fornecidos podem não ser compatíveis com a configuração de outras pessoas. Portanto, as pessoas têm que configurar seu próprio projeto/solução com os arquivos .c/.cpp e .h/.hpp, o que é complicado. Exatamente por essas razões existe uma ferramenta chamada CMake.\nCMake CMake é uma ferramenta que pode gerar arquivos de projeto/solução de escolha do usuário (por exemplo, Visual Studio, Code::Blocks, Eclipse) a partir de uma coleção de arquivos de código-fonte usando scripts CMake pré-definidos. Isso nos permite gerar um arquivo de projeto do Visual Studio 2019 a partir do pacote da GLFW do qual podemos usar para compilar a biblioteca. Primeiro, precisamos baixar e instalar o CMake, que pode ser baixado em sua página de download.\nAssim que o CMake estiver instalado, você pode optar por executar o CMake a partir da linha de comando ou através de sua GUI. Como não estamos tentando complicar as coisas, vamos usar a GUI. O CMake requer uma pasta de código-fonte e uma pasta de destino para os binários. Para a pasta do código-fonte, vamos escolher a pasta raiz do pacote de origem da GLFW baixado e, para a pasta de compilação, criaremos umo novo diretório build e, em seguida, selecionaremos esse diretório.\nUma vez que as pastas de origem e destino tenham sido definidas, clique no botão Configure para que o CMake possa ler as configurações necessárias e o código-fonte. Temos então que escolher o gerador para o projeto e, como estamos usando o Visual Studio 2019, vamos escolher a opção Visual Studio 16 (Visual Studio 2019 também é conhecido como Visual Studio 16). O CMake irá então exibir as opções de compilação possíveis para configurar a biblioteca. Podemos deixá-las com seus valores padrão e clicar em Configure novamente para armazenar as configurações. Depois de definir as configurações, clicamos em Generate e os arquivos de projeto resultantes serão gerados em sua pasta de build.\nCompilação Na pasta build, um arquivo chamado GLFW.sln pode ser encontrado e o abrimos com o Visual Studio 2019. Como o CMake gerou um arquivo de projeto que já contém as configurações adequadas, só temos que compilar a solução. O CMake deveria ter configurado automaticamente a solução para que compile uma biblioteca de 64 bits; agora clique em build solution. Isso nos dará um arquivo de biblioteca compilado que pode ser encontrado em build/src/Debug denominado glfw3.lib.\nDepois de gerar a biblioteca, precisamos ter certeza de que a IDE sabe onde encontrar a biblioteca e os arquivos de inclusão para nosso programa OpenGL. Existem duas abordagens comuns para fazer isso:\n Encontramos as pastas /lib e /include da IDE/compilador e adicionamos o conteúdo da pasta include ds GLFW à pasta /include da IDE e, da mesma forma, adicionamos glfw3.lib à pasta /lib ds IDE. Isso funciona, mas não é a abordagem recomendada. É difícil de manter sua biblioteca e incluir arquivos, e uma nova instalação da sua IDE/compilador resultaria em você ter que fazer todo este processo novamente. Outra abordagem (recomendada) é criar um novo conjunto de diretórios em um local de sua escolha que contenha todos os arquivos de cabeçalho/bibliotecas de terceiros aos quais você pode se referir a partir de sua IDE/compilador. Você pode, por exemplo, criar uma única pasta que contém uma pasta Libs e Include e armazenar todos os nossos arquivos de biblioteca e cabeçalho, respectivamente, para projetos OpenGL. Agora todas as bibliotecas de terceiros são organizadas em um único local (que pode ser compartilhado por vários computadores). O requisito, entretanto, é que cada vez que criarmos um novo projeto, teremos que dizer a IDE onde encontrar esses diretórios.  Assim que os arquivos necessários forem armazenados em um local de sua escolha, podemos começar a criar nosso primeiro projeto OpenGL GLFW.\nNosso Primeiro Projeto Primeiro, vamos abrir o Visual Studio e criar um novo projeto. Escolha C++ se várias opções forem fornecidas e pegue o Projeto Vazio (Empty Project) (não se esqueça de dar ao seu projeto um nome adequado). Como faremos tudo em 64 bits e o padrão do projeto é de 32 bits, precisaremos alterar o menu na parte superior, próximo a Depurar (Debug) de x86 para x64:\nAgora temos uma área de trabalho para criar nossa primeiríssima aplicação OpenGL!\nLinking Para que o projeto use a GLFW, precisamos linkar  (link) a biblioteca ao nosso projeto. Isso pode ser feito especificando o arquivo glfw3.lib nas configurações do linker, mas nosso projeto ainda não sabe onde encontrar o arquivo glfw3.lib, pois armazenamos nossas bibliotecas de terceiros em um diretório diferente. Portanto, precisamos primeiro adicionar este diretório ao projeto.\nPodemos dizer a IDE para levar esse diretório em consideração quando precisar procurar por arquivos de biblioteca e incluir arquivos. Clique com o botão direito do mouse no nome do projeto no solution explorer e vá para os diretórios VC++, conforme mostrado na imagem abaixo:\nA partir daí, você pode adicionar seus próprios diretórios para que o projeto saiba onde pesquisar. Isso pode ser feito inserindo-os manualmente no texto ou clicando na string de localização apropriada e selecionando a opção \u0026lt;Edit ..\u0026gt;. Faça isso para os Diretórios da Biblioteca (Library Directories) e Incluir Diretórios (Include Directories):\nAqui você pode adicionar quantos diretórios extras desejar e, a partir desse ponto, a IDE também pesquisará esses diretórios ao procurar por arquivos de biblioteca e de cabeçalho. Assim que sua pasta Include da GLFW for incluída, você poderá encontrar todos os arquivos de cabeçalho para a GLFW incluindo \u0026lt;GLFW / ..\u0026gt;. O mesmo se aplica aos diretórios da biblioteca.\nUma vez que o VS agora pode encontrar todos os arquivos necessários, podemos finalmente linkar a GLFW ao projeto acessando as guias Linker e Input:\nPara linkar a uma biblioteca, você deve especificar o nome da biblioteca para o linker. Como o nome da biblioteca é glfw3.lib, adicionamos isso ao campo Dependências adicionais (Additional Dependencies) (manualmente ou usando a opção \u0026lt;Edit ..\u0026gt;) e a partir desse ponto o GLFW será linkado quando compilarmos. Além da GLFW, também devemos adicionar um link para a biblioteca OpenGL, mas isso pode variar de acordo com o sistema operacional:\nBiblioteca OpenGL no Windows Se você estiver no Windows, a biblioteca OpenGL opengl32.lib vem com o Microsoft SDK, que é instalado por padrão quando você instala o Visual Studio. Como este capítulo usa o compilador VS e está no Windows, adicionamos opengl32.lib às configurações do linker. Observe que o equivalente de 64 bits da biblioteca OpenGL é chamado opengl32.lib, assim como o equivalente de 32 bits, que é um nome um tanto infeliz.\nBiblioteca OpenGL no Linux Em sistemas Linux, você precisa se linkar à biblioteca libGL.so adicionando -lGL às configurações do linker. Se você não conseguir encontrar a biblioteca, provavelmente precisará instalar qualquer um dos pacotes de desenvolvimento Mesa, NVidia ou AMD.\nEntão, depois de adicionar as bibliotecas GLFW e OpenGL às configurações do linker, você pode incluir os arquivos de cabeçalho para GLFW da seguinte maneira:\n#include \u0026lt;GLFW\\glfw3.h\u0026gt; Para usuários Linux compilando com GCC, as seguintes opções de linha de comando podem ajudá-lo a compilar o projeto: -lglfw3 -lGL -lX11 -lpthread -lXrandr -lXi -ldl. Não linkar corretamente as bibliotecas correspondentes irá gerar muitos erros de referências indefinidas (undefined references).\n Isso conclui a instalação e configuração da GLFW.\nGLAD Ainda não chegamos lá, pois ainda há outra coisa que precisamos fazer. Como a OpenGL é realmente apenas um padrão/especificação, cabe ao fabricante do driver implementar a especificação em um driver compatível com a placa de vídeo específica. Como existem muitas versões diferentes de drivers OpenGL, a localização da maioria de suas funções não é conhecida no tempo de compilação e precisa ser consultada em tempo de execução. É então tarefa do desenvolvedor recuperar a localização das funções de que precisa e armazená-las em ponteiros de função para uso posterior. A recuperação desses locais é específica do sistema operacional. No Windows, é mais ou menos assim:\n// define o prototipo das funcoes typedef void (*GL_GENBUFFERS) (GLsizei, GLuint*); // encontra a funcao e a armazena em um ponteiro de funcao GL_GENBUFFERS glGenBuffers = (GL_GENBUFFERS)wglGetProcAddress(\u0026#34;glGenBuffers\u0026#34;); // a funcao agora pode ser chamada normalmente unsigned int buffer; glGenBuffers(1, \u0026amp;buffer); Como você pode ver, o código parece complexo e é um processo complicado fazer isso para cada função que você pode precisar que ainda não foi declarada. Felizmente, existem bibliotecas para este propósito também, onde a GLAD é uma biblioteca popular e atualizada.\nConfigurando a GLAD GLAD é uma biblioteca de código aberto que gerencia todo o trabalho pesado de que falamos. A GLAD tem uma configuração ligeiramente diferente da maioria das bibliotecas de código aberto comuns. A GLAD usa um serviço da web onde podemos dizer a GLAD para qual versão do OpenGL gostaríamos de definir e carregar todas as funções OpenGL relevantes de acordo com essa versão.\nVá para o serviço da web da GLAD, certifique-se de que a linguagem esteja definida como C++ e, na seção API, selecione uma versão OpenGL de pelo menos 3.3 (que é o que usaremos; versões superiores também servem). Certifique-se também de que o profile esteja definido como Core e que a opção Generate a loader esteja marcada. Ignore as extensões (por enquanto) e clique em Generate para produzir os arquivos da biblioteca.\nA GLAD agora deve ter fornecido a você um arquivo zip contendo duas pastas de include e um único arquivo glad.c. Copie ambas as pastas de inclusão (glad e KHR) em seu diretório de inclusão (ou adicione um item extra apontando para essas pastas) e adicione o arquivo glad.c ao seu projeto.\nApós as etapas anteriores, você deve ser capaz de adicionar a seguinte diretiva de inclusão acima do arquivo:\n#include \u0026lt;glad/glad.h\u0026gt; Apertar o botão de compilação não deveria dar a você nenhum erro, e nesse ponto iremos para o próximo capítulo, onde discutiremos como podemos realmente usar a GLFW e a GLAD para configurar um contexto OpenGL e gerar uma janela. Certifique-se de verificar se todos os seus diretórios de inclusão e biblioteca estão corretos e se os nomes das bibliotecas nas configurações do linker correspondem às bibliotecas correspondentes.\nRecursos adicionais  GLFW: Window Guide: guia oficial da GLFW sobre como instalar e configurar uma janela da GLFW. Building applications: fornece ótimas informações sobre o processo de compilação/linkagem da sua aplicação e uma lista grande de possíveis erros (mais soluções) que podem surgir. GLFW with Code::Blocks: compilando a GLFW na IDE Code::Blocks. Running CMake: breve visão geral de como executar o CMake no Windows e no Linux. Writing a build system under Linux: um tutorial de autotools por Wouter Verholst sobre como escrever um sistema de compilação no Linux. Polytonic/Glitter: um projeto simples que vem pré-configurado com todas as bibliotecas relevantes; ótimo se você deseja um projeto de base sem o incômodo de ter que compilar todas as bibliotecas você mesmo.  "
},
{
	"uri": "https://filipecn.github.io/aprendaopengl/ponto_de_partida/ola_janela/",
	"title": "Olá Janela",
	"tags": [],
	"description": "",
	"content": "Post Original\nVamos ver se conseguimos fazer a GLFW rodar. Primeiro, crie um arquivo .cpp e adicione as seguintes linhas no seu início.\n#include \u0026lt;glad/glad.h\u0026gt;#include \u0026lt;GLFW/glfw3.h\u0026gt; Certifique-se de incluir a GLAD antes da GLFW. O arquivo de inclusão para a GLAD inclui os cabeçalhos OpenGL necessários nos bastidores (como GL/gl.h), portanto, certifique-se de incluir o GLAD antes de outros arquivos de cabeçalho que requerem OpenGL (como GLFW).\n A seguir, criamos a função main  onde instanciaremos a janela GLFW:\nint main() { glfwInit(); glfwWindowHint(GLFW_CONTEXT_VERSION_MAJOR, 3); glfwWindowHint(GLFW_CONTEXT_VERSION_MINOR, 3); glfwWindowHint(GLFW_OPENGL_PROFILE, GLFW_OPENGL_CORE_PROFILE); //glfwWindowHint(GLFW_OPENGL_FORWARD_COMPAT, GL_TRUE);  return 0; }  Na função main, primeiro inicializamos a GLFW com glfwInit  , após o qual podemos configurar a GLFW usando glfwWindowHint  . O primeiro argumento de glfwWindowHint  nos diz qual opção queremos configurar, onde podemos selecionar a opção de um enum de opções possíveis prefixadas com GLFW_. O segundo argumento é um inteiro que define o valor de nossa opção. Uma lista de todas as opções possíveis e seus valores correspondentes pode ser encontrada na documentação sobre manipulação de janelas da GLFW. Se você tentar executar o programa agora e houver muitos erros de referência indefinida, isso significa que você não linkou a biblioteca GLFW com sucesso.\nComo o foco deste livro está na OpenGL versão 3.3, gostaríamos de dizer a GLFW que 3.3 é a versão OpenGL que queremos usar. Desta forma, a GLFW pode fazer os arranjos apropriados ao criar o contexto OpenGL. Isso garante que, quando um usuário não tiver a versão adequada do OpenGL, a GLFW não será executado. Definimos as versões principal e secundária para 3. Também informamos a GLFW que queremos usar explicitamente o core-profile. Dizer a GLFW que queremos usar o core-profile significa que teremos acesso a um subconjunto menor de recursos da OpenGL sem compatibilidade com versões anteriores de que não precisamos mais. Observe que no Mac OS X você precisa adicionar glfwWindowHint  (, GL_TRUE ); ao seu código de inicialização para que isso funcione.\nCertifique-se de ter a OpenGL versão 3.3 ou superior instalada em seu sistema/hardware, caso contrário, o aplicativo irá travar ou exibir um comportamento indefinido. Para encontrar a versão OpenGL em sua máquina, chame glxinfo em máquinas Linux ou use um utilitário como o OpenGL Extension Viewer para Windows. Se sua versão suportada for inferior, tente verificar se sua placa de vídeo suporta OpenGL 3.3+ (caso contrário, ela é muito antiga) e/ou atualize seus drivers.\n Em seguida, somos obrigados a criar um objeto de janela. Este objeto de janela contém todos os dados de janelas e é exigido pela maioria das outras funções do GLFW.\nGLFWwindow* window = glfwCreateWindow(800, 600, \u0026#34;LearnOpenGL\u0026#34;, NULL, NULL); if (window == NULL) { std::cout \u0026lt;\u0026lt; \u0026#34;Failed to create GLFW window\u0026#34; \u0026lt;\u0026lt; std::endl; glfwTerminate(); return -1; } glfwMakeContextCurrent(window);  A função glfwCreateWindow  requer a largura e altura da janela como seus dois primeiros argumentos, respectivamente. O terceiro argumento nos permite criar um nome para a janela; por enquanto, o chamamos de \u0026quot;LearnOpenGL\u0026quot;, mas você tem permissão para nomeá-lo como quiser. Podemos ignorar os 2 últimos parâmetros. A função retorna um objeto GLFWwindow  que mais tarde precisaremos para outras operações GLFW. Depois disso, dizemos a GLFW para tornar o contexto de nossa janela o contexto principal na thread corrente.\nGLAD No capítulo anterior, mencionamos que a GLAD gerencia ponteiros de função para OpenGL, portanto, queremos inicializar a GLAD antes de chamar qualquer função OpenGL:\nif (!gladLoadGLLoader((GLADloadproc)glfwGetProcAddress)) { std::cout \u0026lt;\u0026lt; \u0026#34;Failed to initialize GLAD\u0026#34; \u0026lt;\u0026lt; std::endl; return -1; }  Janela de exibição (Viewport) Antes de começarmos a renderizar, temos que fazer uma última coisa. Temos que dizer a OpenGL o tamanho da janela de renderização para que a OpenGL saiba como queremos exibir os dados e as coordenadas em relação à janela. Podemos definir essas dimensões por meio da função glViewport  :\nglViewport(0, 0, 800, 600);  Os primeiros dois parâmetros de glViewport  definem a localização do canto esquerdo inferior da janela. O terceiro e quarto parâmetros definem a largura e a altura da janela de renderização em pixels, que definimos igual ao tamanho da janela da GLFW.\nNa verdade, poderíamos definir as dimensões da janela de visualização com valores menores do que as dimensões da GLFW; então, toda a renderização da OpenGL seria exibida em uma janela menor e poderíamos, por exemplo, exibir outros elementos fora da janela de exibição OpenGL.\nNos bastidores, a OpenGL usa os dados especificados via glViewport  para transformar as coordenadas 2D que processou em coordenadas na tela. Por exemplo, um ponto processado de localização $(-0.5,0.5)$ seria (como sua transformação final) mapeado para $(200,450)$ nas coordenadas da tela. Observe que as coordenadas processadas em OpenGL estão entre $-1$ e $1$, portanto, mapeamos efetivamente do intervalo ($-1$ a $1$) a $(0, 800)$ e $(0, 600)$.\n No entanto, no momento em que um usuário redimensiona a janela, a janela de visualização também deve ser ajustada. Podemos registrar uma função de callback na janela toda vez que a janela é redimensionada. Esta função tem a seguinte assinatura:\nvoid framebuffer_size_callback(GLFWwindow* window, int width, int height);  A função de tamanho do framebuffer leva um GLFWwindow  como seu primeiro argumento e dois inteiros indicando as novas dimensões da janela. Sempre que a janela muda de tamanho, a GLFW chama esta função e preenche os argumentos apropriados para você processar.\nvoid framebuffer_size_callback(GLFWwindow* window, int width, int height) { glViewport(0, 0, width, height); }  Nós temos que dizer a GLFW que queremos chamar esta função em todo redimensionamento da janela registrando a função:\nglfwSetFramebufferSizeCallback(window, framebuffer_size_callback);  Quando a janela é exibida pela primeira vez, framebuffer_size_callback  é chamada também com as novas dimensões da janela. Para telas de retina, a largura width e a altura height serão significativamente maiores do que os valores de entrada originais.\nExistem muitas funções callback que podemos definir para registrar nossas próprias funções. Por exemplo, podemos fazer uma função para processar as mudanças na entrada do joystick, mensagens de erro do processo, etc. Registramos as funções callback depois de criarmos a janela e antes do loop de renderização ser iniciado.\nPrepare seus motores Não queremos que o programa desenhe uma única imagem e então feche imediatamente. Queremos que o programa continue desenhando imagens e manipulando a entrada do usuário até que ele seja explicitamente instruído a parar. Por esta razão, temos que criar um loop , que agora chamamos de loop  de  renderização  (rendering loop ), que continua em execução até que digamos a GLFW para parar. O código a seguir mostra um loop de renderização muito simples:\nwhile(!glfwWindowShouldClose(window)) { glfwSwapBuffers(window); glfwPollEvents(); }  A função glfwWindowShouldClose  verifica no início de cada iteração do loop se a GLFW foi instruída a fechar. Nesse caso, a função retorna true e o loop de renderização para de executar, e podemos fechar a aplicação.\nA função glfwPollEvents  verifica se algum evento é acionado (como entrada do teclado ou eventos de movimento do mouse), atualiza o estado da janela e chama as funções correspondentes (que podemos registrar por meio de callbacks ). O glfwSwapBuffers  irá trocar o buffer de cor (um grande buffer 2D que contém valores de cor para cada pixel na janela da GLFW) que é usado para renderizar durante esta iteração de renderização e mostrá-lo como saída na tela.\nBuffer Duplo: Quando um programa desenha em um único buffer, a imagem final pode exibir problemas de oscilação. Isso ocorre porque a imagem de saída não é desenhada em um instante, mas desenhada pixel por pixel e geralmente da esquerda para a direita e de cima para baixo. Como essa imagem não é exibida imediatamente para o usuário enquanto ainda está sendo renderizada, o resultado pode conter artefatos. Para contornar esses problemas, as aplicações aplicam um buffer duplo para a renderização. O buffer frontal (front contém a imagem de saída final que é mostrada na tela, enquanto todos os comandos de renderização são direcionados ao buffer posterior (back ). Assim que todos os comandos de renderização forem concluídos, trocamos (swap) o buffer traseiro para o buffer frontal para que a imagem possa ser exibida sem ainda ter sido renderizada, removendo todos os artefatos mencionados acima.\n Para terminar Assim que sairmos do loop de renderização, gostaríamos de limpar/deletar adequadamente todos os recursos da GLFW que foram alocados. Podemos fazer isso por meio da função glfwTerminate  que chamamos no final da função principal.\nglfwTerminate (); return 0;  Isso limpará todos os recursos e sairá do programa corretamente. Agora tente compilar sua aplicação e se tudo correr bem, você verá a seguinte saída:\n\nSe for uma imagem preta sem graça e entediante, você fez as coisas certas! Se você não obteve a imagem certa ou está confuso sobre como tudo se encaixa, verifique o código-fonte completo aqui.\nSe você tiver problemas para compilar, primeiro certifique-se de que todas as opções do linker estejam definidas corretamente e de que incluiu corretamente os diretórios corretos na sua IDE (conforme explicado no capítulo anterior). Verifique também se o seu código está correto; você pode verificá-lo comparando-o com o código-fonte completo.\nEntrada Também queremos ter alguma forma de controle de entrada na GLFW e podemos fazer isso com várias funções de entrada da GLFW. Vamos usar a função glfwGetKey  da GLFW que recebe a janela como entrada junto com uma chave. A função retorna se esta tecla está sendo pressionada. Estamos criando uma função processInput  para manter todo o código de entrada organizado:\nvoid processInput(GLFWwindow *window) { if(glfwGetKey(window, GLFW_KEY_ESCAPE) == GLFW_PRESS) glfwSetWindowShouldClose(window, true); }  Aqui verificamos se o usuário pressionou a tecla escape (se não for pressionada, glfwGetKey  retorna GLFW_RELEASE ). Se o usuário pressionou a tecla escape, fechamos a GLFW definindo sua propriedade WindowShouldClose como true usando glfwSetwindowShouldClose  . A próxima verificação de condição do loop while principal falhará e o aplicativo será fechado.\nEm seguida, chamamos processInput  a cada iteração do loop de renderização:\nwhile (!glfwWindowShouldClose(window)) { processInput(window); glfwSwapBuffers(window); glfwPollEvents(); }  Isso nos dá uma maneira fácil de verificar se há ações de teclas específicas e reagir de acordo a cada frame  (quadro). Uma iteração do loop de renderização é mais comumente chamada de frame  .\nRenderização Queremos colocar todos os comandos de renderização no loop de renderização, já que queremos executar todos os comandos de renderização a cada iteração ou quadro do loop. Isso seria mais ou menos assim:\n// loop de renderizacao while(!glfwWindowShouldClose(window)) { // entrada  processInput(window); // comandos de renderizacao aqui  ... // checar e chamar eventos e troca de buffers  glfwPollEvents(); glfwSwapBuffers(window); }  Apenas para testar se as coisas realmente funcionam, queremos limpar a tela com uma cor de nossa escolha. No início do frame, queremos limpar a tela. Caso contrário, ainda veríamos os resultados do quadro anterior (pode ser o efeito que você está procurando, mas normalmente não). Podemos limpar o buffer de cor da tela usando glClear  , onde passamos os bits de buffer para especificar qual buffer gostaríamos de limpar. Os bits possíveis que podemos definir são GL_COLOR_BUFFER_BIT , GL_DEPTH_BUFFER_BIT e GL_STENCIL_BUFFER_BIT . No momento, só nos importamos com os valores das cores, portanto, apenas limpamos o buffer de cores.\nglClearColor (0.2f, 0.3f, 0.3f, 1.0f); glClear (GL_COLOR_BUFFER_BIT);  Observe que também especificamos a cor para limpar a tela usando glClearColor  . Sempre que chamamos glClear  e limpamos o buffer de cor, todo o buffer de cor será preenchido com a cor configurada por glClearColor  . Isso resultará em uma cor verde-azulada escura.\nComo você deve se lembrar do capítulo OpenGL, a função glClearColor  é uma função de configuração de estado e glClear  é uma função que usa o estado atual para pegar a cor de compensação.\n \nO código fonte completo da aplicação pode ser encontrado aqui.\nAgora temos tudo pronto para preencher o loop de renderização com um monte de chamadas de renderização, mas isso fica para o próximo capitulo. Acho que estamos divagando há tempo suficiente aqui.\n"
},
{
	"uri": "https://filipecn.github.io/aprendaopengl/iluminacao/cores/",
	"title": "Cores",
	"tags": [],
	"description": "",
	"content": "Usamos e manipulamos rapidamente as cores nos capítulos anteriores, mas nunca as definimos adequadamente. Aqui, discutiremos o que são as cores e começaremos a preparar o terreno para os próximos capítulos de Iluminação.\nNo mundo real, as cores podem assumir qualquer valor de cor conhecido, com cada objeto tendo sua(s) própria(s) cor(es). No mundo digital, precisamos mapear as (infinitas) cores reais para valores digitais (limitados) e, portanto, nem todas as cores do mundo real podem ser representadas digitalmente. As cores são representadas digitalmente usando uma componente vermelha, verde e azul comumente abreviadas como RGB. Usando diferentes combinações apenas desses 3 valores, dentro de um intervalo de [0,1], podemos representar quase qualquer cor que existe. Por exemplo, para obter uma cor de coral, definimos um vetor de cor como:\nglm::vec3 coral(1.0f, 0.5f, 0.31f);  A cor de um objeto que vemos na vida real não é a cor que ele realmente tem, mas é a cor refletida  ( reflected  do objeto. As cores que não são absorvidas (rejeitadas) pelo objeto são as cores que percebemos dele. Por exemplo, a luz do sol é percebida como uma luz branca que é a soma combinada de muitas cores diferentes (como você pode ver na imagem). Se jogarmos essa luz branca em um brinquedo azul, ele absorverá todas as subcores da cor branca, exceto a cor azul. Como o brinquedo não absorve a parte de cor azul, ela é refletida. Essa luz refletida entra em nossos olhos, fazendo com que pareça que o brinquedo tem uma cor azul. A imagem a seguir mostra isso para um brinquedo de cor coral, onde reflete várias cores com intensidade diferentes:\n\nVocê pode ver que a luz do sol branca é uma coleção de todas as cores visíveis e o objeto absorve uma grande parte dessas cores. Ele reflete apenas as cores que representam a cor do objeto e a combinação delas é o que percebemos (neste caso, uma cor coral).\nTecnicamente, é um pouco mais complicado, mas chegaremos a isso nos capítulos de PBR.\n Essas regras de reflexão de cores se aplicam diretamente no mundo dos gráficos. Quando definimos uma fonte de luz em OpenGL, queremos dar uma cor a essa fonte de luz. No parágrafo anterior, tínhamos uma cor branca, então daremos à fonte de luz uma cor branca também. Se multiplicássemos a cor da fonte de luz pelo valor da cor de um objeto, a cor resultante seria a cor refletida do objeto (e, portanto, sua cor percebida). Vamos revisitar nosso brinquedo (desta vez com um valor coral) e ver como calcularíamos sua cor percebida no mundo dos gráficos. Obtemos o vetor de cor resultante fazendo uma multiplicação de componentes entre os vetores de luz e cor do objeto:\nglm::vec3 lightColor(1.0f, 1.0f, 1.0f); glm::vec3 toyColor(1.0f, 0.5f, 0.31f); glm::vec3 result = lightColor * toyColor; // = (1.0f, 0.5f, 0.31f);  Podemos ver que a cor do brinquedo absorve grande parte da luz branca, mas reflete vários valores de vermelho, verde e azul com base em seu próprio valor de cor. Esta é uma representação de como as cores funcionariam na vida real. Podemos, portanto, definir a cor de um objeto como a quantidade de cada componente de cor que ele reflete de uma fonte de luz. Agora, o que aconteceria se usássemos uma luz verde?\nglm::vec3 lightColor(0.0f, 1.0f, 0.0f); glm::vec3 toyColor(1.0f, 0.5f, 0.31f); glm::vec3 result = lightColor * toyColor; // = (0.0f, 0.5f, 0.0f);  Como podemos ver, o brinquedo não possui luz vermelha e azul para absorver e / ou refletir. O brinquedo também absorve metade do valor da componente verde da luz, mas também reflete metade desse valor. A cor do brinquedo que percebemos seria então uma cor esverdeada escura. Podemos ver que se usarmos uma luz verde, apenas as componentes da cor verde podem ser refletidas e, portanto, percebidas; nenhuma cor vermelha e azul é percebida. Como resultado, o objeto coral subitamente se torna um objeto esverdeado escuro. Vamos tentar mais um exemplo com uma luz de cor verde-oliva escura:\nglm::vec3 lightColor(0.33f, 0.42f, 0.18f); glm::vec3 toyColor(1.0f, 0.5f, 0.31f); glm::vec3 result = lightColor * toyColor; // = (0.33f, 0.21f, 0.06f);  Como você pode ver, podemos obter cores interessantes de objetos usando cores de luz diferentes. Não é difícil ser criativo com as cores.\nMas chega de cores, vamos começar a construir uma cena onde possamos fazer experiências.\nUma Cena de Iluminação Nos próximos capítulos, criaremos visuais interessantes simulando a iluminação do mundo real fazendo uso extensivo de cores. Como agora usaremos fontes de luz, queremos exibi-las como objetos visuais na cena e adicionar pelo menos um objeto para simular a iluminação.\nA primeira coisa de que precisamos é um objeto para lançar a luz e usaremos o infame cubo contêiner dos capítulos anteriores. Também precisaremos de um objeto de luz para mostrar onde a fonte de luz está localizada na cena 3D. Para simplificar, representaremos a fonte de luz com um cubo também (já temos os dados do vértice, certo?).\nPortanto, preencher um objeto de buffer de vértices, definir ponteiros de atributo de vértice e tudo mais deve ser familiar para você agora, portanto, não o(a) conduziremos por essas etapas. Se você ainda não faz idéia sobre esses assuntos envolvendo vértices, sugiro que reveja os capítulos anteriores e trabalhe com os exercícios, se possível, antes de continuar.\nPortanto, a primeira coisa de que precisaremos é um shader de vértice para desenhar o contêiner. As posições dos vértices do contêiner permanecem as mesmas (embora não precisemos de coordenadas de textura neste momento), portanto, o código não deve ser nada novo. Estaremos usando uma versão simplificada do shader de vértice dos últimos capítulos:\n#version 330 core layout (location = 0) in vec3 aPos; uniform mat4 model; uniform mat4 view; uniform mat4 projection; void main() { gl_Position = projection * view * model * vec4(aPos, 1.0); }  Certifique-se de atualizar os dados dos vértices e os ponteiros de atributos para corresponder ao novo shader de vértice (se quiser, você pode realmente manter os dados de textura e ponteiros de atributo ativos; apenas não os estamos usando agora).\nComo também iremos renderizar um cubo de fonte de luz, queremos gerar um novo VAO especificamente para a fonte de luz. Poderíamos renderizar a fonte de luz com o mesmo VAO e, em seguida, fazer algumas transformações de posição de luz na matriz model , mas nos próximos capítulos iremos alterar os dados de vértice e os ponteiros de atributo do objeto com bastante frequência e não queremos que essas alterações se propaguem para o objeto de fonte de luz (nos preocupamos apenas com as posições dos vértices do cubo de luz), então criaremos um novo VAO:\nunsigned int lightVAO; glGenVertexArrays(1, \u0026amp;lightVAO); glBindVertexArray(lightVAO); // we only need to bind to the VBO, the container\u0026#39;s VBO\u0026#39;s data already contains the data. glBindBuffer(GL_ARRAY_BUFFER, VBO); // set the vertex attribute glVertexAttribPointer(0, 3, GL_FLOAT, GL_FALSE, 3 * sizeof(float), (void*)0); glEnableVertexAttribArray(0);  O código deve ser relativamente simples. Agora que criamos o contêiner e o cubo da fonte de luz, resta uma coisa a definir: o shader de fragmento para o recipiente e a fonte de luz:\n#version 330 core out vec4 FragColor; uniform vec3 objectColor; uniform vec3 lightColor; void main() { FragColor = vec4(lightColor * objectColor, 1.0); }  O shader de fragmento aceita uma cor de objeto e uma cor de luz de variáveis uniformes. Aqui, multiplicamos a cor da luz pela cor (refletida) do objeto, como discutimos no início deste capítulo. Novamente, esse shader deve ser fácil de entender. Vamos definir a cor do objeto para a cor coral da última seção com uma luz branca:\n// nao se esqueca de ativar o programa de shader correspondente primeiro (para configurar os uniformes) lightingShader.use(); lightingShader.setVec3(\u0026#34;objectColor\u0026#34;, 1.0f, 0.5f, 0.31f); lightingShader.setVec3(\u0026#34;lightColor\u0026#34;, 1.0f, 1.0f, 1.0f);  Uma coisa a se notar é que quando começarmos a atualizar esses shaders de iluminação nos próximos capítulos, o cubo da fonte de luz também será afetado, e não é isso que queremos. Não queremos que a cor do objeto da fonte de luz seja afetada nos cálculos de iluminação, mas sim manter a fonte de luz isolada do resto. Queremos que a fonte de luz tenha uma cor brilhante constante, não afetada por outras mudanças de cor (isso faz com que pareça que o cubo da fonte de luz realmente é a fonte da luz).\nPara resolver isso, precisamos criar um segundo conjunto de shaders que usaremos para desenhar o cubo da fonte de luz, portanto, protegidos de quaisquer mudanças nos shaders de iluminação. O shader de vértice é o mesmo que o shader de vértice de iluminação, então você pode simplesmente copiar o código-fonte. O shader de fragmento do cubo da fonte de luz garante que a cor do cubo permaneça brilhante, definindo uma cor branca constante na lâmpada:\n#version 330 core out vec4 FragColor; void main() { FragColor = vec4(1.0); // define todos 4 valores do vetor para 1.0 }  Quando quisermos renderizar, queremos renderizar o objeto recipiente (ou possivelmente muitos outros objetos) usando o shader de iluminação que acabamos de definir, e quando quisermos desenhar a fonte de luz, usamos os shaders da fonte de luz. Durante os capítulos de Iluminação, estaremos atualizando gradativamente os shaders de iluminação para obter resultados mais realistas.\nO objetivo principal do cubo da fonte de luz é mostrar de onde vem a luz. Normalmente definimos a posição de uma fonte de luz em algum lugar da cena, mas esta é simplesmente uma posição sem significado visual. Para mostrar onde a fonte de luz realmente está, renderizamos um cubo no mesmo local da fonte de luz. Renderizamos este cubo com o shader de cubo de fonte de luz para garantir que o cubo sempre permaneça branco, independentemente das condições de luz da cena.\nEntão, vamos declarar uma variável vec3 global que representa a localização da fonte de luz nas coordenadas de mundo:\nglm::vec3 lightPos(1.2f, 1.0f, 2.0f);  Em seguida, transladamos o cubo da fonte de luz para a posição da fonte de luz e o redimensionamos antes de renderizá-lo:\nmodel = glm::mat4(1.0f); model = glm::translate(model, lightPos); model = glm::scale(model, glm::vec3(0.2f));  O código de renderização resultante para o cubo da fonte de luz deve ser semelhante a este:\nlightCubeShader.use(); // define os uniformes das matrizes de model, view e projection [...] // desenhe o objeto do cubo de luz glBindVertexArray(lightCubeVAO); glDrawArrays(GL_TRIANGLES, 0, 36);\t O posicionamento de todos os fragmentos de código em seus locais apropriados resultaria em uma aplicação OpenGL devidamente configurada para fazer experiências com iluminação. Se tudo compilar, deve ficar assim:\n\nNão há muito o que ver agora, mas prometo que ficará mais interessante nos próximos capítulos.\nSe você tiver dificuldades para descobrir onde todos os fragmentos de código se encaixam no programa como um todo, verifique o código-fonte aqui e consulte cuidadosamente o código / comentários.\nAgora que temos um bom conhecimento sobre as cores e criamos uma cena básica para fazer experiências com iluminação, podemos pular para o próximo capítulo, onde começa a verdadeira magia.\n"
},
{
	"uri": "https://filipecn.github.io/aprendaopengl/iluminacao_avancada/correcao_gama/index_/",
	"title": "Correção Gama",
	"tags": [],
	"description": "",
	"content": "Post original\nLogo que calculamos as cores finais de cada pixel, temos que mostrá-los em um monitor. Antigamente a maioria dos monitores utilizavam tubos de raios catódicos (cathode-ray tube - CRT) para gerar imagens na tela. Nesses monitores, dobrar a tensão de entrada não resultava no dobro da quantidade de brilho. Dobrar a tensão de entrada resultava em um brilho que seguia uma relação exponencial de aproximadamente 2.2, conhecida como o gama do monitor. Acontece que, coincidentemente, esta relação também se assemelha a como humanos medem o brilho, dado que este é mostrado na tela com uma relação similar de potência (inversa). Para melhor entender o que tudo isto significa veja a imagem seguinte:\n\nA linha de cima parece com uma escala de brilho correta para o olho humano, dobrando o brilho (de \\(0.1\\) para \\(0.2\\) por exemplo) parece de fato ser duas vezes mais brilhante com algumas diferenças consistentes. Porém, quando estamos falando do brilho físico da luz, ou seja, a quantidade de fótons vindos de uma fonte de luz, a escala debaixo mostra o brilho correto. Na escala inferior, o dobrar o brilho resulta no brilho físico correto, mas já que nossos olhos percebem brilho diferentemente (mais susceptível a mudanças em cores escuras) o brilho parece estranho.\nComo o olho humano prefere enxergar o brilho das cores de acordo com a escala superior, monitores (ainda hoje) usam uma relação de potência para mostrar as cores de saída de tal forma que os brilhos físicos das cores originais sejam mapeados (não linearmente) a brilhos de cores na escala de cima.\nEste mapeamento não linear feito pelos monitores resulta em brilhos mais agradáveis para os nossos olhos, mas quando se fala de renderizar gráficos existe um problema: todas opções de cor e brilho que nós configuramos em nossas aplicações são baseadas no que percebemos do monitor e portanto todas opções são de fato opções não lineares de brilho/cor. Olhe o gráfico a seguir:\n\nA linha pontilhada representa valores de cor/luz no espaço linear e a linha cheia representa o espaço de cor que o monitor mostra. Se dobrarmos a cor no espaço linear, o resultado é de fato o dobro do valor. Por exemplo, pegue o vetor cor da luz \\((0.5, 0.0, 0.0)\\) que representa uma luz vermelha semiescura. Se fossemos dobrar esta luz no espaço linear teríamos \\((1.0, 0.0, 0.0)\\), como podemos ver no gráfico. Entretanto, a cor original é mostrada no monitor como \\((0.218, 0.0, 0.0)\\). Aqui é onde começam a surgir os problemas: uma vez que dobramos a luz vermelha escura no espaço linear, esta se torna mais de \\(4.5\\) vezes mais brilhante no monitor!\nAté este capítulo, nós assumimos trabalhar no espaço linear, mas estivemos de fato trabalhando no espaço da saída do monitor, então todas variáveis de cor e iluminação que configuramos não estavam fisicamente corretas, mas mal pareciam (quase) certas no nosso monitor. Por esta razão, nós (e os artistas) geralmente definimos os valores de iluminação mais brilhantes do que deveriam ser (já que o monitor as escurece), que resulta em invalidar todos calculos feitos no espaço linear. Note que ambos o monitor (CRT) e o gráfico linear, começam e acabam na mesma posição: são os valores intermediários que são escurecidos pela tela.\nComo as cores são baseadas na saída do monitor, todos cálculos (de iluminação) intermediários no espaço linear são fisicamente incorretos. Isto se torna óbvio quando utilizamos algoritmos de iluminação mais avançados, como podemos ver a seguir:\n\nVocê pode ver que a correção gama, os valores de cor (atualizados) funcionam melhor juntos e áreas mais escuras apresentam mais detalhes. No geral, um imagem de melhor qualidade com algumas poucas modificações.\nSem corrigir corretamente o gama do monitor, a iluminação parece errada e artistas terão um trabalho difícil para obter resultados realísticos. A solução é aplicar a correção gama.\nCorreção Gama A ideia da correção gama é aplicar o inverso do gama do monitor a cor final de saída andes de apresentá-la na tela. Relembrando a curva do gama no gráfico observamos uma outra linha tracejada que é a inversa da curva gama do monitor. Multiplicamos cada uma das cores lineares de saída pela sua curva inversa (tornando-as mais brilhantes) e, logo que são mostradas no monitor, a curva gama do monitor é aplicada e as cores resultantes se tornam lineares. Essencialmente, nós deixamos as cores intermediarias mais brilhantes para que assim que o monitor as escureçam, os valores fiquem balanceados.\nVamos a outro exemplo. Temos novamente o vermelho escuro \\((0.5, 0.0, 0.0)\\). Antes de imprimi-lo na tela, aplicamos a curva de correção gama ao valor da cor. Cores lineares mostradas por um monitor são aproximadamente escaladas por uma potência de \\(2.2\\), então o inverso requer que as cores sejam escaladas a uma potência de \\(1/2.2\\). O vermelho escuro corrigido então se torna \\((0.5,0.0,0.0)^{1/2.2}=(0.5,0.0,0.0)^{0.45}=(0.73,0.0,0.0)\\). As cores corrigidas são então enviadas para o monitor e como resultado a cor é mostrada como \\((0.73,0.0,0.0)2.2=(0.5,0.0,0.0)\\). Você pode ver que usando a correção gama, o monitor agora finalmente mostra as cores como as configuramos linearmente na aplicação.\nUm valor gama de 2.2 é um valor padrão que aproximadamente estima o gama médio da maior parte dos monitores. O espaço de cor resultante do gama de valor 2.2 é chamado de espaço de cor sRGB (não 100% exato, mas próximo). Cada monitor tem sua própria curva gama, mas o gama de valor 2.2 gera bons resultados na maior parte dos monitores. Por essa razão, jogos normalmente permitem jogadores escolher a configuração para o gama do jogo já que este varia levemente entre monitores diferentes.\n Existem dois jeitos de se aplicar a correção gama a sua cena:\n Usar o suporte nativo da OpenGL de framebuffer sRGB Fazer a correção gama nós mesmos, no shader de fragmento.  A primeira opção é provavelmente a mais fácil, mas dá menos liberdade. Ao habilitar GL_FRAMEBUFFER_SRGB você diz a OpenGL para aplicar a correção gama aos pixels (do espaço de cor sRGB) antes de armazená-los no buffer de cor. O sRGB é um espaço de cor que corresponde aproximadamente a um gama \\(2.2\\) e um padrão para maioria dos dispositivos. Depois de habilitar GL_FRAMEBUFFER_SRGB, a OpenGL automaticamente aplica a correção gama após executar cada shader de fragmento para todos os framebuffer subsequentes, incluindo o framebuffer padrão.\nHabilitar GL_FRAMEBUFFER_SRGB é tão simples quanto chamar glEnable:\nglEnable(GL_FRAMEBUFFER_SRGB);  De agora em diante, as imagens renderizadas serão corrigidas e como isso é feito no hardware então é completamente de graça. Algo que você deve se lembrar neste método (e no outro também) é que a correção gama (também) transforma as cores do espaço linear para o espaço não linear, então é muito importante que você só aplique a correção no último passo. Se fizer a correção antes do último passo, todas operações posteriores nas cores resultantes irão trabalhar com valores incorretos. Por exemplo, se estiver usando múltiplos framebuffers, provavelmente vai querer que os resultados intermediários passados entre os framebuffers permaneçam no espaço linear e que a correção gama seja aplicada apenas ao último framebuffer antes de enviá-lo a tela do monitor.\nO segundo jeito requer um pouco mais de trabalho, mas também nos dá mais controle sobre as operações de correção gama. Aplicamos a correção gama no final de cada execução de shader de fragmento relevante para que as cores finais estejam corrigidas antes de enviá-las ao monitor:\nvoid main() { // do super fancy lighting in linear space  [...] // apply gamma correction  float gamma = 2.2; FragColor.rgb = pow(fragColor.rgb, vec3(1.0/gamma)); }  A ultima linha de código efetivamente aumenta cada componente individual da cor fragColor para 1.0/gamma, corrigindo a cor de saída deste shader de fragmento.\nUm problema com esse método é que para manter consistência você tem que aplicar a correção gama para cada shader de fragmento que contribui para a imagem final. Se você tiver uma dúzia de shaders de fragmento para múltiplos objetos, você deve adicionar o código para correção gama para cada um desses shaders. Uma solução mais fácil seria inserir um estágio de pós-processamento no seu loop de renderização e aplicar a correção gama no quadrilátero pós-processado como último passo, do qual você precisa fazer apenas uma vez.\nEssa única linha representa a implementação técnica da correção gama. Nem um pouco impressionante, mas existem algumas coisas extras que deve considerar ao fazer a correção gama.\nsRGB textures Pelo fato dos monitores aplicarem o gama ao exibir as cores, toda vez que você desenha, edita, ou pinta uma imagem no seu computador, você deve escolher cores baseando-se no que exerga em seu monitor. Isso significa que todas imagens que você cria ou edita não estão no espaço linear, mas sim no espaço sRGB (ou seja, ao dobrar um vermelho escuro na sua tela baseando-se no brilho perceptivel, não é igual a simplesmente dobrar o valor da componente vermelha).\nComo resultado, quando artistas de textura criam arte pela visão, todas os valores de texture estão no espaço sRGB, então se usarmos essas texturas na nossa aplicação devemos levar isso em conta. Antes de saber o que era correção gama, isso não era um problema, porque as textureas pareciam boas no espaço sRGB do qual era o mesmo com o qual trabalhávamos; as texturas eram exibidas exatamente do jeito que são. Entretanto, agora que estamos exibindo tudo no espaço linear, as cores dessas texturas ficarão estranhas como na imagem a seguir:\n\nA imagem da textura é muito mais brilhante e isso acontece porque esta sendo corrigida duas vezes! Pense sobre isto, quando criamos uma imagem baseando-se no que enxergamos no monitor, efetivamente corrigimos os valores de cor da imagem de tal forma que pareça certa no monitor. Mas como aplicamos a correção gama ao renderizar, a imagem acaba ficando muito brilhante.\nPara resolver este problema, deveríamos assegurar que os artistas de textura trabalhassem no espaço linear. Mas, já que é mais fácil trabalhar no espaço sRGB e a maioria das ferramentas nem suportam direito texturização linear, esta solução não é muito boa.\nA solução mais plausível é recorrigir ou transformar essas texturas sRGB para o espaço linear antes de fazer qualquer cálculo com suas cores. Podemos fazer assim:\nfloat gamma = 2.2; vec3 diffuseColor = pow(texture(diffuse, texCoords).rgb, vec3(gamma));  Para fazer isso com cada texture no espaço sRGB dá muito trabalho. Por sorte a OpenGL nos oferece uma outra solução para os nossos problemas ao nos permitir os formatos internos de textura GL_SRGB e GL_SRGB_ALPHA.\nSe criarmos uma textura em OpenGL com qualquer um desses dois formatos de textura sRGB, a OpenGL vai automaticamente corrigir as cores para o espaço linear assim que forem usadas, nos possibilitando trabalhar no espaço linear. Podemos especificar uma textura como sRGB desse jeito:\nglTexImage2D(GL_TEXTURE_2D, 0, GL_SRGB, width, height, 0, GL_RGB, GL_UNSIGNED_BYTE, data);  Se você também quiser incluir componentes alfa na sua textura terá então que especificar o formato interno da textura como GL_SRGB_ALPHA.\nDevemos tomar cuidado ao especificar texturas no espaço sRGB já que nem todas texturas estarão de fato no espaço sRGB. Texturas utilizadas para colorir objetos (como texturas difusas) são quase sempre do tipo sRGB. Texturas usadas para recuperar parâmetros de iluminação (como mapas especulares e mapas de normais) estão quase sempre no espaço linear, então se configurássemos estas texturas como sRGB a iluminação vai ficar bem estranha. Seja cuidadoso com qual texturas configurar como sRGB.\nCom as texturas difusas definidas como sRGB você terá o resultado visual esperado, mas dessa vez a correção gama ocorre apenas uma vez.\nAtenuação Outra coisa que fica diferente com a correção gama é a atenuação da iluminação. No mundo real físico, a iluminação atenua aproximadamente inversamente proporcional a distancia quadrada da fonte de luz. Em português normal, isso significa que a força da luz reduz quadraticamente com a distancia a fonte de luz, como a seguir:\nfloat attenuation = 1.0 / (distance * distance);  Todavia, quando usamos essa equação o efeito de atenuação é normalmente muito forte, dando as luzes um raio pequeno que não parece fisicamente correto. Por esta razão outras funções de atenuação são utilizadas (como discutimos no capítulo de iluminação básica) que dão muito mais controle, ou o equivalente linear pode ser usado:\nfloat attenuation = 1.0 / distance;  O equivalente linear dá resultados mais plausíveis comparados a sua variante quadrática se a correção gama, mas quando habilitamos a correção gama a atenuação linear parece muito fraca e a atenuação quadrática fisicamente correta de repente oferece melhores resultados. A imagem abaixo mostras as diferenças:\n\nO porquê desta diferença é que as funções de atenuação da luz mudam o brilho, e como não estávamos visualizando nossa cena no espaço linear, escolhemos as funções de atenuação que pareciam melhores no nosso monitor, mas não eram fisicamente corretas. Pense na função de atenuação quadrática: se usássemos esta função sem a correção gama, a função se torna: \\((1.0/distance^2)^{2.2}\\) ao exibirmos no monitor. Isso cria uma atenuação muito maior do que originalmente teríamos antecipado. Isso também explica porque o equivalente linear faz muito mais sentido sem a correção gama já que se torna \\((1.0/distance)^{2.2}=1.0/distance^{2.2}\\), a qual se assemelha muito mais com sua equivalente física.\nA função de atenuação mais avançada discutida no capítulo iluminação básica ainda tem seu lugar em cenas com gama corrigido dado que ela dá mais controle sobre a atenuação exata (mas claro que requer diferentes parâmetros em uma cena com gama corrigido).\n Você pode encontrar o código fonte desta cena demo simples aqui. Pressionando a barra de espaço trocamos entre uma cena com gama corrigido e uma cena sema a correção gama, com ambas cenas utilizando suas texturas e atenuação equivalentes. Não é a demonstração mais incrível, mas mostra como aplicar todas técnicas.\nEm suma, a correção gama nos possibilita fazer todos nossos cálculos de shader/iluminação no espaço linear. Como o espaço linear faz sentido no mundo físico, a maioria das equações físicas passam a dar bons resultados (como uma atenuação real de luz). Quanto mais avançada sua iluminação se torna, mais fácil é conseguir resultados bonitos (e realísticos) com a correção gama. É por isso que também se é aconselhável que se ajuste os parametros de iluminacao só depois que tiver a correção gama no lugar.\nRecursos adicionais  O que todo programador deveria saber sobre gama: um artigo aprofundado e muito bem escrito por John Novak sobre correção gama. www.cambridgeincolour.com: mais sobre gama e correção gama. www.wolfire.com: post por David Rosen sobre os benefícios da correção gama na renderização de gráficos. renderwonk.com: algumas considerações práticas extras.  "
},
{
	"uri": "https://filipecn.github.io/aprendaopengl/",
	"title": "home",
	"tags": [],
	"description": "",
	"content": "APRENDA OPENGL Bem vindo(a) :)\nEste website é uma tradução direta e quase inteiramente fiel do website https://learnopengl.com/, originalmente escrito por Joey de Vries. O site oferece o conteúdo original do livro Learn OpenGL - Graphics Programming, disponível de graça no próprio site e que também tem sua versão impressa vendida por aí.\nEste conteúdo é direcionado para quem quer aprender programação gráfica por meio da OpenGL. Mas também para aqueles(as) que quiserem aprender um pouco de Computação Gráfica na prática.\nA intenção aqui é ajudar quem tem dificuldades em ler textos em inglês ou simplesmente se sente mais confortável com a língua portuguesa.\nComo não sou nenhum expert em inglês, e MENOS ainda em português, espere encontrar algum erro de vez em quando. Não hesite em apontar os erros, dar sugestões, fazer críticas e até desabafos\u0026hellip; sou todo ouvidos! Acredito sinceramente que mesmo com alguns errinhos aqui e ali, o aprendizado e informação transmitidos por esta tradução não serão afetados, então nada para se preocupar!\nComo é feita a tradução? Com sono, muito sono. De madrugada. MAS juro que é com 100% de cérebro que me sobra nessas horas.\nNo início traduzia a partir do zero, frase por frase, escrevendo tudo. Mas de repente ficou claro pra mim que iria demorar uns 100 anos pra escrever tudo na minha velocidade e cansaço! Então tomei a única decisão que alguém preguiçoso como eu tomaria: usar o google translate. Mas logo me deparei com frases do tipo \u0026ldquo;We only want to use the first 3 floats of each vertex\u0026rdquo; sendo traduzidas para \u0026ldquo;Nós só quer usar os primeiros 3 carros alegóricos de cada vértice\u0026rdquo; (lágrimas). No fim o trabalho árduo continuou, mas pelo menos agora dou risada no processo.\nApesar de todas conjugações erradas, frases sem sentido nenhum e termos malucos, ainda é mais rápido revisar o texto todo esquisito e consertá-lo do que escrever todo ele do zero, então google translate é o melhor pior esquema que pude encontrar nos 5 minutos que lutei contra meu dilema.\nQuero ajudar! Caso deseje me ajudar nessa empreitada, ou simplesmente achou o texto/site tão ruins que tem vontade de corrigir por conta própria, ou qualquer outra razão que tenha, eu serei eternamente grato!\nPara contribuir você pode:\n  Contribuir diretamente no repositório do github (clonando e submetendo mudanças). É realmente muito simples, basta criar/modificar os arquivos markdown que estão na pasta content na pasta raiz do repositório. O sistema de pastas/arquivos dentro de content estão organizadas de um jeito bem fácil e intuitivo.\n  Modificar diretamente no github, clicando no botão Edit this page que estará no canto superior de cada página com o conteúdo traduzido. Este botão te leva diretamente pro arquivo markdown dentro do repositório do github e lá você pode fazer suas modificações e submetê-las.\n  Para aqueles(as) que quiserem se basear nas traduções do google translate eu lhes dou de presente o translate.py (que também se encontra na raiz do repositório), script mão na roda para traduzir o post inteiro e gerar o markdown de saída. Por exemplo, digamos que você queira traduzir a seção de materiais https://learnopengl.com/Lighting/Materials e salvar a tradução direto no seu respectivo arquivo (que neste caso deve ser content/iluminacao/materiais/_index.md). O script aceita vários argumentos, mas apenas dois são realmente necessários: o link do post e o arquivo de saída. Bastaria então fazer algo do tipo (partindo da pasta raiz do repositório):\npython3 translate.py https://learnopengl.com/Lighting/Materials content/iluminacao/materiais/_index.md e esperar a porcentagem chegar a 100%. Não precisa se preocupar com as imagens e links contidos no texto original, porque o script cuida deles pra você!\nEsse script pode falhar, e provavelmente vai \u0026hellip;. com certeza :). Todo texto que ele não consegue traduzir ele coloca o texto em inglês original no lugar, por isso mais um motivo para realmente revisar tudo. E não preciso reforçar o fato de que muitas frases vêm erradas, sem sentido\u0026hellip; então a revisão é realmente necessária!\n Status A seguir, temos a lista de todas seções do site com cores indicando quais já foram traduzidas e revisadas  , quais estão sendo traduzidas no momento , quais já foram traduzidas mas precisam de revisão e por fim quais ainda nem foram traduzidas  :\n   Capítulo Seções     Introdução      Ponto de Partida  Criando uma Janela , Olá Janela , Olá Triângulo , Shaders  , Texturas  , Transformações , Sistemas de Coordenadas  , Câmera  , Revisão     Iluminação  Cores , Iluminação Básica , Materiais , Mapas de Iluminação  , Lançadores de Luz  , Múltiplas Luzes  , Revisão     Carregamento de Modelos   Assimp  , Malha  , Modelo     OpenGL Avançada   Teste de Profundidade  , Teste de Estêncil  , Mistura  , Seleção de Faces  , Framebuffers  , Mapas Cúbicos  , Dados Avançados  , GLSL Avançada  , Shader de Geometria  , Instanciação  , Anti-Aliasing     Iluminação Avançada  Iluminação Avançada , Correção Gama , Sombras - Mapeamento de Sombra  , Sombras - Sombras Pontuais  , Mappeamento de Normais  , Mapeamento de Paralaxe  , HDR  , Bloom  , Deferred Shading  , SSAO     PBR  Teoria , Iluminação  , IBL - Irradiância Difusa  , IBL - IBL Especular     Na Prática   Depurando  , Renderização de Texto  , Jogo 2D  , - Breakout  , - Preparação  , Renderizando Sprites  , - Fases  , - Colisões  , -- Bola  , -- Detecção de Colisão  , -- Resolução de Colisão  , - Particulas  , - Pós-Processamento  , - Powerups  , - Áudio  , - Renderizar Texto  , - Considerações Finais     Repositório do Código          Estatística Contagem     já foram traduzidas e revisadas   1   sendo traduzidas no momento  6   já foram traduzidas mas precisam de revisão  8   ainda nem foram traduzidas   55   total 70    "
},
{
	"uri": "https://filipecn.github.io/aprendaopengl/iluminacao/iluminacao_basica/",
	"title": "Iluminação Básica",
	"tags": [],
	"description": "",
	"content": "Post Original\nIluminação no mundo real é extremamente complicada e depende de muitos fatores, algo que não podemos dar ao luxo de calcular com o poder de processamento limitado que temos. Iluminação em OpenGL é, portanto, feita com base em aproximações da realidade, utilizando modelos simplificados que são muito mais fáceis de se processar e se parece relativamente similar. Estes modelos de iluminação são baseados na física da luz como a entendemos. Um desses modelos é chamado o modelo de iluminação Phong  . O modelo de iluminação Phong é composto de 3 componentes: iluminações ambiente, difusa e especular. Abaixo você pode ver o que esses componentes de iluminação fazem sozinhos e combinados:\n\n Iluminação Ambiente  : mesmo quando está escuro, geralmente ainda há alguma luz em algum lugar no mundo (a lua, uma luz distante), então os objetos quase nunca estão completamente no escuro. Para simular isso, usamos uma constante de iluminação ambiente que sempre dá ao objeto um pouco de cor.\n Iluminação Difusa  : simula o impacto direcional que um objeto luz tem sobre um objeto. Este é a componente mais visualmente significativa do modelo de iluminação. Quanto mais uma parte de um objeto se alinha a fonte de luz, mais clara se torna.\n Iluminação Especular  : simula o ponto brilhante de uma luz que aparece em objetos brilhantes. Highlights especulares são mais inclinados à cor da luz do que a cor do objeto.\n  Para criar cenas visualmente interessantes queremos, pelo menos, simular estas 3 componentes de iluminação. Vamos começar com a mais simples: iluminação ambiente.\nIluminação Ambiente (Ambient lighting) A luz geralmente não vem de uma única fonte, mas a partir de muitas fontes de luz espalhadas ao redor de nós, mesmo quando elas não são imediatamente visíveis. Uma das propriedades da luz é que ela pode se espalhar e rebater em muitas direções, atingindo pontos que não são diretamente visíveis; a luz pode, assim, refletir sobre outras superfícies e ter um impacto indireto sobre a iluminação de um objeto. Algoritmos que levam isso em consideração são chamados algoritmos de iluminação global  ( global illumination  , mas estes são complicados e caros para calcular.\nUma vez que não somos grandes fãs de algoritmos complicados e caros, vamos começar usando um modelo muito simplista de iluminação global, ou seja, iluminação ambiente  . Como você viu na seção anterior, usamos uma pequena cor (de luz) constante que podemos adicionar à cor final resultante dos fragmentos do objeto, fazendo parecer que há sempre alguma luz difusa, mesmo quando não há uma fonte de luz direta.\nAdicionando iluminação ambiente a cena é realmente fácil. Pegamos a cor da luz, a multiplicamos por um pequeno fator constante de ambiente, multiplicamos então com a cor do objeto, e que o usamos o resultado como cor do fragmento no shader do cubo:\nvoid main() { float ambientStrength = 0.1; vec3 ambient = ambientStrength * lightColor; vec3 result = ambient * objectColor; FragColor = vec4(result, 1.0); }  Se você executar o programa agora, vai perceber que a primeira fase de iluminação é agora aplicada ao objeto com sucesso. O objeto está bastante escuro, mas não totalmente, uma vez que a iluminação ambiente é aplicada (note que o cubo de luz não é afetado porque usamos um shader diferente). O resultado então é algo assim:\n\nIluminação Difusa (Diffuse lighting) A iluminação ambiente por si só não produz os resultados mais interessantes, mas a iluminação difusa no entanto vai começar a dar um impacto visual significativo sobre o objeto. A Iluminação difusa dá ao objeto mais brilho quanto mais perto os seus fragmentos estão alinhados com os raios de luz a partir de uma fonte de luz. Para lhe dar uma melhor compreensão dê uma olhada a seguinte imagem:\n\nNa esquerda, encontramos uma fonte de luz com um raio de luz dirigido a um único fragmento de nosso objeto. Precisamos medir em que ângulo o raio de luz atinge o fragmento. Se o raio de luz é perpendicular à superfície do objeto a luz tem o maior impacto. Para medir o ângulo entre o raio de luz e o fragmento usamos algo chamado um de vetor normal  ( normal vector  ), que é um vetor perpendicular à superfície do fragmento (aqui representada como uma seta amarela); nós vamos falar disso mais tarde. O ângulo entre os dois vetores pode então ser facilmente calculado com o produto escalar.\nVocê pode se lembrar do capítulo de transformações que, quanto menor o ângulo entre dois vetores unitários, mais o produto escalar é inclinado para um valor de 1. Quando o ângulo entre os dois vetores é de 90 graus, o produto escalar se torna 0. O mesmo se aplica a $\\theta$: quanto maior $\\theta$ é, menos impacto a luz deve ter na cor do fragmento.\nNote que para se obter (apenas) o cosseno do ângulo entre os dois vetores, iremos trabalhar com vetores unitários (vetores de comprimento $1$), de modo que precisamos garantir que todos os vetores são normalizados, caso contrário, o produto escalar nos retornará mais do que apenas o cosseno (veja transformações).\n O produto escalar resultante é, portanto, um escalar que podemos utilizar para calcular o impacto da luz sobre a cor do fragmento, resultando em fragmentos de diferente brilhos com base na sua orientação para a luz.\nEntão, o que precisamos para calcular a iluminação difusa é:\n Vetor normal: um vetor que é perpendicular à superfície do vértice.\n O raio de luz direcionado: um vetor de direção que é o vetor de diferença entre a posição da luz e a posição do fragmento. Para calcular este raio de luz, precisamos do vetor posição da luz e vetor posição do fragmento.\n  Vetores Normais (Normal Vectors) Um vetor normal é um vetor (unitário) que é perpendicular à superfície de um vértice. Como um vértice, por si só, não tem superfície (é apenas um único ponto no espaço) temos que extrair um vetor normal usando seus vértices vizinhos e descobrir a superfície do vértice. Podemos usar um pequeno truque para calcular os vetores normais para todos os vértices do cubo usando o produto vetorial ( cross product ), mas já que um cubo 3D não é uma forma complicada podemos simplesmente adicioná-los manualmente para os dados de vértice. o array de dados de vértice atualizado pode ser encontrado [aqui]((/code_viewer.php?code=lighting/basic_lighting_vertex_data). Tente perceber que as normais são na verdade vetores perpendiculares à superfície de cada plano (um cubo consiste em 6 planos).\nComo adicionamos dados extra ao array de vértice devemos atualizar shader de vértice do cubo:\n#version 330 core layout (location = 0) in vec3 aPos; layout (location = 1) in vec3 aNormal; ...  Agora que nós adicionamos um vetor normal a cada um dos vértices e atualizamos o shader de vértice devemos atualizar os ponteiros de atributos vértice também. Note que o cubo de luz usa o mesmo array de vértices para seus dados de vértice, mas o shader da lâmpada não usa nenhum dos vetores normais recém-adicionados. Não temos que atualizar os shaders da lâmpada ou configurações de atributos, mas temos que, pelo menos, modificar os ponteiros de atributos de vértice para refletir o tamanho do novo array de vértice:\nglVertexAttribPointer(0, 3, GL_FLOAT, GL_FALSE, 6 * sizeof(float), (void*)0); glEnableVertexAttribArray(0);  Nós só queremos usar os primeiros 3 floats de cada vértice e ignorar os últimos 3 floats por isso só precisamos atualizar o parâmetro de stride para 6 vezes o tamanho de um float.\nPode parecer ineficiente usar dados de vértice que não são completamente utilizados pelo shader da lâmpada, mas os dados de vértice já estão armazenados na memória da GPU do objeto de modo que não temos que armazenar novos dados. Isso realmente faz com que seja mais eficiente em comparação com a alocação de um novo VBO especificamente para a lâmpada.\n Todos os cálculos de iluminação são feitos no shader de fragmento e por isso precisamos transmitir os vetores normais do shader vértice para o shader de fragmento. Vamos fazer isso:\nout vec3 Normal; void main() { gl_Position = projection * view * model * vec4(aPos, 1.0); Normal = aNormal; }  O que resta fazer agora é declarar a variável de entrada correspondente no shader de fragmento:\nin vec3 Normal;  Calculando a Cor Difusa Agora temos o vetor normal para cada vértice, mas ainda precisamos do vetor posição da luz e o vetor posição do fragmento. Como a posição da luz é uma única variável estática podemos declará-lo como um uniforme no shader de fragmento:\nuniform vec3 lightPos;  E, em seguida, atualizar o uniforme no loop de renderização (ou fora, uma vez que não muda a todo quadro ( frame ). Usamos o vetor lightPos declarado no capítulo anterior como a localização da fonte de luz difusa:\nlightingShader.setVec3(\u0026#34;lightPos\u0026#34;, lightPos);  Então a última coisa que precisamos é a verdadeira posição do fragmento. Nós vamos fazer todos os cálculos de iluminação no espaço de mundo, por isso queremos primeiramente uma posição de vértice que também está no espaço de mundo. Podemos fazer isso através da multiplicação do atributo de posição vértice com apenas a matriz de modelo (não a matriz de visão nem a matriz de projeção) para transformá-lo em coordenadas espaciais de mundo. Isto pode ser facilmente realizado no shader de vértice, então vamos declarar uma variável de saída e calcular suas coordenadas espaciais de mundo:\nout vec3 FragPos; out vec3 Normal; void main() { gl_Position = projection * view * model * vec4(aPos, 1.0); FragPos = vec3(model * vec4(aPos, 1.0)); Normal = aNormal; }  E, por último adicione a variável de entrada correspondente ao shader de fragmento:\nin vec3 FragPos;  Esta variável in irá ser interpolada a partir dos 3 vetores de posição de mundo do triângulo para formar o vetor FragPos , que é a posição de mundo por fragmento. Agora que todas as variáveis ​​necessárias estão definidas podemos começar os cálculos de iluminação.\nA primeira coisa que precisamos calcular é o vetor de direção entre a fonte luminosa e a posição do fragmento. Da seção anterior, sabemos que o vetor direção da luz é o vetor diferença entre o vetor posição da luz e vetor posição do fragmento. Como você pode se lembrar do capítulo de transformações podemos facilmente calcular essa diferença subtraindo os dois vetores um do outro. Nós também queremos garantir que todos os vetores relevantes acabem como vetores unitários, então normalizamos tanto a normal e o vetor direção resultantes:\nvec3 norm = normalize(Normal); vec3 lightDir = normalize(lightPos - FragPos);  Ao calcular a iluminação, normalmente não nos preocupamos com a magnitude de um vetor ou de sua posição; só nos preocupamos com a sua direção. Portanto, quase todos os cálculos são feitos com vetores unitários uma vez que isso os simplifica (como o produto escalar). Portanto, ao fazer cálculos de iluminação, certifique-se sempre de normalizar os vetores relevantes para assegurar que eles são vetores unitários de verdade. Esquecendo-se de normalizar um vetor é um erro bastante comum.\n Em seguida, precisamos calcular o impacto da luz difusa sobre o fragmento atual, tomando o produto escalar entre os vetores norma e lightDir . O valor resultante é então multiplicado com a cor da luz para se obter a componente difusa, resultando em uma componente difusa mais escura quanto maior o ângulo entre os dois vetores:\nfloat diff = max(dot(norm, lightDir), 0.0); vec3 diffuse = diff * lightColor;  Se o ângulo entre os dois vetores é superior a 90 graus, então o resultado do produto escalar será negativo e vamos acabar com uma componente difusa negativa. Por essa razão, usamos a função max  que retorna o maior de seus parâmetros para garantir que a componente difusa (e, portanto, as cores) nunca se torne negativa. A iluminação para cores negativas não é definida de fato, por isso é melhor ficar longe dela, a menos que você seja um daqueles artistas excêntricos.\nAgora que temos tanto uma componente ambiente e uma componente difusa, somamos as duas cores e, em seguida, multiplicamos o resultado com a cor do objeto para obter a cor de saída do fragmento:\nvec3 result = (ambient + diffuse) * objectColor; FragColor = vec4(result, 1.0);  Se o sua aplicação (e shaders) compilaram com sucesso você deve ver algo assim:\n\nVocê pode ver que com a iluminação difusa o cubo começa a parecer um cubo real novamente. Tente visualizar os vetores normais em sua cabeça e mover a câmera ao redor do cubo para ver que quanto maior o ângulo entre o vetor normal e vetor de direção da luz, mais escuro fica o fragmento.\nSe estiver travado, sinta-se livre para comparar o seu código-fonte com o código-fonte completo aqui.\nUma Última Coisa Na seção anterior, passamos o vetor normal diretamente do shader de vértice ao shader de fragmento. No entanto, os cálculos no shader de fragmento são todos feitos no espaço de mundo, por isso não deveríamos transformar os vetores normais a coordenadas espaciais de mundo também? Basicamente sim, mas não é tão simples como simplesmente multiplicá-los com a matriz de modelo.\nPrimeiro de tudo, vetores normais são apenas vetores de direção e não representam uma posição específica no espaço. Em segundo lugar, os vetores normais não têm uma coordenada homogênea (a componente w de uma posição de vértice). Isto significa que translações não deveriam ter nenhum efeito sobre os vetores normais. Portanto, se queremos multiplicar os vetores normais com uma matriz de modelo, devemos remover a parte de translação da matriz pegando a matriz 3x3 superior esquerde da matriz de modelo (note que nós também poderíamos definir o componente w de um vetor normal para 0 e multiplicar com a matriz de 4x4).\nEm segundo lugar, se a matriz de modelo realizasse uma escala não-uniforme, os vértices seriam alterados de tal maneira que o vetor normal não mais seria perpendicular à superfície. A figura seguinte mostra o efeito que uma matriz de modelo (com a escala não uniforme) tem sobre um vetor normal:\n\nSempre que aplicamos uma escala não-uniforme (nota: uma escala uniforme só muda magnitude da normal, não a sua direção, o que é facilmente corrigida com sua normalização) os vetores normais não são mais perpendiculares à superfície correspondente, o que distorce a iluminação.\nO truque para corrigir este comportamento é a utilização de uma matriz de modelo diferente especificamente adaptada para vetores normais. Esta matriz é chamada de matriz normal  ( normal matrix  ) e usa algumas operações algébricas lineares para remover o efeito de escalar de forma errada os vetores normais. Se você quiser saber como essa matriz é calculada sugiro o seguinte artigo.\nA matriz normal é definida como \u0026quot;a transposta da inversa da parte 3x3 superior esquerda da matriz de modelo\u0026quot;. Ufa, é um bocado de palavra aí e se você realmente não entender o que isso significa, não se preocupe; não discutimos inversas e matrizes de transposição ainda. Note que a maioria dos recursos definem a matriz normal como calculada a partir da matriz de modelo-visão, mas uma vez que estamos trabalhando no espaço de mundo (e não no espaço de visão), vamos derivá-la a partir da matriz de modelo.\nNo shader de vértice podemos gerar a matriz normal, utilizando as funções inverse  e transpose  no shader de vértice que funcionam em qualquer tipo de matriz. Note que damos um cast na matriz para uma matriz 3x3 para garantir que ela perca suas propriedades de translação e para que possamos multiplicá-la com o vetor normal vec3:\nNormal = mat3(transpose(inverse(model))) * aNormal;  Inverter matrizes é uma operação custosa para os shaders, por isso sempre que possível tente evitar fazer operações de inversas, uma vez que tem que ser feito em cada vértice da sua cena. Para fins de aprendizagem isso é bom, mas para uma aplicação eficiente que você provavelmente vai querer calcular a matriz normal na CPU e enviá-la para os shaders via um uniforme antes de desenhar (assim como a matriz de modelo).\n Na seção de iluminação difusa a iluminação ficou boa porque nós não fizemos nenhuma escala no objeto, de modo que não era realmente necessário usar uma matriz normal e nós poderíamos ter apenas multiplicado as normais com a matriz de modelo. No entanto, se você estiver fazendo uma escala não uniforme, é essencial que você multiplique seus vetores normais com a matriz normal.\nIluminação Especular (Specular Lighting) Se você não já estiver esgotado(a) com toda essa conversa de iluminação podemos terminar o modelo de iluminação Phong adicionando reflexos especulares.\nSemelhante a iluminação difusa, a iluminação especular é baseada no vetor de direção da luz e os vetores normais do objeto, mas, desta vez, também na direção da visão, isto é, na direção em que o jogador está olhando para o fragmento. A iluminação especular baseia-se nas propriedades de reflexão das superfícies. Se pensarmos na superfície do objeto como um espelho, a iluminação especular é mais forte onde víssemos a luz refletida na superfície. Você pode ver este efeito na imagem a seguir:\n\nCalculamos um vetor de reflexão, refletindo a direção da luz em torno do vetor normal. Em seguida, calculamos a distância angular entre este vector de reflexão e a direção da visão. Quanto mais próximo o ângulo entre eles, maior o impacto da luz especular. O efeito resultante é que vemos um pouco de highlight quando nós estamos olhando para a direção da luz refletida através da superfície.\nO vetor de visão (da câmera) é a variável extra que precisamos para a iluminação especular, a qual podemos calcular usando a posição no espaço de mundo do espectador e a posição do fragmento. Em seguida, calculamos a intensidade da especular, multiplicamos esta intensidade com a cor da luz e adicionamos as componentes de ambiente e difusa.\nEscolhemos fazer os cálculos de iluminação no espaço de mundo, mas a maioria das pessoas tende a preferir fazer a iluminação no espaço de visão ( view space . Uma vantagem do espaço de visão é que a posição do espectador é sempre na origem (0,0,0), de modo que você já tem a posição do espectador de forma gratuita. No entanto, eu acho o cálculo de iluminação no espaço de mundo mais intuitivo para fins de aprendizagem. Se você ainda quiser calcular a iluminação no espaço de visão, você terá que transformar todos os vetores relevantes com a matriz de visão (não se esqueça de mudar a matriz normal também).\n Para obter as coordenadas espaciais de mundo do espectador nós simplesmente tomamos o vetor posição da câmera (que é o espectador é claro). Então, vamos adicionar outro uniforme para o shader de fragmento e passar o vetor posição da câmera para o shader:\nuniform vec3 viewPos; lightingShader.setVec3(\u0026#34;viewPos\u0026#34;, camera.Position);  Agora que temos todas as variáveis ​​necessárias podemos calcular a intensidade especular. Primeiro vamos definir um valor de intensidade especular para dar uma cor meio brilhante ao brilho especular para que ele não tenha muito impacto:\nfloat specularStrength = 0.5;  Se escolhêssemos o valor de 1.0f teríamos uma componente especular muito brilhante que é um pouco demais para um cubo coral. No próximo capítulo vamos falar sobre como definir corretamente todas essas intensidades de iluminação e como elas afetam os objetos. Em seguida calculamos o vetor direção da câmera e o vetor correspondente de reflexão ao longo do eixo normal:\nvec3 viewDir = normalize(viewPos - FragPos); vec3 reflectDir = reflect(-lightDir, norm);  Note que negamos o vetor lightDir. A função reflect espera que o primeiro vetor apontar da fonte de luz para a posição do fragmento, mas o vetor lightDir está apontando na direção contrária: a partir do fragmento para a fonte de luz (isto depende da ordem de subtração anterior quando calculamos o vetor lightDir). Para ter certeza de que obteremos o vetor reflect correto, invertemos a sua direção, negando o vetor lightDir primeiro. O segundo argumento espera um vetor normal, então fornecemos o vetor normal normalizado.\nEntão o que resta a fazer é realmente calcular a componente especular. Isto é feito com a seguinte fórmula:\nfloat spec = pow(max(dot(viewDir, reflectDir), 0.0), 32); vec3 specular = specularStrength * spec * lightColor;  Calculamos primeiro o produto escalar entre a direção da câmera e a direção de reflexão (e certifique-se que este vetor não é negativo) e, em seguida, elevamos o resultado a potência de 32. Este valor de 32 é o valor de brilhosidade  ( shininess  do highlight. Quanto maior o valor de brilhosidade de um objeto, mais ele reflete adequadamente a luz em vez de espalhá-la ao seu redor e, portanto, menor o highlight se torna. Abaixo você pode ver uma imagem que mostra o impacto visual de diferentes valores de brilhosidade:\n\nNós não queremos que a componente especular chame muito a atenção, então mantemos o expoente de 32. A única coisa que resta a fazer é adicioná-la as componentes de ambiente e difusa e multiplicar o resultado combinando com a cor do objeto:\nvec3 result = (ambient + diffuse + specular) * objectColor; FragColor = vec4(result, 1.0);  Nós agora calculamos todos as componentes de iluminação do modelo de iluminação Phong. Com base no seu ponto de vista você deve ver algo como isto:\n\nVocê pode encontrar o código-fonte completo aqui.\nNos primórdios dos shaders de iluminação, os desenvolvedores implementavam o modelo de iluminação Phong no shader de vértice. A vantagem de fazer iluminação no shader de vértice é que é muito mais eficiente, uma vez que geralmente existem muito menos vértices do que fragmentos, de modo que os cálculos de iluminação são feitos com menos frequência. No entanto, o valor da cor resultante no shader de vértice é o da iluminação de cor resultante somente no vértice e os valores de cor dos fragmentos vizinhos são, em seguida, o resultado de cores de iluminação interpolados. O resultado era que a iluminação não era muito realista, a menos que fossem utilizadas grandes quantidades de vértices:\n\nQuando o modelo de iluminação Phong é implementado no shader de vértice ele é chamado shading de Gouraud  ( Gouraud shading  ) em vez de Phong shading  . Note-se que devido à interpolação a iluminação parece um pouco fora do lugar. O Phong shading dá resultados iluminação muito mais suaves.\n A partir de agora você deve estar começando a ver quão poderosos são os shaders. Com pouca informação, shaders são capazes de calcular como a iluminação afeta as cores de fragmento para todos os nossos objetos. Nos próximos capítulos vamos nos aprofundar muito mais no que podemos fazer com o modelo de iluminação.\nExercícios  A nossa fonte de luz é uma fonte de luz estática muito chata. Tente mover a fonte de luz ao redor da cena ao longo do tempo usando um sin  ou cos  . Observando a mudança de iluminação ao longo do tempo lhe dará uma boa compreensão do modelo de iluminação de Phong: solução.\n Brinque com diferentes valores para as componentes de ambiente, difusa e especular e veja como impactam o resultado. Também experimente com o fator de brilhosidade. Tente compreender por que certos valores têm uma certa saída visual.\n Faça shading de Phong no espaço de visão em vez do espaço de mundo: solução.\n Implemente o shading de Gouraud em vez do shading de Phong. Se você fez as coisas direito a iluminação deve parecer um pouco fora do lugar (especialmente os reflexos especulares) com o objeto do cubo. Tente pensar por que parece tão estranho: solução\n  "
},
{
	"uri": "https://filipecn.github.io/aprendaopengl/iluminacao/materiais/",
	"title": "Materiais",
	"tags": [],
	"description": "",
	"content": "Post Original\nNo mundo real, cada objeto tem uma reação diferente à luz. objetos de aço são muitas vezes mais brilhante do que um vaso de argila, por exemplo, e um recipiente de madeira não reage da mesma à luz como um recipiente de aço. Alguns objetos refletem a luz sem muita dispersão resultando em pequenos reflexos especulares e outros espalhar um monte dando o destaque um raio maior. Se quisermos simular vários tipos de objetos em OpenGL temos que definir as propriedades do material específico para cada superfície.\nNo capítulo anterior, definido um objecto e cor da luz para definir a saída visual do objecto, combinado com um componente de intensidade ambiente e especular. Quando se descreve uma superfície que pode definir uma cor de material para cada um dos 3 componentes de iluminação ambiente, difusa e especular iluminação. Ao especificar uma cor para cada um dos componentes nós temos o controle de grão fino sobre a saída de cor da superfície. Agora adicione um componente brilho aos 3 cores e temos todas as propriedades do material que precisamos:\n#version 330 core struct Material { vec3 ambient; vec3 diffuse; vec3 specular; float shininess; }; uniform Material material;  No sombreador fragmento que criar uma estrutura para armazenar as propriedades do material da superfície. Nós também pode armazená-los como valores uniformes individuais, mas armazená-los como um struct mantém-lo mais organizado. Nós primeiro definir o layout da estrutura e depois simplesmente declarar uma variável uniforme com a estrutura recém-criada como seu tipo.\nComo você pode ver, nós definimos um vetor de cor para cada um dos componentes da iluminação Phong. Os materiais define vector ambiente o que a cor da superfície reflecte sob iluminação ambiente; este é geralmente o mesmo que a cor da superfície. O vector de material difuso define a cor da superfície sob iluminação difusa. A cor é difusa (tal como a iluminação ambiente) ajustado a cor da superfície desejada. O material de vetor especular define a cor do realce especular sobre a superfície (ou possivelmente até mesmo refletir uma cor específica de superfície). Por último, os impactos brilho a dispersão / raio do realce especular.\nCom estes 4 componentes que definem o material de um objeto que pode simular muitos materiais do mundo real. Uma tabela como encontrado em shows devernay.free.fr uma lista das propriedades dos materiais que simulam materiais reais encontrados no mundo exterior. A seguinte imagem mostra o efeito de vários desses valores materiais do mundo real tem na sua cubo:\n(http://devernay.free.fr/cours/opengl/materials.html)\n\nComo você pode ver, especificando corretamente as propriedades do material de uma superfície que parece mudar a percepção que temos do objeto. Os efeitos são claramente visíveis, mas para os resultados mais realistas precisaremos substituir o cubo com algo mais complicado. Nos Carregando capítulos modelo vamos discutir formas mais complicadas.\n(https://learnopengl.com/Model-Loading/Assimp)\nDescobrir as configurações de material certo para um objeto é um feito difícil que requer principalmente a experimentação e muita experiência. Não é incomum para destruir completamente a qualidade visual de um objeto por um material extraviado.\nVamos tentar implementar um sistema de tal material nos shaders.\nSetting materials Nós criamos uma estrutura de material uniforme no shader de fragmento tão próxima que queremos mudar os cálculos de iluminação para cumprir com as novas propriedades dos materiais. Uma vez que todas as variáveis ​​relevantes são armazenados em uma estrutura que pode acessá-los a partir do uniforme de material:\nvoid main() { // ambient  vec3 ambient = lightColor * material.ambient; // diffuse  vec3 norm = normalize(Normal); vec3 lightDir = normalize(lightPos - FragPos); float diff = max(dot(norm, lightDir), 0.0); vec3 diffuse = lightColor * (diff * material.diffuse); // specular  vec3 viewDir = normalize(viewPos - FragPos); vec3 reflectDir = reflect(-lightDir, norm); float spec = pow(max(dot(viewDir, reflectDir), 0.0), material.shininess); vec3 specular = lightColor * (spec * material.specular); vec3 result = ambient + diffuse + specular; FragColor = vec4(result, 1.0); }  Como você pode ver que agora acessar todas as propriedades do material de struct onde quer que precisamos deles e desta vez calcular a cor de saída resultante com a ajuda de cores do material. Cada um dos atributos materiais do objeto são multiplicados com os respectivos componentes de iluminação.\nPodemos definir o material do objeto no aplicativo, definindo os uniformes apropriados. A estrutura em GLSL, contudo, não é especial em qualquer consideração ao definir uniformes; uma estrutura realmente só funciona como um espaço de nomes de variáveis ​​uniformes. Se queremos preencher a estrutura que terá que definir os uniformes individuais, mas prefixado com o nome do struct:\nlightingShader.setVec3(\u0026#34;material.ambient\u0026#34;, 1.0f, 0.5f, 0.31f); lightingShader.setVec3(\u0026#34;material.diffuse\u0026#34;, 1.0f, 0.5f, 0.31f); lightingShader.setVec3(\u0026#34;material.specular\u0026#34;, 0.5f, 0.5f, 0.5f); lightingShader.setFloat(\u0026#34;material.shininess\u0026#34;, 32.0f);  Vamos definir o ambiente e difusa componente para a cor que gostaria que o objeto a tem e definir o componente especular do objeto para uma cor meio-brilhante; não queremos que o componente especular a ser muito forte. Nós também manter o brilho em 32.\nAgora podemos influenciar facilmente o material do objeto da aplicação. Executando o programa dá-lhe algo como isto:\n\nRealmente não olhar embora certo?\nLight properties O objeto é muito brilhante. A razão para o objeto a ser demasiado brilhante é que o ambiente, difusa e cores especulares são refletidas com força total a partir de qualquer fonte de luz. As fontes de luz também têm diferentes intensidades para o seu ambiente, difusa e componentes especulares respectivamente. No capítulo anterior, nós resolvemos isso, variando as intensidades ambiente e especular com um valor de força. Queremos fazer algo semelhante, mas desta vez especificando vetores de intensidade para cada um dos componentes de iluminação. Se tivéssemos visualizar LightColor como vec3 (1.0) o código ficaria assim:\nvec3 ambient = vec3(1.0) * material.ambient; vec3 diffuse = vec3(1.0) * (diff * material.diffuse); vec3 specular = vec3(1.0) * (spec * material.specular);  Assim, cada propriedade do material do objeto é retornado com plena intensidade para cada um dos componentes da luz. Estes valores vec3 (1.0) pode ser influenciada individualmente, bem como para cada fonte de luz e este é geralmente o que queremos. Neste momento, o componente do ambiente do objecto é influenciar totalmente a cor do cubo. O componente de ambiente não deve realmente ter um impacto tão grande sobre a cor final para que possamos restringir a cor ambiente, definindo intensidade ambiente da luz para um valor mais baixo:\nvec3 ambient = vec3(0.1) * material.ambient;  Podemos influenciar o difuso e intensidade especular da fonte de luz da mesma forma. Isto é muito semelhante ao que fizemos no capítulo anterior; você poderia dizer que já criou algumas propriedades de luz para influenciar cada componente de iluminação individualmente. Nós vamos querer criar algo semelhante para a estrutura material para as propriedades de luz:\n(https://learnopengl.com/Lighting/Basic-Lighting)\nstruct Light { vec3 position; vec3 ambient; vec3 diffuse; vec3 specular; }; uniform Light light;  Uma fonte de luz tem uma intensidade diferente para o seu ambiente, difusa e componentes especulares. A luz ambiente é geralmente definida como uma baixa intensidade, porque não queremos que a cor do ambiente a ser demasiado dominante. O componente difuso de uma fonte de luz é geralmente definida como a cor exata que gostaria de uma luz para ter; muitas vezes uma cor branca brilhante. O componente especular é normalmente mantida a vec3 (1.0) que brilha com intensidade máxima. Note que nós também adicionamos da luz vetor posição para a estrutura.\nAssim como com o uniforme de material que precisamos para atualizar o shader de fragmento:\nvec3 ambient = light.ambient * material.ambient; vec3 diffuse = light.diffuse * (diff * material.diffuse); vec3 specular = light.specular * (spec * material.specular);  Em seguida, deseja definir as intensidades de luz na aplicação:\nlightingShader.setVec3(\u0026#34;light.ambient\u0026#34;, 0.2f, 0.2f, 0.2f); lightingShader.setVec3(\u0026#34;light.diffuse\u0026#34;, 0.5f, 0.5f, 0.5f); // darken diffuse light a bit lightingShader.setVec3(\u0026#34;light.specular\u0026#34;, 1.0f, 1.0f, 1.0f);  Agora que nós modulada como as influências claras de material do objeto tenhamos uma saída visual que se parece muito com a saída do capítulo anterior. Desta vez, no entanto, tem o controle total sobre a iluminação e o material do objeto:\n\nAlterando os aspectos visuais de objetos é relativamente fácil agora. de deixar as coisas apimentar um pouco!\nDifferent light colors Até agora usamos cores claras apenas variar a intensidade de seus componentes individuais, escolhendo cores que variam do branco ao cinza ao preto, não afetando as cores reais do objeto (somente sua intensidade). Uma vez que agora têm acesso fácil às propriedades da luz que pode mudar suas cores ao longo do tempo para obter alguns efeitos muito interessantes. Uma vez que tudo já está configurado no shader de fragmento, mudando as cores da luz é fácil e imediatamente cria alguns efeitos funk:\n\nComo você pode ver, uma cor clara diferente influencia grandemente a saída de cores do objeto. Desde a cor da luz influencia diretamente as cores que o objeto pode refletir (como você pode se lembrar do capítulo cores) tem um impacto significativo sobre a saída visual.\n(https://learnopengl.com/Lighting/Colors)\nNós podemos facilmente mudar as cores da luz ao longo do tempo, alterando cores ambientais e difusos da luz através do pecado e glfwGetTime:\nglm::vec3 lightColor; lightColor.x = sin(glfwGetTime() * 2.0f); lightColor.y = sin(glfwGetTime() * 0.7f); lightColor.z = sin(glfwGetTime() * 1.3f); glm::vec3 diffuseColor = lightColor * glm::vec3(0.5f); glm::vec3 ambientColor = diffuseColor * glm::vec3(0.2f); lightingShader.setVec3(\u0026#34;light.ambient\u0026#34;, ambientColor); lightingShader.setVec3(\u0026#34;light.diffuse\u0026#34;, diffuseColor);  Experimente e experimentar com vários iluminação e valores materiais e ver como eles afetam a saída visual. Você pode encontrar o código-fonte do aplicativo aqui.\n(/code_viewer_gh.php?code=src/2.lighting/3.1.materials/materials.cpp)\nExercises você pode fazer com que a mudança da cor da luz muda a cor do objeto de cubo da luz?\nvocê pode simular alguns dos objetos do mundo real através da definição de seus respectivos materiais como vimos no início deste capítulo? Note-se que os valores do ambiente da tabela não são os mesmos que os valores difusos; eles não levaram intensidades de luz em conta. Para definir corretamente seus valores você teria que definir todas as intensidades de luz para vec3 (1.0) para obter o mesmo resultado: solução de recipiente de plástico ciano.\n(http://devernay.free.fr/cours/opengl/materials.html)\n(/code_viewer_gh.php?code=src/2.lighting/3.2.materials_exercise1/materials_exercise1.cpp)\n"
},
{
	"uri": "https://filipecn.github.io/aprendaopengl/ponto_de_partida/ola_triangulo/",
	"title": "Olá Triângulo",
	"tags": [],
	"description": "",
	"content": "Na OpenGL tudo está no espaço 3D, mas a tela ou janela é uma matriz 2D de pixels, então uma grande parte do trabalho da OpenGL é transformar todas as coordenadas 3D em pixels 2D que cabem na sua tela. O processo de transformação de coordenadas 3D em pixels 2D é gerenciado pelo pipeline gráfico  ( graphic pipeline  ) da OpenGL. O pipeline gráfico pode ser dividido em duas grandes partes: a primeira transforma suas coordenadas 3D em coordenadas 2D e a segunda transforma as coordenadas 2D em pixels coloridos. Neste capítulo, discutiremos brevemente o pipeline gráfico e como podemos usá-lo a nosso favor para criar pixels sofisticados.\nO pipeline gráfico recebe como entrada um conjunto de coordenadas 3D e as transforma em pixels 2D coloridos na tela. Ele pode ser dividido em várias etapas onde cada etapa requer a saída da etapa anterior como sua entrada. Todas essas etapas são altamente especializadas (têm uma função específica) e podem ser facilmente executadas em paralelo. Por causa de sua natureza paralela, as placas gráficas de hoje têm milhares de pequenos núcleos de processamento para processar rapidamente seus dados dentro do pipeline gráfico. Os núcleos de processamento executam pequenos programas na GPU para cada etapa do pipeline. Esses pequenos programas são chamados de shaders  .\nAlguns desses shaders são configuráveis, o que nos permite escrever nossos próprios shaders para substituir os shaders existentes. Isso nos dá um controle muito mais refinado sobre partes específicas do pipeline e, como elas são executadas na GPU, também podem economizar um valioso tempo de CPU. Os shaders são escritos em OpenGL Shading Language  (GLSL  ) e vamos nos aprofundar mais nisso no próximo capítulo.\nAbaixo você encontrará uma representação abstrata de todos os estágios do pipeline gráfico. Observe que as seções em azul representam seções onde podemos injetar nossos próprios shaders.\n\nComo você pode ver, o pipeline gráfico contém um número grande de seções em que cada uma lida com uma parte específica da conversão dos dados dos seus vértices em um pixel totalmente renderizado. Explicaremos resumidamente cada parte do pipeline de uma forma simplificada para fornecer uma boa visão geral de como o pipeline funciona.\nComo entrada do pipeline gráfico, passamos uma lista de três coordenadas 3D que devem formar um triângulo em um array  aqui chamado de Vertex Data; esse vertex data é uma coleção de vértices. Um vértice  ( vertex  ) é uma coleção de dados por coordenada 3D. Os dados de cada vértice são representados por atributos de vértice  ( vertex attributes  ) que podem conter quaisquer dados que desejarmos, mas para simplificar vamos assumir que cada vértice consiste em apenas uma posição 3D e algum valor de cor.\nPara que a OpenGL saiba o que fazer com sua coleção de coordenadas e valores de cor, você deve indicar a natureza dos tipos de renderização que deseja formar com os dados. Queremos os dados renderizados como uma coleção de pontos, uma coleção de triângulos ou talvez apenas uma longa linha? Essas dicas são chamadas de primitivas  (primitives  ) e são fornecidas a OpenGL ao chamar qualquer um dos comandos de desenho. Algumas dessas dicas são GL_POINTS , GL_TRIANGLES e GL_LINE_STRIP .\n A primeira parte do pipeline é o shader de vértice  ( vertex shader  ) que recebe como entrada um único vértice. O objetivo principal do shader de vértice é transformar coordenadas 3D em diferentes coordenadas 3D (mais sobre isso mais tarde) e também nos permite fazer algum processamento básico nos atributos de vértice.\nO estágio de montagem de primitivas  ( primitive assembly  ) recebe como entrada todos os vértices (ou vértice se GL_POINTS for escolhido) do shader de vértice que pertencem a uma primitiva e reúne todos os pontos para formar a primitiva fornecida; neste caso, um triângulo.\nA saída do estágio de montagem de primitivas é passada para o shader de geometria  ( geometry shader  ). O shader de geometria recebe como entrada uma coleção de vértices que formam uma primitiva e tem a capacidade de gerar outras formas, emitindo novos vértices para formar novas (ou outras) primitivas. Neste caso de exemplo, ele gera um segundo triângulo com a forma fornecida.\nA saída do shader de geometria é então passada para o estágio de rasterização  ( rasterization stage  ), onde mapeia a(s) primitiva(s) resultante(s) para os pixels correspondentes na tela final, resultando em fragmentos para o shader de fragmento usar. Antes que os shaders de fragmento sejam executados, um recorte  ( clipping  ) é executado. O clipping descarta todos os fragmentos que estão fora de sua visão, aumentando o desempenho.\nUm fragmento em OpenGL são todos os dados necessários para que a OpenGL renderize um único pixel.\n O principal objetivo do shader de fragmento  ( fragment shader  ) é calcular a cor final de um pixel e geralmente é o estágio em que todos os efeitos OpenGL avançados ocorrem. Normalmente, o shader de fragmento contém dados sobre a cena 3D que pode usar para calcular a cor final do pixel (como luzes, sombras, cor da luz e assim por diante).\nDepois que todos os valores de cor correspondentes foram determinados, o objeto final passará por mais um estágio que chamamos de teste alfa  ( alpha test  ) e estágio de mistura  ( blending  ). Este estágio verifica o valor de profundidade ( depth  ) (e estêncil ( stencil  )) correspondente (veremos mais tarde) do fragmento e os usa para verificar se o fragmento resultante está na frente ou atrás de outros objetos e portanto ser descartado ou não. O estágio também verifica os valores alfa  (os valores alfa definem a opacidade de um objeto) e combina ( blend  ) os objetos de acordo. Portanto, mesmo que a cor de saída de um pixel seja calculada no shader de fragmento, a cor final do pixel ainda pode ser algo totalmente diferente ao renderizar vários triângulos.\nComo você pode ver, o pipeline gráfico é bastante complexo e contém muitas partes configuráveis. Porém, para quase todos os casos, só temos que trabalhar com os shaders de vértice e fragmento. O shader de geometria é opcional e geralmente configurado em seu padrão. Há também o estágio de tesselação e o loop de feedback de transformação que não representamos aqui, mas isso fica para depois.\nNa OpenGL moderna, precisamos definir pelo menos um shader de vértice e fragmento por conta própria (não há shaders de vértice / fragmento padrão na GPU). Por esta razão, muitas vezes é muito difícil começar a aprender OpenGL moderna, uma vez que é necessário um grande conhecimento antes de ser capaz de renderizar seu primeiro triângulo. Depois de finalmente renderizar seu triângulo no final deste capítulo, você saberá muito mais sobre programação gráfica.\nVertex input (Entrada de vértices) Para começar a desenhar algo, primeiro temos que fornecer a OpenGL alguns dados de vértice de entrada. A OpenGL é uma biblioteca de gráficos 3D, portanto, todas as coordenadas que especificamos em OpenGL estão em 3D (coordenadas x, y e z). Ela não transforma simplesmente todas as suas coordenadas 3D em pixels 2D na tela; só processa coordenadas 3D quando elas estão em um intervalo específico entre $-1.0$ e $1.0$ em todos os 3 eixos (x, y e z). Todas as coordenadas dentro do chamado intervalo de coordenadas de dispositivo normalizadas  ( normalized device coordinates  ) ficarão visíveis na tela (e todas as coordenadas fora desta região não).\nComo queremos renderizar um único triângulo, queremos especificar um total de três vértices com cada vértice tendo uma posição 3D. Nós os definimos em coordenadas de dispositivo normalizadas (a região visível da OpenGL) em um array de float:\nfloat vertices[] = { -0.5f, -0.5f, 0.0f, 0.5f, -0.5f, 0.0f, 0.0f, 0.5f, 0.0f };  Como a OpenGL funciona no espaço 3D, renderizamos um triângulo 2D com cada vértice tendo uma coordenada z de $0.0$. Desta forma, a profundidade do triângulo permanece a mesma, fazendo com que pareça 2D.\nCoordenadas de dispositivo normalizadas (CDN)\nUma vez que suas coordenadas de vértice foram processadas no shader de vértice, elas devem estar em coordenadas de dispositivo normalizadas, que é um pequeno espaço onde os valores x, y e z variam de $-1.0$ a $1.0$. Quaisquer coordenadas que caiam fora desse intervalo serão descartadas/cortadas e não ficarão visíveis na tela. Abaixo você pode ver o triângulo que especificamos dentro das CDN (ignorando o eixo z): \nAo contrário das coordenadas normais da tela, os pontos positivos do eixo y apontam para para cima e as coordenadas $(0,0)$ estão no centro do gráfico, em vez de no canto superior esquerdo. Eventualmente, você deseja que todas as coordenadas (transformadas) terminem neste espaço de coordenadas, caso contrário, elas não ficarão visíveis.\nSuas coordenadas CDN serão então transformadas em coordenadas de espaço de tela  ( screen-space coordinates  ) por meio da transformação da janela de visualização  ( viewport transform  ) usando os dados fornecidos com a glViewport  . As coordenadas de espaço de tela resultantes são então transformadas em fragmentos como entradas para o shader de fragmento.\n Com os dados do vértice definidos, gostaríamos de enviá-los como entrada para o primeiro processo do pipeline gráfico: o shader de vértice. Isso é feito criando memória na GPU onde armazenamos os dados de vértice, configurando como a OpenGL deve interpretar a memória e especificando como enviar os dados para a placa gráfica. O shader de vértice então processa tantos vértices quanto lhe dizemos de sua memória.\nGerenciamos essa memória por meio dos chamados objetos de buffer de vértice  ( vertex buffer objects  ) (VBO), que podem armazenar um grande número de vértices na memória da GPU. A vantagem de usar esses objetos de buffer é que podemos enviar grandes lotes de dados de uma vez para a placa de vídeo e mantê-los lá se houver memória suficiente, sem ter que enviar dados de um vértice de cada vez. O envio de dados da CPU para a placa de vídeo é relativamente lento, portanto, sempre que podemos, tentamos enviar o máximo de dados possível de uma vez. Uma vez que os dados estão na memória da placa gráfica, o shader de vértice tem acesso quase instantâneo aos vértices tornando-o extremamente rápido.\nUm VBO é a nosso primeiro encontro com um objeto OpenGL, conforme discutimos no capítulo OpenGL. Assim como qualquer objeto em OpenGL, este objeto de buffer tem um ID exclusivo correspondente ao buffer, então podemos gerar um com um ID de buffer usando a função glGenBuffers  :\nunsigned int VBO; glGenBuffers(1, \u0026amp;VBO);  A OpenGL tem muitos tipos de objetos de buffer e o tipo de buffer de um VBO é GL_ARRAY_BUFFER . A OpenGL nos permite conectar a vários buffers de uma vez, desde que eles tenham um tipo de buffer diferente. Podemos associar ( bind  ) o buffer recém-criado ao alvo GL_ARRAY_BUFFER com a função glBindBuffer  :\nglBindBuffer(GL_ARRAY_BUFFER, VBO);  A partir desse ponto, qualquer chamada de buffer que fizermos (no destino GL_ARRAY_BUFFER ) será usada para configurar o buffer atualmente associado, que é o VBO. Então podemos fazer uma chamada para a função glBufferData  que copia os dados de vértice definidos anteriormente para a memória do buffer:\nglBufferData(GL_ARRAY_BUFFER, sizeof(vertices), vertices, GL_STATIC_DRAW);  glBufferData  é uma função voltada especificamente para copiar dados definidos pelo usuário para o buffer atualmente associado. Seu primeiro argumento é o tipo de buffer para o qual queremos copiar os dados: o objeto buffer de vértices atualmente associado ao alvo GL_ARRAY_BUFFER . O segundo argumento especifica o tamanho dos dados (em bytes) que queremos passar para o buffer; um simples sizeof dos dados de vértice é suficiente. O terceiro parâmetro são os dados de fato que queremos enviar.\nO quarto parâmetro especifica como queremos que a placa gráfica gerencie os dados fornecidos. Isso pode assumir três formas:\n GL_STREAM_DRAW : os dados são modificados apenas uma vez e utilizados pela GPU poucas vezes. GL_STATIC_DRAW : os dados são modificados uma única vez e utilizados muitas vezes. GL_DYNAMIC_DRAW : os dados são modificados e utilizados muitas vezes.  Os dados de posição do triângulo não mudam, são muito usados ​​e permanecem os mesmos para cada chamada de renderização, então seu tipo de uso deve ser GL_STATIC_DRAW . Se, por exemplo, alguém tiver um buffer com dados que provavelmente mudam com frequência, um tipo de uso de GL_DYNAMIC_DRAW garante que a placa gráfica colocará os dados na memória que permite gravações mais rápidas.\nA partir de agora, armazenamos os dados de vértice na memória da placa de vídeo gerenciados por um objeto de buffer de vértices chamado VBO. Em seguida, queremos criar um shader de vértice e fragmento que realmente processem esses dados, então vamos começar a construí-los.\nShader de Vértice (Vertex Shader) O shader de vértice é um dos shaders programáveis ​​por pessoas como nós. A OpenGL moderna requer que configuremos pelo menos um shader de vértice e fragmento se quisermos fazer alguma renderização, portanto, apresentaremos rapidamente os shaders e configuraremos dois shaders muito simples para desenhar nosso primeiro triângulo. No próximo capítulo, discutiremos os shaders com mais detalhes.\nA primeira coisa que precisamos fazer é escrever o shader de vértice na linguagem de shader GLSL  ( OpenGL Shading Language  ) e, em seguida, compilar esse shader para que possamos usá-lo em nossa aplicação. Abaixo você encontrará o código-fonte de um shader de vértice muito básico em GLSL:\n#version 330 core layout (location = 0) in vec3 aPos; void main() { gl_Position = vec4(aPos.x, aPos.y, aPos.z, 1.0); }  Como você pode ver, GLSL é semelhante a C. Cada shader começa com uma declaração de sua versão. Desde a OpenGL 3.3 e superior, os números de versão de GLSL correspondem à versão da OpenGL (GLSL versão 420 corresponde a OpenGL versão 4.2, por exemplo). Também mencionamos explicitamente que estamos usando a funcionalidade de core-profile.\nEm seguida, declaramos todos os atributos de vértice de entrada no shader de vértice com a palavra-chave in. No momento, só nos importamos com os dados de posição, portanto, precisamos apenas de um único atributo de vértice. A GLSL tem um tipo de dados vetorial que contém de 1 a 4 floats com base em seu dígito de sufixo. Como cada vértice possui uma coordenada 3D, criamos uma variável de entrada vec3 com o nome aPos . Também definimos especificamente a localização da variável de entrada por meio de layout (location = 0) e você verá mais tarde por que precisaremos dessa localização.\nVetor\nNa programação gráfica, usamos o conceito matemático de vetor com bastante frequência, uma vez que ele representa claramente as posições / direções em qualquer espaço e tem propriedades matemáticas úteis. Um vetor em GLSL tem um tamanho máximo de 4 e cada um de seus valores pode ser accessado via vec.x, vec.y, vec.z e vec.w respectivamente, onde cada um deles representa uma coordenada no espaço. Observe que a componente vec.w não é usado como uma posição no espaço (estamos lidando com 3D, não 4D), mas é usado para algo chamado divisão de perspectiva  ( perspective division  . Discutiremos os vetores com muito mais profundidade em um capítulo posterior.\n Para definir a saída do shader de vértice, temos que atribuir os dados de posição à variável predefinida gl_Position que é um vec4. No final da função main  , tudo o que definirmos como gl_Position será usado como a saída do shader de vértice. Como nossa entrada é um vetor de tamanho 3, temos que convertê-lo em um vetor de tamanho 4. Podemos fazer isso inserindo os valores de vec3 dentro do construtor de vec4 e definir sua componente w para 1.0f (explicaremos o porquê em um capítulo posterior).\nO shader de vértice atual é provavelmente o shader de vértice mais simples que podemos imaginar, porque não fizemos nenhum processamento nos dados de entrada e simplesmente os encaminhamos para a saída do shader. Em aplicações reais, os dados de entrada geralmente não estão em coordenadas de dispositivo normalizadas, portanto, primeiro temos que transformar os dados de entrada em coordenadas que caiam na região visível da OpenGL.\nCompiling a shader Pegamos o código-fonte do shader de vértice e o armazenamos em uma string const C no topo do arquivo de código por enquanto:\nconst char *vertexShaderSource = \u0026#34;#version 330 core\\n\u0026#34; \u0026#34;layout (location = 0) in vec3 aPos;\\n\u0026#34; \u0026#34;void main()\\n\u0026#34; \u0026#34;{\\n\u0026#34; \u0026#34; gl_Position = vec4(aPos.x, aPos.y, aPos.z, 1.0);\\n\u0026#34; \u0026#34;}\\0\u0026#34;;  Para que a OpenGL use o shader, ela precisa compilá-lo dinamicamente em tempo de execução a partir de seu código-fonte. A primeira coisa que precisamos fazer é criar um objeto shader, novamente referenciado por um ID. Portanto, armazenamos o shader de vértice como um unsigned int e criamos o shader com glCreateShader  :\nunsigned int vertexShader; vertexShader = glCreateShader(GL_VERTEX_SHADER);  Fornecemos o tipo de shader que queremos criar como um argumento para glCreateShader  . Como estamos criando um shader de vértice, passamos GL_VERTEX_SHADER .\nEm seguida, anexamos o código-fonte do shader ao objeto shader e compilamos o shader:\nglShaderSource(vertexShader, 1, \u0026amp;vertexShaderSource, NULL); glCompileShader(vertexShader);  A função glShaderSource  pega o objeto shader para compilar como seu primeiro argumento. O segundo argumento especifica quantas strings estamos passando como código-fonte, que é apenas uma. O terceiro parâmetro é o código-fonte propriamente dito do shader de vértice e podemos deixar o quarto parâmetro como NULL.\nVocê provavelmente deseja verificar se a compilação foi bem-sucedida após a chamada de glCompileShader e, caso contrário, quais erros foram encontrados para que você possa corrigi-los. A verificação de erros em tempo de compilação é realizada da seguinte maneira:\nint success; char infoLog [512]; glGetShaderiv (vertexShader, GL_COMPILE_STATUS, \u0026amp; sucess);  Primeiro, definimos um inteiro para indicar o sucesso e um recipiente de armazenamento para as mensagens de erro (se houver). Em seguida, verificamos se a compilação foi bem-sucedida com glGetShaderiv. Se a compilação falhar, devemos recuperar a mensagem de erro com glGetShaderInfoLog e imprimir a mensagem de erro.\nif (!success) { glGetShaderInfoLog (vertexShader, 512, NULL, infoLog); std::cout \u0026lt;\u0026lt; \u0026#34;ERROR::SHADER::VERTEX::COMPILATION_FAILED\\n\u0026#34; \u0026lt;\u0026lt; infoLog \u0026lt;\u0026lt; std::endl; }  \n Se nenhum erro foi detectado durante a compilação do shader de vértice, agora ele está compilado.\nShader de Fragmento (Fragment shader) O shader de fragmento é o segundo e último shader que iremos criar para renderizar um triângulo. O shader de fragmento tem como objetivo calcular a cor de saída de seus pixels. Para manter as coisas simples, o shader de fragmento sempre produzirá uma cor laranja.\nAs cores na Computação Gráfica são representadas como um array de 4 valores: a componente vermelho, verde, azul e alfa (opacidade), comumente chamadas de RGBA. Ao definir uma cor em OpenGL ou GLSL, definimos a influencia de cada componente como um valor entre $0.0$ e $1.0$. Se, por exemplo, definirmos o vermelho como $1.0$ e o verde como $1.0$, obteremos uma mistura de ambas as cores e com isso a cor amarela. Com essas três componentes de cores, podemos gerar mais de 16 milhões de cores diferentes!\n #version 330 core out vec4 FragColor; void main() { FragColor = vec4(1.0f, 0.5f, 0.2f, 1.0f); }  O shader de fragmento requer apenas uma variável de saída e essa é um vetor de tamanho 4 que define a cor final de saída que devemos calcular nós mesmos. Podemos declarar valores de saída com a palavra-chave out, que chamamos aqui prontamente de FragColor . Em seguida, simplesmente atribuímos um vec4 à saída de cor como uma cor laranja com um valor alfa de $1.0$ ($1.0$ sendo completamente opaco).\nO processo para compilar um shader de fragmento é semelhante ao shader de vértice, embora desta vez usemos a constante GL_FRAGMENT_SHADER como o tipo de shader:\nunsigned int fragmentShader; fragmentShader = glCreateShader(GL_FRAGMENT_SHADER); glShaderSource(fragmentShader, 1, \u0026amp;fragmentShaderSource, NULL); glCompileShader(fragmentShader);  Ambos os shaders agora estão compilados e a única coisa que resta a fazer é atrelar os dois objetos de shader em um programa de shader  (shader program  ) que podemos usar para renderização. Certifique-se de verificar se há erros de compilação aqui também!\nPrograma de Shader (Shader Program) Um objeto de programa de shader é a versão final linkada de vários shaders combinados. Para usar os shaders compilados recentemente, temos que atrelálos-los ( link  ) a um objeto de programa de shader e, em seguida, ativar este programa de shader ao renderizar objetos. Os shaders do programa de shader ativado serão usados quando fizermos chamadas de renderização.\nAo atrelar os shaders a um programa, ele associa as saídas de cada shader às entradas do próximo shader. É aqui também que você obterá erros de linkagem ( linking ) se suas saídas e entradas não corresponderem.\nCriar um objeto de programa é fácil:\nunsigned int shaderProgram; shaderProgram = glCreateProgram();  A função glCreateProgram cria um programa e retorna a referência ID para o objeto de programa recém-criado. Agora precisamos anexar os shaders compilados anteriormente ao objeto do programa e, em seguida, conectá-los com glLinkProgram:\nglAttachShader(shaderProgram, vertexShader); glAttachShader(shaderProgram, fragmentShader); glLinkProgram(shaderProgram);  O código deveria ser autoexplicativo, nós anexamos os shaders ao programa e os associamos via glLinkProgram.\n Assim como a compilação de shader, também podemos verificar se o linking de um programa de shader falhou e recuperar o log correspondente. No entanto, em vez de usar glGetShaderiv e glGetShaderInfoLog, agora usamos:\nglGetProgramiv(shaderProgram, GL_LINK_STATUS, \u0026amp;success); if(!success) { glGetProgramInfoLog(shaderProgram, 512, NULL, infoLog); ... }  \n O resultado é um objeto de programa que podemos ativar chamando glUseProgram:\nglUseProgram(shaderProgram);  Cada shader e chamada de renderização após glUseProgram agora usarão esse objeto de programa (e, portanto, os shaders).\nAh, sim, e não se esqueça de excluir os objetos de shader depois de associá-los ao objeto de programa; não precisamos mais deles:\nglDeleteShader(vertexShader); glDeleteShader(fragmentShader);  Agora enviamos os dados de vértice de entrada para a GPU e instruímos a GPU como ela deve processar os dados de vértice em um shader de vértice e fragmento. Estamos quase lá, mas ainda não. A OpenGL ainda não sabe como deve interpretar os dados do vértice na memória e como deve conectar os dados do vértice aos atributos do shader de vértice. Seremos legais e diremos a OpenGL como fazer isso.\nLinkando Attributos de Vértice (Linking Vertex Attributes) O shader de vértice nos permite especificar qualquer entrada que desejamos na forma de atributos de vértice e, embora isso permita grande flexibilidade, significa que temos que especificar manualmente que parte de nossos dados de entrada vai para qual atributo de vértice no shader de vértice. Isso significa que temos que especificar como a OpenGL deve interpretar os dados do vértice antes da renderização.\nNossos dados de buffer de vértices são formatados da seguinte maneira:\n\n Os dados de posição são armazenados como valores de ponto flutuante de 32 bits (4 bytes). Cada posição é composta por 3 desses valores. Não há espaço (ou outros valores) entre cada conjunto de 3 valores. Os valores são compactados (tightly packed  no array. O primeiro valor nos dados está no início do buffer.  Com esse conhecimento, podemos dizer a OpenGL como ela deve interpretar os dados do vértice (por atributo de vértice) usando glVertexAttribPointer:\nglVertexAttribPointer(0, 3, GL_FLOAT, GL_FALSE, 3 * sizeof(float), (void*)0); glEnableVertexAttribArray(0);  A função glVertexAttribPointer tem alguns parâmetros, então vamos examiná-los cuidadosamente:\n O primeiro parâmetro especifica qual atributo de vértice queremos configurar. Lembre-se de que especificamos a localização do atributo de vértice de posição no shader de vértice com layout (location = 0). Isso define a localização do atributo de vértice como 0 e, como queremos passar dados para esse atributo de vértice, passamos 0.\n O próximo argumento especifica o tamanho do atributo. É um vec3, portanto, é composto por 3 valores.\n O terceiro argumento especifica o tipo de dado que é GL_FLOAT (um vec* em GLSL consiste em valores de ponto flutuante).\n O próximo argumento especifica se queremos que os dados sejam normalizados. Se estivermos inserindo tipos de dados inteiros (int, byte) e definimos isso como GL_TRUE, os dados inteiros são normalizados para 0 (ou -1 para dados com sinal) e 1 quando convertidos para float. Isso não é relevante para nós, então deixaremos como GL_FALSE.\n O quinto argumento é conhecido como stride  e nos indica o espaço entre atributos de vértices consecutivos. Como o próximo conjunto de dados de posição está localizado a exatamente 3 vezes o tamanho de um float, especificamos esse valor como o stride. Observe que, como sabemos que o array está compactado (não há espaço entre o próximo valor de atributo do vértice), poderíamos também ter especificado a distância como 0 para permitir que a OpenGL determine a distância (isso só funciona quando os valores estão compactados). Sempre que temos mais atributos de vértice, temos que definir cuidadosamente o espaçamento entre cada atributo, mas veremos mais exemplos disso mais tarde.\n O último parâmetro é do tipo void * e, portanto, requer aquela conversão (cast ) estranha. Este é o deslocamento  (offset  ) de onde os dados de posição começam no buffer. Uma vez que os dados de posição estão no início do array de dados, este valor é apenas 0. Exploraremos este parâmetro em mais detalhes posteriormente.\n  Cada atributo de vértice obtém seus dados da memória gerenciada por um VBO e de qual VBO ele obtém seus dados (você pode ter vários VBOs) é determinado pelo VBO atualmente associado a GL_ARRAY_BUFFER ao chamar glVertexAttribPointer. Como o VBO definido anteriormente ainda está associado antes de chamar glVertexAttribPointer, o atributo de vértice 0 agora está associado a seus dados de vértice.\n Agora que especificamos como a OpenGL deve interpretar os dados do vértice, devemos também habilitar o atributo do vértice com glEnableVertexAttribArray fornecendo a localização do atributo do vértice como seu argumento; atributos de vértice são desabilitados por padrão. A partir desse ponto, temos tudo configurado: inicializamos os dados de vértice em um buffer usando um objeto de buffer de vértices, configuramos um shader de vértice e fragmento e informamos a OpenGL como conectar os dados de vértice aos atributos de vértice do shader de vértice. Desenhar um objeto em OpenGL agora seria assim:\n// 0. copia nosso array de vertices em um buffer para a OpenGL usar glBindBuffer(GL_ARRAY_BUFFER, VBO); glBufferData(GL_ARRAY_BUFFER, sizeof(vertices), vertices, GL_STATIC_DRAW); // 1. configure os ponteiros dos atributos de vertice glVertexAttribPointer(0, 3, GL_FLOAT, GL_FALSE, 3 * sizeof(float), (void*)0); glEnableVertexAttribArray(0); // 2. use nosso programa de shader quando quisermos renderizar um objeto glUseProgram(shaderProgram); // 3. agora desenhe o objeto someOpenGLFunctionThatDrawsOurTriangle();  Temos que repetir esse processo toda vez que quisermos desenhar um objeto. Pode não parecer muito, mas imagine se tivermos mais de 5 atributos de vértice e talvez centenas de objetos diferentes (o que não é incomum). Associar os objetos de buffer apropriados e configurar todos os atributos de vértice para cada um desses objetos rapidamente se torna um processo complicado. E se houvesse alguma maneira de armazenar todas essas configurações de estado em um objeto e simplesmente associar esse objeto para restaurar seu estado?\nObjecto de Array de Vértices (Vertex Array Object) Um objeto de array de vértice  (vertex array object  ) (também conhecido como VAO  ) pode ser associado igual a um objeto de buffer de vértices e qualquer chamada de atributo de vértice desse ponto em diante será armazenada dentro do VAO. Isso tem a vantagem de que, ao configurar ponteiros de atributo de vértice, você só precisa fazer essas chamadas uma vez e sempre que quisermos desenhar o objeto, podemos apenas associar o VAO correspondente. Isso torna a troca entre diferentes dados de vértice e configurações de atributo tão fácil quanto associar um VAO diferente. Todo o estado que acabamos de definir é armazenado dentro do VAO.\nA OpenGL requer que usemos um VAO para que ela saiba o que fazer com nossas entradas de vértice. Se não conseguirmos associar um VAO, a OpenGL provavelmente se recusará a desenhar qualquer coisa.\n Um objeto de array de vértices armazena o seguinte:\n Chamadas para glEnableVertexAttribArray ou glDisableVertexAttribArray.\n Configurações de atributo de vértice via glVertexAttribPointer.\n Objectos de buffer de vérticess associados com atributos de vértice através de chamadas de glVertexAttribPointer.\n  \nO processo para gerar um VAO é semelhante ao de um VBO:\nunsigned int VAO; glGenVertexArrays(1, \u0026amp;VAO);  Para usar um VAO, tudo o que você precisa fazer é associar o VAO usando glBindVertexArray. A partir desse ponto, devemos associar/configurar o(s) VBO(s) e o(s) ponteiro(s) de atributo correspondentes e, em seguida, desassociar o VAO para uso posterior. Assim que quisermos desenhar um objeto, simplesmente associamos o VAO com as configurações que quisermos antes de desenhar o objeto e pronto. No código, seria mais ou menos assim:\n// ..:: Codigo de inicializacao (feito uma vez (a menos que seu objeto sofra alteracoes)) :: .. // 1. associe Vertex Array Object glBindVertexArray(VAO); // 2. copie nosso array de vertices em um buffer para OpenGL usar glBindBuffer(GL_ARRAY_BUFFER, VBO); glBufferData(GL_ARRAY_BUFFER, sizeof(vertices), vertices, GL_STATIC_DRAW); // 3. entao defina nossos ponteiros de atributos de vertices glVertexAttribPointer(0, 3, GL_FLOAT, GL_FALSE, 3 * sizeof(float), (void*)0); glEnableVertexAttribArray(0); [...] // ..:: Codigo de desenho (no loop de renderizacao) :: .. // 4. desenhe o objeto glUseProgram(shaderProgram); glBindVertexArray(VAO); someOpenGLFunctionThatDrawsOurTriangle();  E é isso! Tudo o que fizemos nos últimos milhões de páginas levou a este momento, um VAO que armazena nossa configuração de atributo de vértice e qual VBO usar. Normalmente, quando você tem vários objetos que deseja desenhar, primeiro gera / configura todos os VAOs (e, portanto, o VBO necessário e os ponteiros de atributo) e os armazena para uso posterior. No momento em que queremos desenhar um de nossos objetos, pegamos o VAO correspondente, o associamos e, em seguida, desenhamos o objeto e desassociamos o VAO novamente.\nO Triângulo que todos nós estivemos esperando Para desenhar nossos objetos de escolha, a OpenGL nos fornece a função glDrawArrays que desenha primitivas usando o shader ativo no momento, a configuração de atributo de vértice definida anteriormente e os dados de vértice do VBO (indiretamente associados por VAO).\nglUseProgram(shaderProgram); glBindVertexArray(VAO); glDrawArrays(GL_TRIANGLES, 0, 3);  A função glDrawArrays toma como primeiro argumento o tipo de primitiva OpenGL que gostaríamos de desenhar. Como eu disse no início que queríamos desenhar um triângulo, não gosto de mentir para vocês, passamos GL_TRIANGLES . O segundo argumento especifica o índice inicial do array de vértices que gostaríamos de desenhar; apenas deixamos isso em 0. O último argumento especifica quantos vértices queremos desenhar, que é 3 (renderizamos apenas 1 triângulo de nossos dados, que tem exatamente 3 vértices de comprimento).\nAgora tente compilar o código e checar seus passos novamente caso apareça algum erro. Assim que sua aplicação for compilada, você verá o seguinte resultado:\n\nO código-fonte do programa completo pode ser encontrado aqui.\nSe sua saída não parecer a mesma, você provavelmente fez algo errado ao longo do caminho, então verifique o código-fonte completo e veja se você esqueceu alguma coisa.\nObjetos de Buffer de Elementos (Element Buffer Objects) Há uma última coisa que gostaríamos de discutir ao renderizar vértices: objetos de buffer de elementos  ( element buffer objects  ) abreviados para EBO. Para explicar como os objetos de buffer de elementos funcionam, é melhor dar um exemplo: suponha que desejamos desenhar um retângulo em vez de um triângulo. Podemos desenhar um retângulo usando dois triângulos (a OpenGL funciona principalmente com triângulos). Isso irá gerar o seguinte conjunto de vértices:\nfloat vertices[] = { // primeiro triangulo  0.5f, 0.5f, 0.0f, // canto superior direito  0.5f, -0.5f, 0.0f, // canto inferior direito  -0.5f, 0.5f, 0.0f, // canto superior esquerdo  // segundo triangulo  0.5f, -0.5f, 0.0f, // canto inferior direito  -0.5f, -0.5f, 0.0f, // canto inferior esquerdo  -0.5f, 0.5f, 0.0f // canto superior esquerdo };  Como você pode ver, há alguma sobreposição nos vértices especificados. Especificamos o canto inferior direito e o canto superior esquerdo duas vezes! Isso é um overhead de 50%, já que o mesmo retângulo também pode ser especificado com apenas 4 vértices, em vez de 6. Isso só vai piorar assim que tivermos modelos mais complexos com mais de 1000 triângulos onde haverá grandes pedaços que se sobrepõem. O que seria uma solução melhor é armazenar apenas os vértices únicos e, em seguida, especificar a ordem em que queremos desenhar esses vértices. Nesse caso, teríamos apenas que armazenar 4 vértices para o retângulo e, em seguida, apenas especificar em que ordem gostaríamos de desenhá-los. Não seria ótimo se a OpenGL nos fornecesse um recurso como esse?\nFelizmente, os objetos de buffer de elementos funcionam exatamente assim. Um EBO é um buffer, assim como um objeto de buffer de vértices, que armazena índices que a OpenGL usa para decidir quais vértices desenhar. Este chamado desenho indexado  ( indexed drawing  ) é exatamente a solução para o nosso problema. Para começar, primeiro temos que especificar os vértices (únicos) e os índices para desenhá-los como um retângulo:\nfloat vertices[] = { 0.5f, 0.5f, 0.0f, // canto superior direito  0.5f, -0.5f, 0.0f, // canto inferior direito  -0.5f, -0.5f, 0.0f, // canto inferior esquerdo  -0.5f, 0.5f, 0.0f // canto superior esquerdo }; unsigned int indices[] = { // note que comecamos com 0!  0, 1, 3, // primeiro triangulo  1, 2, 3 // segundo triangulo };  Você pode ver que, ao usar índices, precisamos apenas de 4 vértices em vez de 6. Em seguida, precisamos criar o objeto buffer de elementos:\nunsigned int EBO; glGenBuffers(1, \u0026amp;EBO);  Semelhante ao VBO, associamos o EBO e copiamos os índices no buffer com glBufferData. Além disso, assim como o VBO, queremos colocar essas chamadas entre uma chamada bind e uma chamada unbind, embora desta vez especifiquemos GL_ELEMENT_ARRAY_BUFFER como o tipo de buffer.\nglBindBuffer(GL_ELEMENT_ARRAY_BUFFER, EBO); glBufferData(GL_ELEMENT_ARRAY_BUFFER, sizeof(indices), indices, GL_STATIC_DRAW);  Observe que agora estamos fornecendo GL_ELEMENT_ARRAY_BUFFER como o destino do buffer. A última coisa que falta fazer é substituir a chamada de glDrawArrays por glDrawElements para indicar que queremos renderizar os triângulos de um buffer de índices. Ao usar glDrawElements, vamos desenhar usando índices fornecidos no objeto de buffer de elementos atualmente associado:\nglBindBuffer(GL_ELEMENT_ARRAY_BUFFER, EBO); glDrawElements(GL_TRIANGLES, 6, GL_UNSIGNED_INT, 0);  O primeiro argumento especifica o modo que queremos desenhar, semelhante a glDrawArrays. O segundo argumento é a contagem ou número de elementos que gostaríamos de desenhar. Especificamos 6 índices, então queremos desenhar 6 vértices no total. O terceiro argumento é o tipo dos índices que é do tipo GL_UNSIGNED_INT. O último argumento nos permite especificar um deslocamento no EBO (ou passar uma matriz de índice, mas isso é quando você não está usando objetos de buffer de elemento), mas vamos apenas deixar isso em 0.\nA função glDrawElements obtém seus índices do EBO atualmente associado ao alvo GL_ELEMENT_ARRAY_BUFFER . Isso significa que temos que associar o EBO correspondente cada vez que quisermos renderizar um objeto com índices, o que, novamente, é um pouco complicado. Acontece que um objeto de array de vértices também mantém registro de associações de objeto de buffer de elementos. O último objeto de buffer de elementos que é associado enquanto um VAO está associado é armazenado como o objeto de buffer de elementos do VAO. A associação a um VAO também vincula automaticamente esse EBO.\n\nUm VAO armazena as chamadas glBindBuffer quando o destino é GL_ELEMENT_ARRAY_BUFFER . Isso também significa que ele armazena suas chamadas de desassociação, portanto, certifique-se de não desassociar o buffer de array de elementos antes de desassociar seu VAO, caso contrário, ele não terá um EBO configurado.\n A inicialização resultante e o código de desenho agora se parecem com isto:\n// ..:: Codigo de inicializacao :: .. // 1. associe o Vertex Array Object glBindVertexArray(VAO); // 2. copie nosso array de vertices em um buffer de vertices para a OpenGL usar glBindBuffer(GL_ARRAY_BUFFER, VBO); glBufferData(GL_ARRAY_BUFFER, sizeof(vertices), vertices, GL_STATIC_DRAW); // 3. copie nosso array de indices em um buffer de elementos para a OpenGL usar glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, EBO); glBufferData(GL_ELEMENT_ARRAY_BUFFER, sizeof(indices), indices, GL_STATIC_DRAW); // 4. entao configure os ponteiros de atributos de vertice glVertexAttribPointer(0, 3, GL_FLOAT, GL_FALSE, 3 * sizeof(float), (void*)0); glEnableVertexAttribArray(0); [...] // ..:: Codigo de desenho (no loop de renderizacao) :: .. glUseProgram(shaderProgram); glBindVertexArray(VAO); glDrawElements(GL_TRIANGLES, 6, GL_UNSIGNED_INT, 0) glBindVertexArray(0);  A execução do programa deve fornecer uma imagem conforme ilustrado abaixo. A imagem da esquerda deve parecer familiar e a imagem da direita é o retângulo desenhado no modo wireframe  . O retângulo de wireframe mostra que o retângulo de fato consiste em dois triângulos.\n\nModo Wireframe\nPara desenhar seus triângulos no modo wireframe, você pode configurar como a OpenGL desenha suas primitivas via glPolygonMode (GL_FRONT_AND_BACK, GL_LINE). O primeiro argumento diz que queremos aplicá-lo à frente e atrás de todos os triângulos e a segunda linha nos diz para desenhá-los como linhas. Quaisquer chamadas de desenho subsequentes irão renderizar os triângulos no modo wireframe até que o definamos de volta ao seu padrão usando glPolygonMode (GL_FRONT_AND_BACK, GL_FILL).\n Se você tiver algum erro, retroceda e veja se esqueceu alguma coisa. Você pode encontrar o código-fonte completo aqui.\nSe você conseguiu desenhar um triângulo ou retângulo exatamente como fizemos, parabéns, você conseguiu passar por uma das partes mais difíceis da OpenGL moderno: desenhar seu primeiro triângulo. Esta é uma parte difícil, pois é necessário um grande conhecimento antes de ser capaz de desenhar o primeiro triângulo. Felizmente, agora superamos essa barreira e os próximos capítulos serão muito mais fáceis de entender.\nRecursos Adicionais  antongerdelan.net/hellotriangle: A discussão de Anton Gerdelan sobre a renderização do primeiro triângulo.\n open.gl/drawing: Versão do Alexander Overvoorde.\n antongerdelan.net/vertexbuffers: alguns insights extras sobre objetos de buffer de vértices.\n learnopengl.com/In-Practice/Debugging: há várias etapas envolvidas neste capítulo; se você estiver travado, pode valer a pena ler um pouco sobre depuração em OpenGL (até a seção de saída de depuração).\n  Exercícios Para realmente ter uma boa compreensão dos conceitos discutidos, alguns exercícios foram preparados. É aconselhável trabalhar com eles antes de passar para o próximo assunto para ter certeza de ter uma boa compreensão do que está acontecendo.\n Tente desenhar 2 triângulos próximos um do outro usando glDrawArrays adicionando mais vértices aos seus dados: solução.\n Agora crie os mesmos 2 triângulos usando dois VAOs e VBOs diferentes para seus dados: solução.\n Crie dois programas de shader onde o segundo programa usa um shader de fragmento diferente que produz a cor amarela; desenhe os dois triângulos novamente, onde um resulta na cor amarela: solução.\n  "
},
{
	"uri": "https://filipecn.github.io/aprendaopengl/pbr/teoria/",
	"title": "Teoria",
	"tags": [],
	"description": "",
	"content": "Post Original\nPBR, ou mais comumente conhecido como physically based rendering  (renderização baseada em física), é uma coleção de técnicas de renderização mais ou menos baseadas na mesma base teórica que mais se aproxima ao mundo físico. Como a renderização baseada em física visa imitar a luz de maneira fisicamente plausível, geralmente parece mais realista em comparação aos nossos algoritmos de iluminação originais como Phong e Blinn-Phong. Não só parece melhor, pois se aproxima de perto da física real, nós (e especialmente os artistas) podem criar materiais de superfícies baseados em parâmetros físicos sem ter que recorrer a hacks baratos e ajustes para fazer a iluminação parecer certa. Uma das maiores vantagens da criação de materiais baseada em parâmetros físicos é que esses materiais parecerão corretos, independentemente das condições de iluminação; algo que não é verdade em pipelines não-PBR.\nA PBR ainda é uma aproximação da realidade (com base nos princípios da física), e é por isso que não é chamado de shading físico, mas physically based shading . Para um modelo de iluminação PBR ser considerado fisicamente baseado, ele deve satisfazer as seguintes 3 condições (não se preocupe, chegaremos a elas em breve):\n Ser baseado no modelo de superfície de microfacets ( microfacet surface model ).\n Conservar energia.\n Utilizar uma BRDF baseada em física.\n  Nos próximos capítulos PBR, estaremos nos concentrando na abordagem PBR como originalmente explorada pela Disney e adotada para exibição em tempo real pela Epic Games. Sua abordagem, com base no metallic workflow  , é decentemente documentada, amplamente adotada nas engines mais populares e parece visualmente incrível. No final desses capítulos, teremos algo que se parece com isso:\n\nTenha em mente, os tópicos nesses capítulos são bastante avançados, por isso é aconselhável ter uma boa compreensão da OpenGL e iluminação em shader. Alguns dos conhecimentos mais avançados que você precisará para esta série são: frambuffers, cubemaps, correção gama, HDR e normal mapping. Também nos aprofundaremos em algumas matemáticas avançadas, mas farei o meu melhor para explicar os conceitos o mais claro possível.\nO Modelo Microfacet Todas as técnicas PBR são baseadas na teoria dos microfacets . A teoria descreve que qualquer superfície em escala microscópica pode ser descrita por pequenos espelhos reflexivos minúsculos chamados microfacetes  . Dependendo da rugosidade de uma superfície, o alinhamento desses pequenos espelhos pode diferir bastante:\n\nQuanto mais áspera uma superfície é, mais caoticamente alinhado será cada microfacet ao longo da superfície. O efeito do alinhamento desses \u0026quot;espelhinhos\u0026quot; minúsculos é que, quando especificamente falando sobre iluminação / reflexão especular, os raios de luz recebidos são mais propensos a se espalhar ( scatter  ) ao longo de direções completamente diferentes em superfícies mais ásperas, resultando em uma reflexão especular mais espalhada. Em contraste, em uma superfície lisa, os raios de luz são mais propensos a refletir aproximadamente na mesma direção, nos dando reflexos menores e mais nítidos:\n\nNenhuma superfície é completamente suave em um nível microscópico, mas visto que esses microfacets são pequenos o suficiente para que não possamos fazer uma distinção entre eles em uma resolução de pixel, nós aproximamos estatisticamente a rugosidade de microfacet da superfície, dado um parâmetro de rugosidade ( roughness  ). Com base na aspereza de uma superfície, podemos calcular a proporção de microfacets aproximadamente alinhados a algum vetor $h$. Este vetor $h$ é o halfway vector  que fica no meio do caminho entre o vetor de luz $l$ e visão $v$. Discutimos o vetor halfway antes no capítulo de iluminação avançada que é calculado como a soma de $l$ e $v$ dividido pelo seu comprimento:\n\\[h = \\frac{l+v}{\\parallel l + v\\parallel}\\]\nQuanto mais os microfacets estão alinhados ao vetor halfway, o mais nítido e mais forte é a reflexão especular. Juntamente com o parâmetro roughness que varia entre 0 e 1, podemos aproximar estatisticamente o alinhamento dos microfacets:\n\nPodemos ver que os valores de rugosidade mais elevados exibem uma forma de reflexão especular muito maior, em contraste com a forma de reflexão especular menor e mais nítida de superfícies lisas.\nConservação de Energia A aproximação da microfacet emprega uma forma de conservação de energia  : a energia da luz de saída nunca deve exceder a energia da luz de entrada (excluindo superfícies emissivas). Olhando para a imagem acima, vemos a área de reflexão especular aumentando, mas também seu brilho diminuindo no aumento dos níveis de roughness. Se a intensidade especular fosse a mesma em cada pixel (independentemente do tamanho da forma especular), as superfícies mais rígidas emitiriam muito mais energia, violando o princípio da conservação de energia. É por isso que vemos reflexos especulares mais intensamente em superfícies lisas e mais vagamente em superfícies ásperas.\nPara a conservação de energia se manter, precisamos fazer uma distinção clara entre a luz difusa e especular. No momento em que um raio de luz atinge uma superfície, é dividido em uma parte de refração ( refraction  ) e uma parte de reflexão ( reflection  ). A parte da reflexão é a luz que é refletida diretamente e não entra na superfície; Isto é o que conhecemos como iluminação especular. A parte de refração é a luz restante que entra na superfície e é absorvida; Isto é o que conhecemos como iluminação difusa.\nHá algumas nuances aqui como a luz refratada não é imediatamente absorvida ao tocar a superfície. Da física, sabemos que a luz pode ser modelada como um feixe de energia que continua avançando até perder toda a sua energia; A maneira como um feixe de luz perde energia é por colisão. Cada material consiste em pequenas partículas que podem colidir com o raio de luz, conforme ilustrado na imagem a seguir. As partículas absorvem alguma ou toda, energia da luz em cada colisão que é convertida em calor.\n\nGeralmente, nem toda a energia é absorvida e a luz continuará a dispersar ( scatter  ) em uma direção (na maior parte) aleatória de modo que colide com outras partículas até que sua energia seja esgotada ou deixa a superfície novamente. Raios de luz reemergindo da superfície contribuem para a cor observada da superfície (difusa). Em PBR no entanto, fazemos a suposição simplificadora de que toda a luz refratada é absorvida e espalhada a uma pequena área de impacto, ignorando o efeito de raios de luz espalhados que teriam saído da superfície à uma distância. Técnicas de shading específicas que levam isso em conta são conhecidos como técnicas de subsurface scattering  que melhoram significativamente a qualidade visual em materiais como pele, mármore ou cera, mas vêm ao preço do desempenho.\nUma sutileza adicional quando se trata de reflexão e refração são superfícies metálicas ( metallic  ). As superfícies metálicas reagem diferentemente à luz em comparação a superfícies não metálicas (também conhecidas como dielétricas ( dielectrics  )). Superfícies metálicas seguem os mesmos princípios de reflexão e refração, mas toda a luz refratada fica diretamente absorvida sem dispersão. Isso significa que as superfícies metálicas só deixam sair a luz refletida ou especular; Superfícies metálicas não mostram cores difusas. Devido a essa aparente distinção entre metais e dielétricos, ambos são tratados de maneira diferente no pipeline PBR que vamos aprofundar mais no capítulo.\nEssa distinção entre a luz refletida e refratada nos leva a outra observação em relação à preservação da energia: elas são mutuamente exclusivas. Qualquer energia de luz que é ​​refletida, não será mais absorvida pelo próprio material. Assim, a energia deixada para entrar na superfície, como luz refratada é diretamente a energia resultante depois que cuidamos da reflexão.\nPreservamos essa relação de conservação de energia, primeiro calculando a fração especular que acumula a porcentagem de energia da luz de entrada que é refletida. A fração de luz refratada é então calculada diretamente a partir da fração especular como:\nfloat kS = calculateSpecularComponent(...); // reflection/specular fraction float kD = 1.0 - kS; // refraction/diffuse fraction  Desta forma, sabemos tanto a quantidade de luz recebida que é refletida e a quantidade de luz recebida que é refrata, aderindo ao mesmo tempo ao princípio da conservação de energia. Dada esta abordagem, é impossível para a contribuição refratada / difusa e refletida / especular de exceder 1.0, garantindo assim que a soma de sua energia nunca exceda a energia de luz recebida. Algo que não levamos em conta nos capítulos de iluminação anteriores.\nA Equação de Refletância Isso nos leva a algo chamado de render equation, uma equação elaborada por algumas pessoas muito inteligentes, que atualmente é o melhor modelo que temos para simular as aparências da luz. O PBR segue fortemente uma versão mais especializada da render equation conhecida como a equação de refletância ( reflectance equation  ). Para entender adequadamente o PBR, é importante primeiro construir uma sólida compreensão da equação de refletância:\n\\[L_o(p,\\omega_o)=\\int_\\Omega f_r(p,\\omega_i,\\omega_o)L_i(p,\\omega_i)n\\cdot\\omega_id\\omega_i\\]\nA equação de reflectância parece assustadora a primeira vista, mas ao dissecá-la, você verá que ela faz sentido. Para entender a equação, temos que nos aprofundar em um pouco de radiometria ( radiometry  ). A radiometria é a medição da radiação eletromagnética, incluindo a luz visível. Existem várias quantidades radiométricas que podemos usar para medir a luz sobre superfícies e direções, mas só discutiremos uma única quantidade que é relevante para a equação de refletância conhecida como radiância ( radiance  ), denotada aqui como $L$. A radiância é usada para quantificar a magnitude ou a força da luz que vem de uma única direção. É um pouco complicado entender a princípio já que a radiância é na verdade uma combinação de várias quantidades físicas, então vamos nos concentrar nelas primeiro:\nFluxo radiante ( radiant flux ): o fluxo radiante $\\Phi$ é a energia transmitida de uma fonte de luz medida em Watts. A luz é uma soma coletiva de energia sobre vários comprimentos de onda diferentes, cada comprimento de onda associado a uma cor específica (visível). A energia emitida de uma fonte de luz pode, portanto, ser pensada como uma função de todos os seus diferentes comprimentos de onda. Comprimentos de onda entre 390nm a 700nm (nanômetros) são considerados parte do espectro de luz visível, isto é, os comprimentos de onda que o olho humano é capaz de perceber. Abaixo você encontrará uma imagem das diferentes energias por comprimento de onda da luz do dia:\n\nO fluxo radiante mede a área total dessa função de diferentes comprimentos de onda. Tomando diretamente essa medida de comprimentos de onda como entrada é ligeiramente impraticável, portanto normalmente simplificamos a representação do fluxo radiante, não como uma função de diferentes forças de comprimento de onda, mas como uma tupla de cores de luz codificada como RGB (ou como costumávamos chamá-la: cor de luz). Essa codificação traz bastante perda de informação, mas isso geralmente é insignificante para aspectos visuais.\nSolid angle: O solid angle, denotado como $\\omega$, nos diz o tamanho ou área de uma forma projetada sob uma esfera unitária. A área da forma projetada nesta esfera unitária é conhecida como solid angle  ; Você pode visualizar o solid angle como uma direção com o volume:\n\nPense como um observador no centro desta esfera olhando na direção da forma; O tamanho da silhueta que enxerga é o solid angle.\nIntensidade radiante ( radiant intensity  ): a intensidade radiante mede a quantidade de fluxo radiante por solid angle, ou a força de uma fonte de luz sobre uma área projetada em uma esfera unitária. Por exemplo, dada uma luz omnidirecional que irradia igualmente em todas as direções, a intensidade radiante pode nos dar sua energia sobre uma área específica (solid angle):\n\nA equação para descrever a intensidade radiante é definida da seguinte forma:\n\\[I=\\frac{d\\Phi}{d\\omega}\\]\nOnde $I$ é o fluxo radiante $\\Phi$ sobre o solid angle $\\omega$.\nCom conhecimento de fluxo radiante, intensidade radiante e solid angle, podemos finalmente descrever a equação para a radiância ( radiance ). A radiância é descrita como a energia total observada em uma área $A$ sobre o solid angle $\\omega$ de uma luz de intensidade radiante $\\Phi$:\n\nA radiância é uma medida radiométrica da quantidade de luz em uma área, escalada pelo ângulo incidente  (ou de chegada) $\\theta$ da luz para a superfície normal como $cos \\theta$: a luz é mais fraca a medida que menos irradia diretamente para a superfície, e mais forte quanto mais é diretamente perpendicular à superfície. Isso é semelhante à nossa percepção de iluminação difusa do capítulo de iluminação básica como $cos \\theta$ corresponde diretamente ao produto escalar entre o vetor de direção da luz e a normal da superfície:\nfloat cosTheta = dot(lightDir, N);  A equação de radiância é bastante útil, pois contém mais quantidades físicas em que estamos interessados. Se considerarmos o solid angle $\\omega$ e a área $A$ sendo infinitamente pequenos, podemos usar a radiância para medir o fluxo de um único raio de luz batendo em um único ponto no espaço. Essa relação permite calcular o brilho de um único raio de luz que influencia um único ponto (de fragmento); Nós efetivamente traduzimos o solid angle $\\omega$ em um vector de direção $\\omega$ e $A$ em um ponto $p$. Dessa forma, podemos usar diretamente a radiância em nossos shaders para calcular a contribuição de um único raio de luz por fragmento.\nDe fato, quando se trata de radiância, geralmente nos importamos com toda a luz recebida em um ponto $p$, que é a soma de toda a radiância conhecida como irradiância  ( irradiance ). Com conhecimento de ambas a radiância e irradiância, podemos voltar à equação de refletância:\n\\[L_o(p,\\omega_o)=\\int_\\Omega f_r(p,\\omega_i,\\omega_o)L_i(p,\\omega_i)n\\cdot\\omega_id\\omega_i\\]\nSabemos agora que $L$ na render equation representa a radiância de algum ponto $p$ e algum solid angle incidente infinitamente pequeno $\\omega_i$ que pode ser considerado como um vetor de direção de incidência $\\omega_i$. Lembre-se que $cos$ $\\theta$ escala a energia com base no ângulo de incidência da luz na superfície, que encontramos na equação de reflectância como $n \\cdot \\omega_i$. A equação de refletância calcula a soma da radiância refletida $L_o(p, \\omega_o)$ de um ponto $p$ na direção $\\omega_o$, que é a direção de saída para o observador. Em outras palavras: $L_o$ mede a soma refletida da irradiância das luzes no ponto $p$ observada de $\\omega_o$.\nA equação de reflectância é baseada em torno da irradiância, que é a soma de toda a radiância recebida que medimos da luz. Não apenas de uma única direção de luz recebida, mas de todas as incidências de luz dentro de um hemisfério $\\Omega$ centrada em torno do ponto $p$. Um hemisfério  pode ser descrito como a meia esfera alinhada em torno de uma normal de superfície $n$:\n\nPara calcular o total de valores dentro de uma área ou (no caso de um hemisfério) um volume, usamos uma construção matemática chamada integral denotada na equação de refletância como $\\int$ sobre todas as direções de incidência $d\\omega_i$ dentro do hemisfério $\\Omega$. Uma integral mede a área de uma função, que pode ser calculada analiticamente ou numericamente. Como não há solução analítica para a equação de renderização e de reflexão, vamos ter que resolver numericamente a integral discretamente. Isso se traduz em tomar o resultado de pequenos passos discretos da equação de refletância sobre o hemisfério $\\Omega$ e calcular os resultados sobre o tamanho do passo. Isso é conhecido como a soma de Riemann  que podemos visualizar no código da seguinte forma:\nint steps = 100; float sum = 0.0f; vec3 P = ...; vec3 Wo = ...; vec3 N = ...; float dW = 1.0f / steps; for(int i = 0; i \u0026lt; steps; ++i) { vec3 Wi = getNextIncomingLightDir(i); sum += Fr(P, Wi, Wo) * L(P, Wi) * dot(N, Wi) * dW; }  Ao dimensionar os passos por dW, a soma será igual à área total ou volume da função integral. Como dW escala cada passo discreto, este pode ser pensado como $d\\omega_i$ na equação de reflectância. Matematicamente $d \\omega_i$ é o símbolo contínuo sobre o qual calculamos a integral e, embora não se relacione diretamente com o dW no código (já que este é um passo discreto da soma de Riemann), ajuda a pensar nisso desta maneira. Tenha em mente que tomar medidas discretas sempre nos dará uma aproximação da área total da função. Um leitor cuidadoso perceberá que podemos aumentar a precisão da soma do Riemann, aumentando o número de passos.\nA equação de reflectância soma a radiância de todas as direções de luzes incidentes $\\omega_i$ sobre o hemisfério $\\Omega$ dimensionada por $f_r$ que atingem o ponto $p$ e retorna a soma da luz refletida $L_o$ na direção do observador. A radiância recebida pode vir de fontes de luz, como estamos familiarizados ou de um environment map medindo a radiância de todas as direções incidentes, como vamos discutir nos capítulos de IBL.\nAgora, a única parte desconhecida que resta é o símbolo $f_r$ conhecido como a função de distribuição reflexiva bidirecional BRDF  ( bidirectional reflective distribution function ) que escala ou pesa a radiância recebida com base nas propriedades do material da superfície.\nBRDF A BRDF  , ou bidirectional reflective distribution function  , é uma função que recebe como entrada a direção de incidência (luz) $\\omega_i$, a direção de saída (observador) $\\omega_o$, a normal da superfície $n$ e um parâmetro de superfície $a$ que representa a rugosidade do micro-superfície. A BRDF aproxima o quanto cada raio individual de luz $\\omega_i$ contribui para a luz final refletida de uma superfície opaca, dadas suas propriedades materiais. Por exemplo, se a superfície for perfeitamente lisa (~ como um espelho), a função BRDF retornaria 0.0 para todos os raios de luz incidentes $\\omega_i$, exceto o único raio que tem o mesmo ângulo (refletido) como o raio de reflexão $\\omega_o$ em que a função retorna 1.0.\nUma BRDF aproxima as propriedades reflexivas e refrativas do material com base na teoria de microfacet discutida anteriormente. Para uma BRDF ser fisicamente plausível, precisa que respeitar a lei da conservação de energia, isto é, a soma de luz refletida nunca deve exceder a quantidade de luz incidente. Tecnicamente, Blinn-Phong é considerado uma BRDF que toma o mesmo $\\omega_i$ e $\\omega_o$ como entradas. No entanto, Blinn-Phong não é considerado ser fisicamente baseado já que não adere ao princípio da conservação de energia. Existem várias BRDFs baseadas em física para aproximar a reação da superfície à luz. No entanto, quase todos os pipelines de renderização de PBR em tempo real usam uma BRDF conhecido como a Cook-Torrance BRDF  .\nA Cook-Torrance BRDF contém uma parte difusa e uma parte especular:\n\\[f_r=k_df_{lambert} + k_sf_{cook-torrance}\\]\nAqui $k_d$ é a relação anterior mencionada de energia da luz incidente que é refratada com $k_s$ sendo a proporção refletida. O lado esquerdo da BRDF afirma a parte difusa da equação denotada aqui como $f_{Lambert}$. Isso é conhecido como Lambertian  semelhante ao que usamos para diffuse shading, que é um fator constante denotado como:\n\\[f_{lambert}=\\frac{c}{\\pi}\\]\nCom $c$ sendo o albedo ou cor da superfície (pense na textura de superfície difusa). A divisão por $\\pi$ está lá para normalizar a luz difusa, já que a integral anterior que contém a BRDF é dimensionada por $\\pi$ (chegaremos a isso nos capítulos de IBL).\nVocê pode se perguntar como esse Lambertian diffuse se relaciona com a iluminação difusa que usamos antes: a cor da superfície multiplicada pelo produto escalar entre a normal da superfície e a direção da luz. O produto escalar ainda está lá, mas saiu da BRDF como encontramos $n \\cdot \\omega_i$ no final da integral $L_o$.\n Existem diferentes equações para a parte difusa da BRDF, que tendem a parecer mais realistas, mas também são mais custosas computacionalmente. Como concluído pela Epic Games, no entanto, o Lambertian diffuse é suficiente para a maioria dos propósitos de renderização em tempo real.\nA parte especular da BRDF é um pouco mais avançada e é descrita como:\n\\[f_{CookTorrance}=\\frac{DFG}{4(\\omega_o\\cdot n)(\\omega_i\\cdot n)}\\]\nA BRDF especular de Cook-Torrance é composta de três funções e um fator de normalização no denominador. Cada um dos símbolos $D$, $F$ e $G$ representam um tipo de função que aproxima uma parte específica das propriedades reflexivas da superfície. Estes são definidos como a função de Distribuição normal, a equação de Fresnel e a função de Geometria:\n Função de distribuição normal: aproxima o quanto os microfacets da superfície estão alinhados ao vetor halfway, influenciada pela rugosidade da superfície; Esta é a função principal que aproxima os microfacets.\n Função da geometria: descreve a propriedade de auto-sombreamento dos microfacets. Quando uma superfície é relativamente áspera, os microfacets da superfície podem ofuscar outros microfacets reduzindo a luz que a superfície reflete.\n Equação de Fresnel: A equação de Fresnel descreve a proporção de reflexão da superfície em diferentes ângulos de superfície.\n  Cada uma dessas funções é uma aproximação de seus equivalentes físicos e você encontrará mais de uma versão de cada uma que visa aproximar a física de maneiras diferentes; alguns mais realistas, outros mais eficientes. É perfeitamente normal escolher qualquer versão aproximada dessas funções que você deseja usar. Brian Karis da Epic Games fez uma grande pesquisa sobre os vários tipos de aproximações aqui. Nós vamos escolher as mesmas funções usadas pela Unreal Engine 4 da Epic Games que são: Trowbridge-Reitz GGX para D, a aproximação de Fresnel-Schlick para F, e o Schlick-GGX de Smith para G.\nFunção de Distribuição Normal (NDF) A função de distribuição normal  $D$ aproxima estatisticamente a área de superfície relativa de microfacets exatamente alinhados ao vetor (halfway) $h$. Há uma infinidade de NDFs que aproximam estatisticamente do alinhamento geral dos microfacets dado algum parâmetro de rugosidade. O que estaremos usando é conhecido como o Trowbridge-Reitz GGX:\n\\[NDF_{GGXTR}(n,h,\\alpha)=\\frac{\\alpha^2}{\\pi((n\\cdot h)^2(\\alpha^2-1)+1)^2}\\]\nAqui $h$ é o vetor halfway para medir contra os microfacets da superfície, com $\\alpha$ sendo uma medida da aspereza da superfície. Se tomarmos $h$ como o vetor halfway entre a normal da superfície e a direção da luz variando os parâmetros de rugosidade, recebemos o seguinte resultado visual:\n\nQuando a rugosidade é baixa (assim a superfície é suave), um número altamente concentrado de microfacets está alinhado a vetores halfway sobre um raio pequeno. Devido a esta alta concentração, a NDF exibe um ponto muito brilhante. Em uma superfície áspera no entanto, onde os microfacets estão alinhados em direções muito mais aleatórias, você encontrará um número muito maior de vetores halfway $h$ um meramente alinhados aos microfacets (mas menos concentrados), nos dando os resultados mais acinzentados .\nEm GLSL, a função de distribuição normal Trowbridge-Reitz GGX é traduzida para o seguinte código:\nfloat DistributionGGX(vec3 N, vec3 H, float a) { float a2 = a*a; float NdotH = max(dot(N, H), 0.0); float NdotH2 = NdotH*NdotH; float nom = a2; float denom = (NdotH2 * (a2 - 1.0) + 1.0); denom = PI * denom * denom; return nom / denom; }  Função de Geometria A função de geometria aproxima estatisticamente a área de superfície relativa, onde seus micro detalhes de superfície se auto obstruem, fazendo com que os raios de luz sejam ocluídos.\n\nSemelhante ao NDF, a função de geometria recebe o parâmetro de rugosidade de um material como entrada, com superfícies mais rígidas tendo uma maior probabilidade de ocluir microfacets. A função de geometria que usaremos é uma combinação da aproximação GGX e Schlick-Beckmann conhecida como Schlick-GGX:\n\\[G_{SchlickGGX}(n,v,k)=\\frac{n\\cdot v}{(n\\cdot v}(1-k) + k\\]\nAqui $k$ é um remapeamento de $\\alpha$ com base em se estamos usando a função de geometria para iluminação direta ou iluminação IBL:\n\\[k_{direct}=\\frac{(\\alpha + 1)^2}{8}\\]\n\\[k_{IBL}=\\frac{\\alpha^2}{2}\\]\nObserve que o valor de $\\alpha$ pode ser diferente com base em como seu mecanismo traduz a rugosidade $\\alpha$. Nos capítulos a seguir, discutiremos extensivamente como e onde esse remapeamento se torna relevante.\nPara efetivamente aproximar a geometria, precisamos levar em conta tanto a direção do observador (obstrução da geometria) quanto o vetor de direção da luz (sombreamento de geometria). Podemos levar ambos em conta usando o método de Smith  :\n\\[G(n,v,l,k)=G_{sub}(n,v,k)G_{sub}(n,l,k)\\]\nUsando o método de Smith com Schlick-GGX como $G_{sub}$ fornece a seguinte aparência visual variando-se a rugosidade R:\n\nA função de geometria é um multiplicador entre [0.0, 1.0] com 1.0 (ou branco) medindo nenhum sombreamento de microfacet, e 0.0 (ou preto), sombreamento completo de microfacet.\nEm GLSL, a função de geometria se traduz no seguinte código:\nfloat GeometrySchlickGGX(float NdotV, float k) { float nom = NdotV; float denom = NdotV * (1.0 - k) + k; return nom / denom; } float GeometrySmith(vec3 N, vec3 V, vec3 L, float k) { float NdotV = max(dot(N, V), 0.0); float NdotL = max(dot(N, L), 0.0); float ggx1 = GeometrySchlickGGX(NdotV, k); float ggx2 = GeometrySchlickGGX(NdotL, k); return ggx1 * ggx2; }  Equação de Fresnel A equação de Fresnel (pronunciada como freh-nel) descreve a proporção de luz que se reflete sobre a luz que é refratada, o que varia sobre o ângulo que estamos olhando para uma superfície. No momento em que a luz atinge uma superfície, com base no ângulo da superfície com a vista, a equação de Fresnel nos diz a porcentagem de luz que se reflete. A partir dessa proporção de reflexão e do princípio da conservação de energia, podemos obter diretamente a porção refratada da luz.\nCada superfície ou material tem um nível de refletividade base ( base reflectivity  ) quando olhamos diretamente para sua superfície, mas ao olhar para a superfície de um ângulo, todas as reflexões se tornam mais aparentes em comparação com a refletividade base da superfície. Você pode verificar isso por si mesmo olhando para a sua mesa (presumivelmente) de madeira / metálica que tem um certo nível de refletividade base de um ângulo de visão perpendicular, mas olhando para sua mesa de um ângulo de quase 90 graus, você verá as reflexões muito mais aparentes. Todas as superfícies teoricamente refletem totalmente a luz se vistas de ângulos perfeitos de 90 graus. Este fenômeno é conhecido como Fresnel  e é descrito pela equação de Fresnel.\nA equação de Fresnel é uma equação bastante complexa, mas felizmente pode ser aproximada usando a aproximação Fresnel-Schlick:\n\\[F_{Schlick}(h,v,F_0)=F_0+(1-F_0)(1-(h\\cdot v))^5\\]\n$F_0$ representa a refletividade base da superfície, que calculamos usando algo chamado os índices de refração ou IOR. Como você pode ver em uma superfície de esfera, quanto mais olhamos para os ângulos rasos da superfície (com o ângulo entre os vetores halfway e do observador atingindo 90 graus), mais forte o Fresnel e, portanto, as reflexões:\n\nExistem algumas sutilezas envolvidas com a equação de Fresnel. Uma é que a aproximação Fresnel-Schlick só é realmente definida para superfícies dielétricas  ou não metálicas. Para superfícies condutoras  (metais), o cálculo da refletividade base com índices de refração não se mantém adequadamente e precisamos usar uma equação de fresnel diferente para os condutores. Como isso é inconveniente, aproximamos ainda mais pela pré-computação da resposta da superfície em incidência normal  ($F_0$) em um ângulo de 0 grau como se estivesse olhando diretamente em uma superfície. Nós interpolamos esse valor com base no ângulo de visão, conforme a aproximação Fresnel-Schlick, de modo que podemos usar a mesma equação para metais e não-metais.\nA resposta da superfície em incidência normal, ou a refletividade base, pode ser encontrada em grandes bancos de dados, como estes com alguns dos valores mais comuns listados abaixo, conforme tirado das notas do curso de Naty Hoffman:\n   Material $F_0$(Linear) $F_0$(sRGB) Cor     Water $(0.02, 0.02, 0.02)$ $(0.15, 0.15, 0.15)$    Plastic / Glass (Low) $(0.03, 0.03, 0.03)$ $(0.21, 0.21, 0.21)$   Plastic High $(0.05, 0.05, 0.05)$ $(0.24, 0.24, 0.24)$   Glass (high) / Ruby $(0.08, 0.08, 0.08)$ $(0.31, 0.31, 0.31)$   Diamond $(0.17, 0.17, 0.17)$ $(0.45, 0.45, 0.45)$   Iron $(0.56, 0.57, 0.58)$ $(0.77, 0.78, 0.78)$   Copper $(0.95, 0.64, 0.54)$ $(0.98, 0.82, 0.76)$   Gold $(1.00, 0.71, 0.29)$ $(1.00, 0.86, 0.57)$   Aluminium $(0.91, 0.92, 0.92)$ $(0.96, 0.96, 0.97)$   Silver $(0.95, 0.93, 0.88)$ $(0.98, 0.97, 0.95)$    O que é interessante de observar aqui é que, para todas as superfícies dielétricas, a refletividade base nunca fica acima de 0.17, que é a exceção, em vez da regra, enquanto para os condutores a refletividade base é muito maior e (principalmente) varia entre 0.5 e 1.0. Além disso, para condutores (ou superfícies metálicas) a refletividade base é colorida. É por isso que $F_0$ é apresentado como uma tupla RGB (a refletividade na incidência normal pode variar por comprimento de onda); Isso é algo que só vemos em superfícies metálicas.\nEsses atributos específicos de superfícies metálicas em comparação com superfícies dielétricas deu origem a algo chamado metallic workflow  . No metallic workflow, nós configuramos os materiais das superfícies com um parâmetro extra conhecido como metalness  que descreve se uma superfície é uma superfície metálica ou não metálica.\nTeoricamente, o metalness de um material é binário: o material é um metal ou não é; Não pode ser ambos. No entanto, a maioria dos pipelines de renderização permitem a configuração do metalness de uma superfície linearmente entre 0.0 e 1.0. Isso é principalmente por causa da falta de precisão de textura do material. Por exemplo, uma superfície com pequenas partículas / arranhões de pó / areia (não metal) sobre uma superfície metálica é difícil de renderizar com valores de metalness binários.\n Ao pré-computar $F_0$ para ambos os dielétricos e condutores, podemos usar a mesma aproximação de Fresnel-Schlick para ambos os tipos de superfícies, mas temos que tingir a refletividade base se tivermos uma superfície metálica. Geralmente, conseguimos isso da seguinte forma:\nvec3 F0 = vec3(0.04); F0 = mix(F0, surfaceColor.rgb, metalness);  Definimos uma refletividade base que é aproximada para a maioria das superfícies dielétricas. Esta é mais uma aproximação como $F_0$ é calculada em torno de dielétricos mais comuns. Uma refletividade base de 0.04 se mantém para a maioria dos dielétricos e produz resultados fisicamente plausíveis sem ter que criar um parâmetro de superfície adicional. Em seguida, com base em quão metálica uma superfície é, tomamos a refletividade base dielétrica ou tomamos $F_0$ criadas como a cor da superfície. Como as superfícies metálicas absorvem toda a luz refratada, elas não têm reflexões difusas e podemos usar diretamente a textura de cor de superfície como sua refletividade base.\nNo código, a aproximação de Fresnel Schlick se traduz em:\nvec3 fresnelSchlick(float cosTheta, vec3 F0) { return F0 + (1.0 - F0) * pow(1.0 - cosTheta, 5.0); }  Com a cosTheta sendo o resultado do produto escalar entre a direção normal $n$ e a direção halfway $h$ (ou de vista $v$).\nEquação de Refletância Cook-Torrance Com cada componente da BRDF Torrance do Cook descrita, podemos incluir a BRDF baseada em física na equação de reflectância final agora:\n\\[L_0(p,\\omega_o)=\\int_\\Omega(k_d\\frac{c}{\\pi} + k_s\\frac{DFG}{4(\\omega_o\\cdot n)(\\omega_i\\cdot n)})L_i(p,\\omega_i)n\\cdot\\omega_i d\\omega_i\\]\nEsta equação não é matematicamente totalmente correta no entanto. Você pode lembrar que o termo Fresnel $F$ representa a proporção de luz que é refletida de uma superfície. Isso é efetivamente nossa relação $k_s$, o que significa que a parte (BRDF) especular da equação de refletância contém implicitamente a relação de reflectância $k_s$. Dado isso, nossa equação final de refletância final torna-se:\n\\[L_0(p,\\omega_o)=\\int_\\Omega(k_d\\frac{c}{\\pi} + \\frac{DFG}{4(\\omega_o\\cdot n)(\\omega_i\\cdot n)})L_i(p,\\omega_i)n\\cdot\\omega_i d\\omega_i\\]\nEssa equação agora descreve completamente um modelo de renderização fisicamente baseado que é geralmente reconhecido como o que comumente entendemos como renderização baseada em física, ou PBR. Não se preocupe se você ainda não entendeu completamente como precisaremos adequar todas as matemáticas discutidas no código. Nos próximos capítulos, vamos explorar como utilizar a equação de refletância para obter resultados muito mais fisicamente plausíveis em nossa iluminação renderizada e todos os pedacinhos devem começar lentamente a se encaixar.\nCriando (Authoring) Materiais PBR Com o conhecimento do modelo matemático subjacente de PBR, finalizaremos a discussão descrevendo como os artistas geralmente criam as propriedades físicas de uma superfície com as quais podemos alimentar diretamente as equações da PBR. Cada um dos parâmetros da superfície que precisamos para um pipeline PBR pode ser definido ou modelado por texturas. O uso de texturas nos dá controle por fragmento sobre como cada ponto de superfície específico deve reagir à luz: se esse ponto é metálico, áspero ou suave, ou como a superfície responde a diferentes comprimentos de onda.\nAbaixo, você verá uma lista de texturas que você frequentemente encontrará em um pipeline PBR junto com sua saída visual, se fornecida a um renderizador de PBR:\n\nAlbedo: A textura de albedo  ( albedo texture  ) especifica para cada texel a cor da superfície, ou a refletividade base se esse texel é metálico. Isso é muito semelhante ao que usamos antes como uma textura difusa, mas todas as informações de iluminação são extraídas da textura. Texturas difusas muitas vezes têm ligeiras sombras ou fendas escuras dentro da imagem que é algo que você não quer em uma textura de albedo; Deve-se conter apenas as cores (ou coeficientes de absorção refratados) da superfície.\nNormal: A textura do mapa de normais ( normal map texture  ) é exatamente como usamos antes no capítulo mapeamento de normais. O mapa de normais nos permite especificar, por fragmento, uma normal única para dar a ilusão de que uma superfície é mais curvada ( bumpier ) do que sua contraparte plana.\nMetálico: O mapa metálico ( metallic map  ) especifica por texel se um texel é metálico ou não é. Com base em como o motor PBR é configurado, os artistas podem criar o metalness tanto com valores de cinza ou com valores binários de preto ou branco.\nRugosidade: O mapa de rugosidade ( roughness map  ) especifica o quão áspero uma superfície é para cada texel. O valor de rugosidade amostrado da rugosidade influencia as orientações estatísticas de microfacets da superfície. Uma superfície mais áspera recebe reflexões mais amplas e borradas, enquanto uma superfície lisa é focada e reflexões bem definidas. Alguns motores PBR esperam um mapa de suavidade ( smoothness map  ) em vez de um mapa de rugosidade que alguns artistas acham mais intuitivo. Esses valores são então traduzidos (1,0 - smoothness) para aspereza no momento em que eles são amostrados.\nAO: A oclusão ambiente ( ambient occlusion  ) ou mapa AO  especifica um fator de sombreamento extra da superfície e geometrias potencialmente próximas. Se tivermos uma superfície de tijolo, por exemplo, a textura do albedo não deve ter informações de sombreamento dentro das fendas do tijolo. O mapa do AO, no entanto, especifica essas bordas escuras, pois é mais difícil para a luz escapar. Tomar a oclusão ambiente em conta no final do estágio de iluminação pode aumentar significativamente a qualidade visual de sua cena. O mapa de oclusão ambiente de uma malha / superfície é gerado manualmente ou pré-calculado em programas de modelagem 3D.\nArtistas definem e ajustam esses valores de entrada fisicamente baseados em uma base por texel e podem basear seus valores de textura nas propriedades da superfície física dos materiais do mundo real. Esta é uma das maiores vantagens de um pipeline de renderização do PBR, uma vez que essas propriedades físicas de uma superfície permanecem as mesmas, independentemente do ambiente ou configuração de iluminação, facilitando a vida para os artistas obterem resultados fisicamente plausíveis. As superfícies criadas em um pipeline PBR podem ser facilmente compartilhadas entre os diferentes motores de renderização da PBR, e ficarão corretos, independentemente do ambiente em que estão e, como resultado, parecerão muito mais naturais.\nLeitura Adicional  \u0026quot;Background: Physics and Math of Shading\u0026quot; por Natur Hoffmann: Há muita teoria para discutir plenamente em um único artigo, logo a teoria aqui mal arranha sua superfície; Se você quiser saber mais sobre a física da luz e como esta se refere à teoria do PBR, este é o recurso que você deseja ler.\n Real Shading in Unreal Engine 4: discute o modelo PBR adotado pela Epic Games em sua 4ª versão da Unreal Engine. O sistema PBR que nos concentraremos nestes capítulos é baseado neste modelo de PBR.\n \u0026quot;SH17C Physically Based Shading\u0026quot; por knarkowicz: Grande showcase de todos os elementos individuais de PBR em uma demo interativa do Shadertoy.\n Marmoset: PBR Theory: uma introdução à PBR principalmente destinada a artistas, mas, no entanto, uma boa leitura.\n Coding Labs: Physically based rendering: uma introdução à equação de renderização e como se refere ao PBR.\n Coding Labs: Physically Based Rendering - Cook–Torrance: uma introdução ao cozinheiro-torrance BRDF.\n Wolfire Games - Physically based rendering: uma introdução ao PBR por Lukas Orsvärn.\n  "
},
{
	"uri": "https://filipecn.github.io/aprendaopengl/ponto_de_partida/transformacoes/",
	"title": "Transformações",
	"tags": [],
	"description": "",
	"content": "Post Original\nSabemos agora como criar objetos, cor-los e / ou dar-lhes uma aparência detalhada usando texturas, mas ainda não é tão interessante, pois eles estão todos os objetos estáticos. Poderíamos tentar e fazê-los passar por mudar seus vértices e re-configurar seus buffers cada quadro, mas isso é complicado e custos bastante poder de processamento. Há muito melhores maneiras de transformar um objeto e isso é usando (múltiplos) objetos de matriz. Isso não significa que nós estamos indo falar sobre Kung Fu e um grande mundo artificial digital.\nMatrizes são muito poderosas construções matemáticas que parecem assustador no começo, mas uma vez que você vai se acostumar a eles que eles vão revelar-se extremamente útil. Ao discutir matrizes, nós vamos ter que fazer um pequeno mergulhar em um pouco de matemática e para os leitores mais matematicamente Vou postar recursos adicionais para outras leituras.\nNo entanto, para compreender plenamente as transformações primeiro temos que aprofundar um pouco mais em vetores antes de discutir matrizes. O foco deste capítulo é dar-lhe um fundo matemática básica em temas que vai exigir mais tarde. Se as matérias são difíceis, tentar compreendê-los tanto quanto você pode e voltar a este capítulo mais tarde para rever os conceitos sempre que você precisar deles.\nVectors Em sua definição mais básica, vetores são sentidos e nada mais. Um vector tem uma direcção e uma amplitude (também conhecida como a sua resistência ou comprimento). Você pode pensar em vetores como instruções sobre um mapa do tesouro: 'Vá para a esquerda 10 passos, agora vá norte 3 etapas e ir direto 5 passos'; aqui 'esquerda' é a direção e '10 passos é a magnitude do vetor. As indicações para o mapa do tesouro, portanto, contém 3 vetores. Vetores podem ter qualquer dimensão, mas que geralmente trabalham com dimensões de 2 a 4. Se um vetor tem 2 dimensões que representa uma direção em um avião (pense em gráficos 2D) e quando ele tem 3 dimensões pode representar qualquer direção em um 3D mundo.\nAbaixo, você verá 3 vetores, onde cada vetor é representado com (x, y) como flechas em um gráfico 2D. Porque é mais intuitivo para exibir vetores em 2D (ao invés de 3D) você pode pensar dos 2D vetores como 3D vetores com uma coordenada z de 0. Desde vetores representam as direções, a origem do vector não muda o seu valor. No gráfico abaixo, podemos ver que os vetores \\ (\\ color {vermelho} {\\ bar {v}} ) e \\ (\\ color {azul} {\\ bar {w}} ) são iguais mesmo que sua origem é diferente:\n\nQuando vetores que descrevem os matemáticos preferem geralmente para descrever vetores como símbolos de caracteres com um pouco de bar sobre sua cabeça como \\ (\\ bar {v} ). Além disso, quando visualizadas em vectores de fórmulas eles são geralmente apresentados como se segue:\n\\ [\\ Bar {V} = \\ begin {pmatrix} \\ cor vermelho x \\ \\ cor verde y \\ \\ cor azul z \\ final {pmatrix} ]\nPorque vetores são especificados como direções às vezes é difícil visualizá-los como posições. Se quisermos visualizar vetores como posições podemos imaginar a origem do vetor de direção para ser (0,0,0) e depois apontar para uma certa direção que especifica o ponto, tornando-se um vetor posição (que também pode especificar um diferente origem e, em seguida, dizer: 'este vetor aponta para esse ponto no espaço a partir desta origem'). O vector de posição (3,5), em seguida, iria apontar para (3,5) no gráfico com uma origem de (0,0). Usando vetores podemos assim descrever orientações e posições no espaço 2D e 3D.\nAssim como com números normais também pode definir várias operações em vetores (alguns dos quais você já visto).\nScalar vector operations Um escalar é um único dígito. Ao adicionar / subtrair / multiplicar ou dividir um vetor com um escalar nós simplesmente adicionar / subtrair / multiplicar ou dividir cada elemento do vector por escalar. Para além disso, ficaria assim:\n\\ [\\ Begin {pmatrix} \\ cor vermelho 1 \\ \\ cor verde 2 \\ \\ cor azul 3 \\ final {pmatrix} + x \\ rightarrow \\ begin {pmatrix} \\ cor vermelho 1 \\ \\ \\ cor verde 2 \\ \\ cor azul 3 \\ final {pmatrix} + \\ begin {pmatrix} x \\ \\ x x \\ final {pmatrix} = \\ begin {pmatrix} \\ cor vermelho {1} + x \\ \\ cor verde 2 + x \\ \\ cor azul 3 + x \\ final {pmatrix} ]\nOnde \\ (+ ) pode ser \\ (+ ), \\ (- ), \\ (\\ cdot ) ou \\ (\\ div ) onde \\ (\\ cdot ) é o operador de multiplicação.\nVector negation Negando um vector resulta em um vector na direcção inversa. Um vetor que aponta Nordeste apontaria Sudoeste depois de negação. Para negar um vetor que adicionar uma menos-sinal para cada componente (você também pode representá-lo como uma multiplicação escalar-vetor com um valor escalar -1):\n\\ [- \\ bar {v} = - \\ begin {pmatrix} \\ color {vermelho} {v_x} \\ \\ color {azul} {v_y} \\ \\ color {verde} {v_z} \\ end {pmatrix} = \\ begin {pmatrix} - \\ color {vermelho} {v_x} \\ - \\ color {azul} {v_y} \\ - \\ color {verde} {v_z} \\ end {pmatrix} ]\nAddition and subtraction A adição de dois vectores é definida como a adição do componente-sábio, isto é, cada componente de um vector é adicionado ao mesmo componente do outro vector da seguinte forma:\n\\ [\\ Bar {V} = \\ begin {pmatrix} \\ cor vermelho 1 \\ \\ cor verde 2 \\ \\ cor azul 3 \\ final {pmatrix}, \\ bar {k} = \\ {começar pmatrix} \\ cor vermelho 4 \\ \\ cor verde 5 \\ \\ cor azul 6 \\ final {pmatrix} \\ rightarrow \\ bar {v} + \\ bar {k} = \\ begin {pmatrix} \\ cor {vermelho} 1 + \\ cor vermelho 4 \\ \\ cor verde 2 + \\ cor verde 5 \\ \\ cor azul 3 + \\ cor azul 6 \\ final {pmatrix} = \\ {começar pmatrix} \\ cor vermelho 5 \\ \\ cor verde 7 \\ \\ cor azul 9 \\ final {pmatrix} ]\nVisualmente, parece que esta em vectores de v = (4,2) e k = (1,2), em que o segundo vector é adicionado em cima da extremidade do primeiro vector para encontrar o ponto final do vector resultante (cabeça-a método -tail):\n\nAssim como adição normal e subtracção, vector de subtracção é o mesmo como de adição com um segundo vector negada:\n\\ [\\ Bar {V} = \\ begin {pmatrix} \\ cor vermelho {1} \\ \\ cor verde {2} \\ \\ cor azul {3} \\ final {pmatrix}, \\ bar { k} = \\ begin {pmatrix} \\ cor vermelho {4} \\ \\ cor verde {5} \\ \\ cor azul {6} \\ final {pmatrix} \\ rightarrow \\ bar {v} + - \\ bar {k} = \\ begin {pmatrix} \\ cor vermelho {1} + (- \\ cor vermelho {4}) \\ \\ cor verde {2} + (- \\ cor verde { 5}) \\ \\ cor azul {3} + (- \\ cor azul {6}) \\ final {pmatrix} = \\ begin {pmatrix} - \\ cor vermelho {3} \\ - \\ cor {verde} {3} \\ - \\ color {azul} {3} \\ end {pmatrix} ]\nSubtraindo-se dois vectores de uns aos outros resulta num vector que é a diferença das posições de ambos os vectores estão a apontar na. Isso prova útil em certos casos em que precisamos para recuperar um vector que é a diferença entre dois pontos.\n\nLength Para recuperar o comprimento / magnitude de um vetor usamos o teorema de Pitágoras que você pode se lembrar de suas aulas de matemática. Um vector de forma um triângulo quando a visualizar a sua x individual e componente y como dois lados de um triângulo:\n\nDesde o comprimento dos dois lados (x, y) são conhecidos e queremos saber o comprimento do lado inclinado \\ (\\ color {vermelho} {\\ bar {v}} ) podemos calcular isso usando o teorema de Pitágoras como :\n\\ [|| \\ color {vermelho} {\\ bar {v}} || = \\ Sqrt {\\ cor verde x ^ 2 + \\ cor azul y ^ 2} ]\nOnde \\ (|| \\ cor vermelho {\\ bar {v}} || ) é indicada como o comprimento de vector de \\ (\\ cor vermelho {\\ bar {v}} ). Isto é facilmente estendido para 3D adicionando \\ (z ^ 2 ) para a equação.\nNeste caso, o comprimento de vector de (4, 2) é igual a:\n\\ [|| \\ color {vermelho} {\\ bar {v}} || = \\ Sqrt {\\ cor {verde} 4 ^ 2 + \\ cor {azul} 2 ^ 2} = \\ sqrt {\\ cor {verde} 16 + \\ cor {azul} 4} = \\ sqrt {20} = 4,47 ]\nQue é 4,47.\nHá também um tipo especial de vetor que nós chamamos um vetor unitário. Um vetor de unidade tem uma propriedade extra e isso é que seu comprimento é exatamente 1. Nós podemos calcular um vetor de unidade \\ (\\ hat {n} ) a partir de qualquer vector, dividindo cada um dos componentes do vetor por seu comprimento:\n\\ [\\ Chapéu {n} = \\ frac {\\ bar {v}} {|| \\ bar {V} ||} ]\nChamamos isso de normalizar um vetor. vetores unitários são exibidos com um pouco de teto sobre sua cabeça e geralmente são mais fáceis de trabalhar, especialmente quando só se preocupam com suas direções (a direção não muda se mudarmos o comprimento de um vetor).\nVector-vector multiplication A multiplicação de dois vetores é um pouco de um caso estranho. multiplicação normal não está realmente definido em vetores, uma vez que não tem sentido visual, mas temos dois casos específicos que poderíamos escolher quando multiplicando: um é o produto escalar denotado como \\ (\\ bar {v} \\ cdot \\ bar {k } ) e o outro é o produto cruzado denotado como \\ (\\ bar {v} \\ \\ vezes bar {k} ).\nO produto escalar de dois vectores é igual ao produto escalar dos seus comprimentos vezes o co-seno do ângulo entre elas. Se isso soa confuso dar uma olhada em sua fórmula:\n\\ [\\ Bar {V} \\ cdot \\ bar {k} = || \\ bar {V} || \\ Cdot || \\ bar {k} || \\ Cdot \\ cos \\ theta ]\nQuando o ângulo entre eles é representado como teta (\\ (\\ theta )). Por que isso é interessante? Bem, imagine se \\ (\\ bar {v} ) e \\ (\\ bar {k} ) são vetores unitários, em seguida, seu comprimento seria igual a 1. Isso efetivamente reduzir a fórmula para:\n\\ [\\ Chapéu {V} \\ cdot \\ chapéu {k} = 1 \\ cdot 1 \\ cdot \\ cos \\ teta = \\ cos \\ teta ]\nAgora, o produto escalar só define o ângulo entre os dois vetores. Você pode se lembrar que o co-seno ou cos função se torna 0 quando o ângulo é de 90 graus ou 1 quando o ângulo é 0. Isso nos permite facilmente testar se os dois vetores são ortogonais ou paralelas entre si usando o produto de ponto (meio ortogonais as Os vectores estão em ângulo recto um com o outro). No caso de você querer saber mais sobre o pecado ou o cos funções sugiro os seguintes vídeos Khan Academy sobre trigonometria básica.\n(https://www.khanacademy.org/math/trigonometry/basic-trigonometry/basic_trig_ratios/v/basic-trigonometry)\nVocê também pode calcular o ângulo entre dois vetores não-unitárias, mas então você teria que dividir os comprimentos de ambos vetores do resultado a ser deixado com \\ (cos \\ theta ).\n Então, como podemos calcular o produto dot? O produto escalar é uma multiplicação componente-wise onde adicionar os resultados juntos. Parece que este com dois vetores unitários (você pode verificar que ambos os seus comprimentos são exatamente 1):\n\\ [\\ Begin {pmatrix} \\ cor vermelho {0,6} \\ - \\ cor verde {0,8} \\ \\ cor azul 0 \\ final {pmatrix} \\ cdot \\ begin {pmatrix} \\ {cor vermelho } 0 \\ \\ cor verde 1 \\ \\ cor azul 0 \\ final {pmatrix} = (\\ cor vermelho {0,6} * \\ cor vermelho 0) + (- \\ cor verde { 0,8} * \\ cor verde 1) + (\\ cor azul 0 * \\ cor azul 0) = -0,8 ]\nPara calcular o grau entre estes dois vectores unitários que usamos o inverso da função de coseno \\ cos (^ {- 1} ) e isto resulta em 143,1 graus. Nós agora efetivamente calculado o ângulo entre esses dois vetores. O produto escalar é muito útil ao fazer cálculos de iluminação mais tarde.\nO produto cruzado é apenas definida no espaço 3D e leva dois vectores não-paralelas como entrada e produz um terceiro vector que é ortogonal tanto para os vectores de entrada. Se ambos os vectores de entrada são ortogonais entre si, bem como, um produto cruzado resultaria em 3 vectores ortogonais; isso vai ser útil nos próximos capítulos. Os seguintes imagem mostra que isso parece no espaço 3D:\n\nAo contrário das outras operações, o produto cruzado não é realmente intuitiva, sem aprofundar em álgebra linear por isso é melhor apenas memorizar a fórmula e você vai ficar bem (ou não fizer isso, você provavelmente vai ficar bem também). Abaixo, você verá o produto cruzado entre dois vetores ortogonais A e B:\n\\ [\\ Begin {pmatrix} \\ cor vermelho {A_ {x}} \\ \\ cor verde {A_ {y \\}} \\ cor azul {A_ {z}} \\ final {pmatrix} \\ \\ vezes começam {pmatrix} \\ cor vermelho {B_ {x}} \\ \\ cor verde {B_ {y \\}} \\ cor azul {B_ {z}} \\ final {pmatrix} = \\ begin {pmatrix} \\ color {verde} {A_ {y}} \\ cdot \\ color {azul} {B_ {z}} - \\ color {azul} {A_ {z}} \\ cdot \\ color {verde} {B_ { y}} \\ \\ cor azul {A_ {z}} \\ cdot \\ cor vermelho {B_ {x}} - \\ cor vermelho {A_ {x}} \\ cdot \\ cor azul {B_ {z}} \\ \\ cor vermelho {A_ {x}} \\ cdot \\ cor verde {B_ {y}} - \\ cor verde {A_ {y}} \\ cdot \\ cor vermelho { B_ {x}} \\ end {pmatrix} ]\nComo você pode ver, ele realmente não parece fazer sentido. No entanto, se você apenas seguir estes passos você obterá um outro vector que é perpendicular aos seus vetores de entrada.\nMatrices Agora que nós discutimos quase tudo o que há de vetores é hora de entrar na matriz! A matriz é uma matriz rectangular de números, símbolos e / ou expressões matemáticas. Cada artigo individual numa matriz é chamado um elemento da matriz. Um exemplo de uma matriz de 2x3 é mostrado abaixo:\n\\ [\\ Begin {bmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\ 4 \u0026amp; 5 \u0026amp; 6 \\ final {bmatrix} ]\nAs matrizes são indexadas por (i, j), onde i é a linha e a coluna j é, por isso a matriz acima é chamado uma matriz de 2x3 (3 colunas e linhas 2, também conhecida como as dimensões da matriz). Este é o oposto do que você está acostumado ao indexar 2D gráficos como (x, y). Para recuperar o valor 4 teríamos posicioná-lo como (2,1) (segunda linha, primeira coluna).\nMatrizes são basicamente nada mais do que isso, apenas matrizes retangulares de expressões matemáticas. Eles têm um muito bom conjunto de propriedades matemáticas e, assim como vetores podemos definir várias operações sobre matrizes, a saber: adição, subtração e multiplicação.\nAddition and subtraction Além da matriz e subtração entre duas matrizes é feito em uma base per-elemento. Assim, as mesmas regras gerais aplicam-se de que está familiarizado com os números normais, mas feito sobre os elementos de ambas as matrizes com o mesmo índice. Isto significa que a adição e subtracção só é definida para matrizes com as mesmas dimensões. Uma matriz 3x2 e uma matriz de 2x3 (ou uma matriz 3x3 e uma matriz 4x4) não podem ser adicionados ou subtraídos juntos. Vamos ver como adição de matriz funciona em dois 2x2 matrizes:\n\\ [\\ Begin {bmatrix} \\ cor vermelho 1 \u0026amp; \\ cor vermelho 2 \\ \\ cor verde 3 \u0026amp; \\ cor verde 4 \\ final {bmatrix} + \\ begin {bmatrix} \\ {cor vermelho } 5 \u0026amp; \\ cor vermelho 6 \\ \\ cor verde 7 \u0026amp; \\ cor verde 8 \\ final {bmatrix} = \\ begin {bmatrix} \\ cor vermelho 1 + \\ cor vermelho 5 \u0026amp; \\ cor vermelho 2 + \\ cor vermelho 6 \\ \\ cor verde 3 + \\ cor verde 7 \u0026amp; \\ cor verde + 4 \\ cor verde 8 \\ final {bmatrix} = \\ {começar bmatrix} \\ cor vermelho 6 \u0026amp; \\ cor vermelho 8 \\ \\ cor verde 10 \u0026amp; \\ cor verde 12 \\ final {bmatrix} ]\nAs mesmas regras se aplicam para a subtração de matriz:\n\\ [\\ Begin {bmatrix} \\ color {red} 4 \u0026amp; \\ color {red} 2 \\ \\ color {verde} 1 \u0026amp; \\ color {verde} 6 \\ end {bmatrix} - \\ begin {bmatrix} \\ color {vermelho } 2 \u0026amp; \\ color {red} 4 \\ \\ color {verde} 0 \u0026amp; \\ color {verde} 1 \\ end {bmatrix} = \\ begin {bmatrix} \\ color {red} 4 - \\ color {red} 2 \u0026amp; \\ cor {red} 2 - \\ color {red} 4 \\ \\ color {verde} 1 - \\ color {verde} 0 \u0026amp; \\ color {verde} 6 - \\ color {verde} 1 \\ end {bmatrix} = \\ begin { bmatrix} \\ color {red} 2 \u0026amp; - \\ color {red} 2 \\ \\ color {verde} 1 \u0026amp; \\ color {verde} 5 \\ end {bmatrix} ]\nMatrix-scalar products Uma matriz de escalar múltiplos produtos cada elemento da matriz por um escalar. O exemplo a seguir ilustra a multiplicação:\n\\ [\\ Cor verde 2 \\ cdot \\ begin {bmatrix} 1 \u0026amp; 2 \\ 3 \u0026amp; 4 \\ final {bmatrix} = \\ begin {bmatrix} \\ cor verde 2 \\ cdot 1 \u0026amp; \\ cor verde {2} \\ cdot 2 \\ \\ cor verde 2 \\ cdot 3 \u0026amp; \\ cor verde 2 \\ cdot 4 \\ final {bmatrix} = \\ begin {bmatrix} 2 \u0026amp; 4 \u0026amp; 8 6 \\ \\ final {bmatrix} ]\nAgora também faz sentido a respeito de porque esses números individuais são chamados de escalares. Um escalar basicamente escalas todos os elementos da matriz pelo seu valor. No exemplo anterior, todos os elementos foram dimensionadas por dois.\nAté aí tudo bem, todos os nossos casos não foram realmente muito complicado. Isto é, até que começamos a multiplicação de matrizes-matriz.\nMatrix-matrix multiplication Multiplicando as matrizes não é necessariamente complexa, mas bastante difícil de se sentir confortável com. A multiplicação de matrizes significa, basicamente, a seguir um conjunto de regras pré-definidas quando multiplicando. Existem algumas restrições, porém:\nVocê só pode multiplicar duas matrizes se o número de colunas na matriz do lado esquerdo é igual ao número de linhas na matriz do lado direito.\nA multiplicação de matrizes não é comutativa, que é \\ (A \\ cdot B \\ neq B \\ cdot A ).\nVamos começar com um exemplo de uma multiplicação de matrizes de 2 2x2 matrizes:\n\\ [\\ Begin {bmatrix} \\ cor vermelho 1 \u0026amp; \\ cor vermelho 2 \\ \\ cor verde 3 \u0026amp; \\ cor verde 4 \\ final {bmatrix} \\ cdot \\ begin {bmatrix} \\ {cor azul} 5 \u0026amp; \\ cor púrpura 6 \\ \\ cor azul 7 \u0026amp; \\ cor púrpura 8 \\ final {bmatrix} = \\ begin {bmatrix} \\ cor vermelho 1 \\ cdot \\ cor azul {5} + \\ cor 2 \\ cdot \\ cor 7 \u0026amp; \\ cor vermelho 1 \\ cdot \\ cor púrpura 6 + \\ cor vermelho 2 \\ cdot \\ cor púrpura 8 \\ \\ {azul} {vermelho verde} 3 \\ cdot \\ cor {azul} 5 + \\ cor {verde} 4 \\ cdot \\ cor {azul} 7 \u0026amp; \\ cor {verde} 3 \\ cdot \\ cor {roxo} 6 + \\ cor {verde} 4 \\ cdot \\ cor púrpura 8 \\ final {bmatrix} = \\ begin {bmatrix} 19 \u0026amp; 22 \\ 43 \u0026amp; 50 \\ final {bmatrix} ]\nAgora você provavelmente está tentando descobrir o que diabos aconteceu? A multiplicação de matrizes é uma combinação de multiplicação normal e além utilizando linhas de o-matriz esquerda com colunas de o-matriz direita. Vamos tentar discutir isso com a seguinte imagem:\n\nEm primeiro lugar, tomar a linha superior da matriz esquerda e depois tomar uma coluna da matriz direita. A linha e coluna que escolhemos decide que valor da matriz 2x2 resultando vamos calcular a produção. Se tomarmos a primeira linha da matriz esquerda o valor resultante vai acabar na primeira linha da matriz resultado, então nós escolhemos uma coluna e se é a primeira coluna o valor do resultado vai acabar na primeira coluna da matriz resultado . Este é exatamente o caso da via vermelho. Para calcular o resultado inferior direito tomamos a linha inferior da primeira matriz e da coluna mais à direita da segunda matriz.\nPara calcular o valor resultante que multiplicam o primeiro elemento da linha e da coluna em conjunto, utilizando multiplicação normal, fazemos o mesmo para o segundo elementos, terceiro, quarto, etc. Os resultados das multiplicações individuais são então somados e temos o nosso resultado. Agora, também faz sentido que um dos requisitos é que o tamanho das colunas de o-matriz para a esquerda e a matriz-direita de linhas forem iguais, caso contrário, não podemos terminar as operações!\nO resultado é, em seguida, uma matriz que tem dimensões de (n, m), onde n é igual ao número de linhas da matriz do lado esquerdo e m é igual às colunas da matriz do lado direito.\nNão se preocupe se você tem dificuldades imaginando as multiplicações dentro de sua cabeça. Apenas continue tentando fazer os cálculos à mão e retorno a esta página sempre que você tem dificuldades. Com o tempo, a multiplicação de matrizes torna-se uma segunda natureza para você.\nVamos acabar com a discussão de multiplicação de matrizes-matriz com um exemplo maior. Tente visualizar o padrão usando as cores. Como um exercício útil, veja se você pode vir até com sua própria resposta da multiplicação e, em seguida, compará-los com a matriz resultante (uma vez que você tentar fazer uma multiplicação de matrizes à mão você vai rapidamente obter a compreensão deles).\n\\ [\\ Begin {bmatrix} \\ color {red} 4 \u0026amp; \\ color {red} 2 \u0026amp; \\ color {red} 0 \\ \\ color {verde} 0 \u0026amp; \\ color {verde} 8 \u0026amp; \\ color {verde} 1 \\ \\ \\ color {azul} 0 \u0026amp; \\ color {azul} 1 \u0026amp; \\ color {azul} 0 \\ end {bmatrix} \\ cdot \\ begin {bmatrix} \\ color 4 \u0026amp; \\ color 2 \u0026amp; \\ color {verde} {red} { azul} 1 \\ \\ cor vermelho 2 \u0026amp; \\ cor verde 0 \u0026amp; \\ cor azul 4 \\ \\ cor vermelho 9 \u0026amp; \\ cor verde 4 \u0026amp; \\ cor azul 2 \\ final { bmatrix} = \\ begin {bmatrix} \\ color {red} 4 \\ cdot \\ color {red} 4 + \\ color {red} 2 \\ cdot \\ color {red} 2 + \\ color {red} 0 \\ cdot \\ color {vermelho } 9 \u0026amp; \\ cor {vermelho} 4 \\ cdot \\ cor {verde} 2 + \\ cor {vermelho} 2 \\ cdot \\ cor {verde} 0 + \\ cor {vermelho} 0 \\ cdot \\ cor 4 \u0026amp; \\ cor verde {vermelho} 4 \\ cdot \\ cor azul 1 + \\ cor vermelho 2 \\ cdot \\ cor azul + 4 \\ cor vermelho 0 \\ cdot \\ cor azul 2 \\ \\ cor verde 0 \\ cdot \\ cor {vermelho} 4 + \\ cor {verde} 8 \\ cdot \\ cor {vermelho} 2 + \\ cor {verde} 1 \\ cdot \\ cor {vermelho} 9 \u0026amp; \\ cor {verde} 0 \\ cdot \\ cor { verde} 2 + \\ cor {verde} 8 \\ cdot \\ cor {verde} 0 + \\ cor {verde} 1 \\ cdot \\ cor {verde} 4 \u0026amp; \\ cor {verde} 0 \\ cdot \\ cor {azul} 1 + \\ cor {verde} 8 \\ cdot \\ color {azul} 4 + \\ color {verde} 1 \\ cdot \\ cor azul 2 \\ \\ cor azul 0 \\ cdot \\ cor vermelho + 4 \\ cor azul 1 \\ cdot \\ cor vermelho 2 + \\ cor azul 0 \\ cdot \\ cor {vermelho} 9 \u0026amp; \\ cor {azul} 0 \\ cdot \\ cor {verde} 2 + \\ cor {azul} 1 \\ cdot \\ cor {verde} 0 + \\ cor {azul} 0 \\ cdot \\ cor {verde} 4 \u0026amp; \\ cor {azul} 0 \\ cdot \\ cor {azul} 1 + \\ cor {azul} 1 \\ cdot \\ cor {azul} 4 + \\ cor {azul} 0 \\ cdot \\ cor {azul} 2 \\ final {bmatrix} \\ = \\ begin {bmatrix} 20 \u0026amp; 8 \u0026amp; 12 \\ 25 \u0026amp; 4 \u0026amp; 34 \\ 2 \u0026amp; 0 \u0026amp; 4 \\ final {bmatrix} ]\nComo você pode ver, a multiplicação de matrizes de matriz é um processo bastante complicado e muito propenso a erros (que é por isso que normalmente permitir que computadores fazer isso) e isso fica bem rápido problemático quando as matrizes se tornam maiores. Se você ainda está sedento por mais e você está curioso sobre mais algumas das propriedades matemáticas de matrizes eu sugiro fortemente que você dê uma olhada nestes vídeos Khan Academy cerca de matrizes.\n(https://www.khanacademy.org/math/algebra2/algebra-matrices)\nDe qualquer forma, agora que sabemos como matrizes multiplicar juntos, podemos começar a ficar para as coisas boas.\nMatrix-Vector multiplication Até agora nós tivemos nossa parcela de vetores. Nós utilizado para representar posições, cores e coordenadas, mesmo textura. Vamos passar um pouco mais abaixo do furo de coelho e dizer que um vector é basicamente uma matriz Nx1 onde N é o número do vector de componentes (também conhecido como um vector n-dimensional). Se você pensar sobre isso, faz muito sentido. Os vectores são como matrizes uma matriz de números, mas com apenas uma coluna. Então, como é este novo pedaço de informação ajuda-nos? Bem, se temos um MxN matriz podemos multiplicar esta matriz com o nosso vector Nx1, já que as colunas da matriz são iguais ao número de linhas do vetor, assim, a multiplicação de matrizes é definida.\nMas porque é que nós nos importamos se nós pode multiplicar matrizes com um vector? Bem, acontece que há muitas interessantes transformações 2D / 3D que pode colocar dentro de uma matriz, e multiplicando que a matriz com um vector, em seguida, transforma essa vetor. No caso de você ainda está um pouco confuso, vamos começar com alguns exemplos e em breve você vai ver o que queremos dizer.\nIdentity matrix Em OpenGL que normalmente trabalham com matrizes de transformação 4x4 por várias razões e uma delas é que a maioria dos vetores são de tamanho 4. A matriz de transformação mais simples que podemos pensar é a matriz identidade. A matriz de identidade é uma matriz NxN com apenas 0s excepto na sua diagonal. Como você verá, esta transformação de matriz folhas um vetor completamente ileso:\n\\ [\\ Begin {bmatrix} \\ color {red} 1 \u0026amp; \\ color {red} 0 \u0026amp; \\ color {red} 0 \u0026amp; \\ color {red} 0 \\ \\ color {verde} 0 \u0026amp; \\ color {verde} 1 \u0026amp; \\ color {verde} 0 \u0026amp; \\ color {verde} 0 \\ \\ color {azul} 0 \u0026amp; \\ color {azul} 0 \u0026amp; \\ color {azul} 1 \u0026amp; \\ color {azul} 0 \\ \\ color {roxo} 0 \u0026amp; \\ cor púrpura 0 \u0026amp; \\ cor púrpura 0 \u0026amp; \\ cor púrpura 1 \\ final {bmatrix} \\ cdot \\ begin {bmatrix} 1 2 \\ \\ \\ 3 4 \\ final {bmatrix} = \\ begin {bmatrix} \\ cor vermelho 1 \\ cdot 1 \\ \\ cor verde 1 \\ cdot 2 \\ \\ cor azul 1 \\ cdot 3 \\ \\ cor púrpura 1 \\ cdot 4 \\ final {bmatrix } = \\ begin {bmatrix} 1 2 \\ \\ \\ 3 4 \\ final {bmatrix} ]\nO vector é completamente intactas. Isto torna-se óbvio a partir das regras de multiplicação: o primeiro elemento é resultado de cada elemento individual da primeira linha da matriz multiplicado com cada elemento do vector. Uma vez que cada um dos elementos da linha são 0, exceto a primeira, obtemos: \\ (\\ color {red} 1 \\ cdot1 + \\ color {red} 0 \\ cdot2 + \\ color {red} 0 \\ cdot3 + \\ color {vermelho} 0 \\ cdot4 = 1 ) e o mesmo se aplica para os outros 3 elementos do vetor.\nVocê pode estar se perguntando o que o uso é de uma matriz de transformação que não transforma? A matriz de identidade é geralmente um ponto de partida para gerar outras matrizes de transformação e se cavar ainda mais fundo álgebra linear, uma matriz muito útil para demonstração de teoremas e resolução de equações lineares.\n Scaling Quando estamos escalar um vector estamos aumentando o comprimento da flecha pela quantidade que gostaríamos de escala, mantendo sua direção o mesmo. Uma vez que estamos a trabalhar em 2 ou 3 dimensões, podemos definir escala por um vector de 2 ou 3 variáveis ​​de escalonamento, cada um dimensionamento um eixo (x, y ou z).\nVamos tentar escalar o vector \\ (\\ color {vermelho} {\\ bar {v}} = (3,2) ). Vamos escalar do vector ao longo do eixo-x por 0,5, tornando-se, assim, duas vezes mais estreita; e vamos escalar do vector por dois ao longo do eixo y, tornando-se duas vezes mais alta. Vamos ver o que parece que se dimensionar o vetor por (0.5,2) como \\ (\\ color {azul} {\\ bar {s}} ):\n\nTenha em mente que OpenGL normalmente opera no espaço 3D para que para este caso 2D poderíamos definir a escala do eixo z a 1, deixando-a ilesa. A operação de escala que acabou de realizar uma escala não uniforme, porque o fator de escala não é o mesmo para cada eixo. Se o escalar seria igual em todos os eixos que seria chamado de uma escala uniforme.\nVamos começar a construir uma matriz de transformação que faz o escalonamento para nós. Vimos a partir da matriz de identidade que cada um dos elementos da diagonal foram multiplicados com o seu elemento de vector correspondente. E se mudássemos os 1s na matriz de identidade para 3s? Nesse caso, estaríamos multiplicando cada um dos elementos do vetor por um valor de 3 e assim, efetivamente uniformemente escalar o vector por 3. Se nós representam as variáveis ​​de escala como \\ ((\\ color {vermelho} {S_1}, \\ color { verde} {S_2}, \\ cor azul s_3) ) que podem definir uma matriz de dimensionamento em qualquer vector \\ ((x, y, z) ) como:\n\\ [\\ Begin {bmatrix} \\ color {vermelho} S_1 \u0026amp; \\ color {red} 0 \u0026amp; \\ color {red} 0 \u0026amp; \\ color {red} 0 \\ \\ color {verde} 0 \u0026amp; \\ color {verde} {S_2} e \\ color {verde} 0 \u0026amp; \\ color {verde} 0 \\ \\ color {azul} 0 \u0026amp; \\ color {azul} 0 \u0026amp; \\ color {azul} s_3 \u0026amp; \\ color {azul} 0 \\ \\ cor púrpura 0 \u0026amp; \\ cor púrpura 0 \u0026amp; \\ cor púrpura 0 \u0026amp; \\ cor púrpura 1 \\ final {bmatrix} \\ cdot \\ begin {pmatrix} x \\ Y \\ z \\ 1 \\ final {pmatrix} = \\ begin {pmatrix} \\ cor vermelho S_1 \\ cdot x \\ \\ cor verde S_2 \\ cdot y \\ \\ cor azul s_3 \\ cdot z \\ 1 \\ final {pmatrix} ]\nNote-se que mantemos o valor de escala 4º 1. O componente w é usado para outros fins, como veremos mais tarde.\nTranslation A tradução é o processo de adição de um outro vector no topo do vector original para voltar um novo vector com uma posição diferente, movendo-se assim o vector baseado em um vetor de translação. Nós já falamos sobre adição de vectores de modo que este não deve ser muito nova.\nAssim como a matriz de escala existem vários locais em uma matriz de 4 por 4, que podemos usar para executar determinadas operações e para a tradução esses são os 3 principais valores da 4ª coluna. Se nós representam o vector de tradução como \\ ((\\ cor vermelho T_x, \\ cor verde T_y, \\ cor azul T_z) ) podemos definir a matriz de tradução por:\n\\ [\\ Begin {bmatrix} \\ color {red} 1 \u0026amp; \\ color {red} 0 \u0026amp; \\ color {red} 0 \u0026amp; \\ color {vermelho} {T_x} \\ \\ color {verde} 0 \u0026amp; \\ color {verde} 1 \u0026amp; \\ color {verde} 0 \u0026amp; \\ color {verde} {T_y} \\ \\ color {azul} 0 \u0026amp; \\ color {azul} 0 \u0026amp; \\ color {azul} 1 \u0026amp; \\ color {azul} {T_z} \\ \\ cor púrpura 0 \u0026amp; \\ cor púrpura 0 \u0026amp; \\ cor púrpura 0 \u0026amp; \\ cor púrpura 1 \\ final {bmatrix} \\ cdot \\ begin {pmatrix} x \\ Y \\ z \\ 1 \\ final {pmatrix} = \\ begin {pmatrix} x + \\ cor vermelho T_x \\ y + \\ cor verde T_y \\ Z + \\ cor azul T_z \\ 1 \\ final { pmatrix} ]\nIsso funciona porque todos os valores de tradução são multiplicados por coluna w do vetor e acrescentou aos valores originais do vetor (lembre-se das regras matriz de multiplicação). Isto não teria sido possível com uma matriz 3-por-3.\nHomogeneous coordinates The w component of a vector is also known as a homogeneous coordinate. To get the 3D vector from a homogeneous vector we divide the x, y and z coordinate by its w coordinate. We usually do not notice this since the w component is 1.0 most of the time. Using homogeneous coordinates has several advantages: it allows us to do matrix translations on 3D vectors (without a w component we can't translate vectors) and in the next chapter we'll use the w value to create 3D perspective.\nAlso, whenever the homogeneous coordinate is equal to 0, the vector is specifically known as a direction vector since a vector with a w coordinate of 0 cannot be translated.\n Com uma matriz de tradução que pode mover objectos em qualquer dos 3 direcções dos eixos (x, y, z), tornando-se uma matriz de transformação muito útil para a nossa caixa de ferramentas de transformação.\nRotation Os últimos transformações eram relativamente fáceis de entender e visualizar em 2D ou 3D espaço, mas rotações são um pouco mais complicado. Se você quer saber exatamente como essas matrizes são construídos eu recomendo que você assista os itens de rotação de vídeos de álgebra linear da Khan Academy.\n(https://www.khanacademy.org/math/linear-algebra/matrix_transformations)\nPrimeiro vamos definir o que uma rotação de um vetor é realmente. Uma rotação em 2D ou 3D é representado com um ângulo. Um ângulo poderia estar em graus ou radianos onde um círculo completo tem 360 graus ou 2 radianos PI. Eu prefiro explicar rotações usando graus como estamos geralmente mais acostumado a eles.\nA maioria das funções de rotação requer um ângulo em radianos, mas graus felizmente são facilmente convertidos em radianos: ângulo em graus de ângulo em radianos = * (180 / PI) ângulo em radianos = ângulo em graus * (PI / 180) Onde PI é igual a (arredondada) 3,14159265359.\nRotativa roda metade de um círculo nos 360/2 = 180 graus de rotação e 1/5 para os meios adequados que rodam 360/5 = 72 graus para a direita. Isto é demonstrado por uma base 2D vector onde \\ (\\ cor {vermelho} {\\ bar {v}} ) é rodado de 72 graus para a direita, ou no sentido horário, a partir de \\ (\\ cor {verde} {\\ bar {k} } ):\n(http://en.wikipedia.org/wiki/Pi)\nA maioria das funções de rotação requer um ângulo em radianos, mas graus felizmente são facilmente convertidos em radianos: ângulo em graus de ângulo em radianos = * (180 / PI) ângulo em radianos = ângulo em graus * (PI / 180) Onde PI é igual a (arredondada) 3,14159265359.\n \nRotações em 3D são especificados com um ângulo e de um eixo de rotação. O ângulo específico, vai rodar o objecto ao longo do eixo de rotação dada. Tente visualizar esta girando a cabeça um certo grau enquanto continuamente olhando para baixo um eixo de rotação única. Ao rodar os vectores 2D num mundo 3D por exemplo, definir o eixo de rotação para o eixo z (tentam visualizar este).\nUsando a trigonometria, é possível transformar vectores para vectores recém giradas a um ângulo. Isso geralmente é feito através de uma combinação inteligente das funções seno e cosseno (comumente abreviado para o pecado e cos). A discussão de como as matrizes de rotação são gerados está fora do escopo deste capítulo.\nUma matriz de rotação é definido para cada eixo unidade no espaço 3D, onde o ângulo é representado como o símbolo teta \\ (\\ teta ).\nRotação em torno do eixo-X:\n\\ [\\ Begin {bmatrix} \\ color {red} 1 \u0026amp; \\ color {red} 0 \u0026amp; \\ color {red} 0 \u0026amp; \\ color {red} 0 \\ \\ color {verde} 0 \u0026amp; \\ color {verde} {\\ cos \\ theta} e - \\ color {verde} {\\ sin \\ theta} e \\ color {verde} 0 \\ \\ color {azul} 0 \u0026amp; \\ color {azul} {\\ sin \\ theta} e \\ color {azul} {\\ cos \\ theta} e \\ color {azul} 0 \\ \\ color {roxo} 0 \u0026amp; \\ color {roxo} 0 \u0026amp; \\ color {roxo} 0 \u0026amp; \\ color {roxo} 1 \\ end {bmatrix} \\ cdot \\ begin {pmatrix} x \\ Y \\ z \\ 1 \\ final {pmatrix} = \\ begin {pmatrix} x \\ \\ cor verde {\\ cos \\ teta} \\ cdot y - \\ cor verde {\\ sin \\ teta} \\ cdot z \\ \\ cor azul {\\ sin \\ teta} \\ cdot y + \\ cor azul {\\ cos \\ teta} \\ cdot z \\ 1 \\ final {pmatrix} ]\nRotação em torno do eixo Y:\n\\ [\\ Begin {bmatrix} \\ color {vermelho} {\\ cos \\ theta} e \\ color {red} 0 \u0026amp; \\ color {vermelho} {\\ sin \\ theta} e \\ color {red} 0 \\ \\ color {verde } 0 \u0026amp; \\ color {verde} 1 \u0026amp; \\ color {verde} 0 \u0026amp; \\ color {verde} 0 \\ - \\ color {azul} {\\ sin \\ theta} e \\ color {azul} 0 \u0026amp; \\ color {azul} {\\ cos \\ theta} e \\ color {azul} 0 \\ \\ color {roxo} 0 \u0026amp; \\ color {roxo} 0 \u0026amp; \\ color {roxo} 0 \u0026amp; \\ color {roxo} 1 \\ end {bmatrix} \\ cdot \\ begin {pmatrix} x \\ Y \\ z \\ 1 \\ final {pmatrix} = \\ begin {pmatrix} \\ cor vermelho {\\ cos \\ teta} \\ cdot x + \\ cor vermelho {\\ sin \\ teta } \\ cdot z \\ Y \\ - \\ cor azul {\\ sin \\ teta} \\ cdot x + \\ cor azul {\\ cos \\ teta} \\ cdot z \\ 1 \\ final {pmatrix} ]\nRotação em torno do eixo Z:\n\\ [\\ Begin {bmatrix} \\ color {vermelho} {\\ cos \\ theta} e - \\ color {vermelho} {\\ sin \\ theta} e \\ color {red} 0 \u0026amp; \\ color {red} 0 \\ \\ color { verde} {\\ sin \\ theta} e \\ color {verde} {\\ cos \\ theta} e \\ color {verde} 0 \u0026amp; \\ color {verde} 0 \\ \\ color {azul} 0 \u0026amp; \\ color {azul} 0 \u0026amp; \\ color {azul} 1 \u0026amp; \\ color {azul} 0 \\ \\ color {roxo} 0 \u0026amp; \\ color {roxo} 0 \u0026amp; \\ color {roxo} 0 \u0026amp; \\ color {roxo} 1 \\ end {bmatrix} \\ cdot \\ begin {pmatrix} x \\ Y \\ z \\ 1 \\ final {pmatrix} = \\ begin {pmatrix} \\ cor vermelho {\\ cos \\ teta} \\ cdot x - \\ cor vermelho {\\ sin \\ teta } \\ cdot y \\ \\ cor verde {\\ sin \\ teta} \\ cdot x + \\ cor verde {\\ cos \\ teta} \\ cdot y \\ z \\ 1 \\ final {pmatrix} ]\nUsando a rotação matrizes podemos transformar os nossos vetores posição em torno de um dos três eixos de unidade. Para girar em torno de um eixo 3D arbitrária podemos combinar todos 3 eles, em primeiro lugar girar em torno do eixo X, em seguida, Y e Z, em seguida, por exemplo. No entanto, este rapidamente introduz um problema chamado bloqueio cardan. Não vamos discutir os detalhes, mas a melhor solução é para girar em torno de uma unidade arbitrária eixo por exemplo (0.662,0.2,0.722) (note que este é um vetor unitário) de imediato, em vez de combinar as matrizes de rotação. Tal (detalhado) existe matriz e é dada abaixo com \\ ((\\ cor {vermelho} {R_x}, \\ cor {verde} {R_y}, \\ cor {azul} {R_z}) ) como o eixo de rotação arbitrária:\n\\ [\\ Begin {bmatrix} \\ cos \\ theta + \\ color {vermelho} {R_x} ^ 2 (1 - \\ cos \\ theta) \u0026amp; \\ color {vermelho} {R_x} \\ color {verde} {R_y} (1 - \\ cos \\ theta) - \\ color {azul} {R_z} \\ sin \\ theta \u0026amp; \\ color {vermelho} {R_x} \\ color {azul} {R_z} (1 - \\ cos \\ theta) + \\ color {verde} { R_y} \\ sin \\ theta \u0026amp; 0 \\\\ \\ color {verde} {R_y} \\ color {vermelho} {R_x} (1 - \\ cos \\ theta) + \\ color {azul} {R_z} \\ sin \\ theta \u0026amp; \\ cos \\ teta + \\ cor {verde} {R_y} ^ 2 (1 - \\ cos \\ teta) {verde} {R_y} \\ cor {azul} {R_z} \u0026amp; \\ cor (1 - \\ cos \\ teta) - \\ cor { vermelho} {R_x} \\ sin \\ theta \u0026amp; 0 \\\\ \\ color {azul} {R_z} \\ color {vermelho} {R_x} (1 - \\ cos \\ theta) - \\ color {verde} {R_y} \\ sin \\ theta \u0026amp; \\ color {azul} {R_z} \\ color {verde} {R_y} (1 - \\ cos \\ theta) + \\ color {vermelho} {R_x} \\ sin \\ theta \u0026amp; \\ cos \\ theta + \\ color {azul} { R_z} ^ 2 (1 - \\ cos \\ teta) \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\ final {bmatrix} \\] Uma discussão matemático de gerar tal matriz um está fora do âmbito do presente capítulo. Tenha em mente que mesmo essa matriz não impedir completamente bloqueio cardan (embora fica muito mais difícil). Para evitar que verdadeiramente Gimbal bloqueia temos para representar rotações usando quaternions, que não só são mais seguros, mas também computacionalmente mais amigável. No entanto, uma discussão sobre quaternions está fora do escopo deste capítulo. Combining matrices O verdadeiro poder de usar matrizes para transformações é que podemos combinar várias transformações em uma única matriz, graças à multiplicação de matrizes-matriz. Vamos ver se podemos gerar uma matriz de transformação que combina várias transformações. Digamos que temos um vector (x, y, z) e que queremos dimensioná-lo por 2 e, em seguida, traduzi-lo por (1,2,3). Precisamos de uma tradução e uma matriz de escala para nossos passos necessários. A matriz de transformação resultante seria semelhante:\n \\ [Trans. Escala = \\ begin {bmatrix} \\ color {red} 1 \u0026amp; \\ color {red} 0 \u0026amp; \\ color {red} 0 \u0026amp; \\ color {red} 1 \\\\ \\ color {verde} 0 \u0026amp; \\ color {verde} 1 \u0026amp; \\ cor verde {} 0 \u0026amp; \\ cor verde {} 2 \\\\ \\ cor azul {} 0 \u0026amp; \\ cor azul {} 0 \u0026amp; \\ cor azul {} 1 \u0026amp; \\ cor azul {} 3 \\\\ \\ cor púrpura {} 0 \u0026amp; \\ color {roxo} 0 \u0026amp; \\ color {roxo} 0 \u0026amp; \\ color {roxo} 1 \\ end {bmatrix}. \\ Begin {bmatrix} \\ color {red} 2 \u0026amp; \\ color {red} 0 \u0026amp; \\ color {red} 0 \u0026amp; \\ color {red} 0 \\\\ \\ color {verde} 0 \u0026amp; \\ color {verde} 2 \u0026amp; \\ color {verde} 0 \u0026amp; \\ color {verde} 0 \\\\ \\ color {azul} 0 \u0026amp; \\ color {azul} 0 \u0026amp; \\ color {azul} 2 \u0026amp; \\ color {azul} 0 \\\\ \\ color {roxo} 0 \u0026amp; \\ cor {roxo} 0 \u0026amp; \\ color {roxo} 0 \u0026amp; \\ color {roxo} 1 \\ end {bmatrix} = \\ begin {bmatrix} \\ color {red} 2 \u0026amp; \\ color {red} 0 \u0026amp; \\ color {red} 0 \u0026amp; \\ color {red} 1 \\\\ \\ color {verde} 0 \u0026amp; \\ color {verde} 2 \u0026amp; \\ color {verde} 0 \u0026amp; \\ color {verde} 2 \\\\ \\ color {azul} 0 \u0026amp; \\ color {azul} 0 \u0026amp; \\ cor azul {} 2 \u0026amp; \\ cor azul {} 3 \\\\ \\ cor púrpura {} 0 \u0026amp; \\ cor púrpura {} 0 \u0026amp; \\ cor púrpura {} 0 \u0026amp; \\ cor púrpura {} 1 \\ final {bmatrix} \\ ] Note que primeiro fazer uma tradução e, em seguida, uma transformação de escala ao multiplicar matrizes. A multiplicação de matrizes não é comutativa, o que significa que sua ordem é importante. Ao multiplicar matrizes mais à direita da matriz é multiplicado primeiro com o vector de modo que você deve ler as multiplicações da direita para a esquerda. É aconselhável primeiro fazer operações, em seguida, rotações e, por último traduções quando combinando matrizes caso contrário eles podem (negativamente) afetam uns aos outros de escala. Por exemplo, se você primeiro fazer uma tradução e, em seguida, escala, o vetor de translação também escalável! Executando a matriz de transformação final sobre os resultados do vetor no seguinte vetor:\n\\ [\\ Begin {bmatrix} \\ color {red} 2 \u0026amp; \\ color {red} 0 \u0026amp; \\ color {red} 0 \u0026amp; \\ color {red} 1 \\ \\ color {verde} 0 \u0026amp; \\ color {verde} 2 \u0026amp; \\ color {verde} 0 \u0026amp; \\ color {verde} 2 \\ \\ color {azul} 0 \u0026amp; \\ color {azul} 0 \u0026amp; \\ color {azul} 2 \u0026amp; \\ color {azul} 3 \\ \\ color {roxo} 0 \u0026amp; \\ color {roxo} 0 \u0026amp; \\ color {roxo} 0 \u0026amp; \\ color {roxo} 1 \\ end {bmatrix}. \\ Begin {bmatrix} x \\ Y \\ z \\ 1 \\ final {bmatrix} = \\ begin {bmatrix} \\ cor vermelho 2x + \\ cor vermelho 1 \\ \\ cor verde 2y + \\ cor {verde} 2 \\ \\ cor azul + 2z \\ cor azul 3 \\ 1 \\ final {bmatrix} ]\nÓtimo! O vector é primeiramente escalonado por dois e, em seguida, traduzido por (1,2,3).\nIn practice Agora que nós explicamos toda a teoria por trás de transformações, é hora de ver como nós podemos realmente usar esse conhecimento para nossa vantagem. OpenGL não tem qualquer forma de conhecimento matriz ou vector construído em, por isso temos de definir nossas próprias aulas de matemática e funções. Neste livro, prefiro abstrair de todos os pequenos detalhes matemáticos e simplesmente usar pré-fabricados bibliotecas de matemática. Felizmente, há um fácil de usar e adaptados-for-OpenGL matemática biblioteca chamada GLM.\nGLM GLM significa OpenGL Matemática e é uma biblioteca somente cabeçalho, o que significa que só temos de incluir os arquivos de cabeçalho adequados e estamos a fazer; sem vinculação e compilar necessário. GLM pode ser baixado de seu site. Copie o diretório raiz dos arquivos de cabeçalho em seu inclui pasta e vamos rolar.\n\n(https://glm.g-truc.net/0.9.8/index.html)\nA maioria das funcionalidades do GLM que precisamos podem ser encontrados em arquivos de 3 cabeçalhos que vamos incluir os seguintes:\n#include \u0026lt;glm/glm.hpp\u0026gt;#include \u0026lt;glm/gtc/matrix_transform.hpp\u0026gt;#include \u0026lt;glm/gtc/type_ptr.hpp\u0026gt; Vamos ver se podemos colocar nosso conhecimento transformação para uma boa utilização, traduzindo um vetor de (1,0,0) por (1,1,0) (note que nós defini-lo como um glm :: vec4 com seu coordenadas homogêneas definido para 1.0:\nglm::vec4 vec(1.0f, 0.0f, 0.0f, 1.0f); glm::mat4 trans = glm::mat4(1.0f); trans = glm::translate(trans, glm::vec3(1.0f, 1.0f, 0.0f)); vec = trans * vec; std::cout \u0026lt;\u0026lt; vec.x \u0026lt;\u0026lt; vec.y \u0026lt;\u0026lt; vec.z \u0026lt;\u0026lt; std::endl;  Nós primeiro definir um vetor chamado vec usando a classe vector built-in da GLM. Em seguida, definir uma mat4 e inicializar explicitamente à matriz identidade por inicializar diagonais da matriz a 1,0; se não inicializá-lo para a matriz identidade a matriz seria uma matriz nula (todos os elementos 0) e todas as operações da matriz subseqüentes iria acabar uma matriz nula também.\nO próximo passo é para criar uma matriz de transformação, passando a matriz de identidade com a GLM :: traduzir função, juntamente com um vector de tradução (dada matriz é então multiplicado por uma matriz de tradução e a matriz resultante é devolvido).\nDepois multiplicamos nosso vector pela matriz de transformação e de saída o resultado. Se ainda me lembro como tradução matriz funciona, então o vetor resultante deve ser (1 + 1,0 + 1,0 + 0) que é (2,1,0). Este trecho de código gera 210 de modo a matriz de tradução fez o seu trabalho. Vamos fazer algo mais interessante e escala e girar o objeto recipiente do capítulo anterior:\nglm::mat4 trans = glm::mat4(1.0f); trans = glm::rotate(trans, glm::radians(90.0f), glm::vec3(0.0, 0.0, 1.0)); trans = glm::scale(trans, glm::vec3(0.5, 0.5, 0.5));  Em primeiro lugar, dimensionar o recipiente de 0,5 em cada um dos eixos e em seguida rodar o recipiente 90 graus em torno do eixo Z. GLM espera que seus ângulos em radianos para que converter os graus em radianos usando glm :: radianos. Note que o retângulo textura é no plano XY por isso queremos girar em torno do eixo Z. Tenha em mente que o eixo que gira em torno deve ser um vetor de unidade, por isso não deixe para normalizar o vetor primeiro se você não está girando em torno do X, Y ou eixo Z. Porque nós passar a matriz para cada uma das funções do GLM, GLM automaticamente múltiplos as matrizes em conjunto, resultando em uma matriz de transformação que combina todas as transformações.\nA próxima grande questão é: como é que vamos chegar a matriz de transformação para os shaders? Nós brevemente mencionado antes que GLSL também tem um tipo mat4. Então, vamos adaptar o shader de vértice para aceitar uma variável mat4 uniforme e multiplicar o vetor posição pelo uniforme de matriz:\n#version 330 core layout (location = 0) in vec3 aPos; layout (location = 1) in vec2 aTexCoord; out vec2 TexCoord; uniform mat4 transform; void main() { gl_Position = transform * vec4(aPos, 1.0f); TexCoord = vec2(aTexCoord.x, aTexCoord.y); }  GLSL também tem mat2 e mat3 tipos que permitem swizzling-like operações apenas como vetores. Todas as operações matemáticas acima mencionados (tais como multiplicação escalar da matriz, a multiplicação de matrizes do vector e multiplicação matriz-matriz) são permitidos nos tipos de matriz. Onde quer que são utilizadas operações com matrizes especiais, vamos ter certeza de explicar o que está acontecendo.\n Adicionamos a uniforme e multiplicou-se o vector de posição com a matriz de transformação antes de passá-lo para gl_Position. O nosso recipiente deve agora ser duas vezes mais pequena e rodado 90 graus (inclinado para a esquerda). Nós ainda precisamos de passar a matriz de transformação para o shader no entanto:\nunsigned int transformLoc = glGetUniformLocation(ourShader.ID, \u0026#34;transform\u0026#34;); glUniformMatrix4fv(transformLoc, 1, GL_FALSE, glm::value_ptr(trans));  Em primeiro lugar, consultar a localização da variável uniforme e, em seguida, enviar os dados da matriz para os shaders utilizando glUniform com Matrix4fv como postfix. O primeiro argumento deve ser familiar até agora, que é a localização do uniforme. O segundo argumento diz ao OpenGL quantas matrizes gostaríamos de enviar, que é 1. O terceiro argumento nos pergunta se queremos transpor nossa matriz, que é trocar as colunas e linhas. desenvolvedores OpenGL costumam usar um layout de matriz interna chamada coluna-major ordenação que é o layout da matriz padrão no GLM por isso não há necessidade de transpor as matrizes; podemos mantê-lo em GL_FALSE. O último parâmetro é os dados da matriz reais, mas GLM armazena os dados de seus matrizes de uma maneira que nem sempre correspondem às expectativas do OpenGL para que primeiro converte os dados com value_ptr função built-in da GLM.\nNós criamos uma matriz de transformação, declarou um uniforme no shader de vértice e enviou a matriz para os shaders em que transformam nossas coordenadas dos vértices. O resultado deve ser algo como isto:\n\nPerfeito! Nosso recipiente é realmente inclinado para a esquerda e duas vezes mais pequeno para que a transformação foi bem sucedida. Vamos ficar um pouco mais funky e ver se podemos rodar o recipiente ao longo do tempo, e para se divertir também vamos reposicionar o recipiente no lado inferior direito da janela. Para rodar o recipiente ao longo do tempo nós temos que atualizar a matriz de transformação no circuito tornar porque ele precisa atualizar cada quadro. Nós usamos a função do tempo de GLFW para obter um ângulo ao longo do tempo:\nglm::mat4 trans = glm::mat4(1.0f); trans = glm::translate(trans, glm::vec3(0.5f, -0.5f, 0.0f)); trans = glm::rotate(trans, (float)glfwGetTime(), glm::vec3(0.0f, 0.0f, 1.0f));  Tenha em mente que no caso anterior, poderíamos declarar a qualquer lugar matriz de transformação, mas agora temos que criá-lo cada iteração para atualizar continuamente a rotação. Isto significa que temos para recriar a matriz de transformação em cada iteração do circuito de renda. Normalmente, quando o processamento de cenas temos várias matrizes de transformação que são recriados com novos valores cada quadro.\nAqui podemos rodar o recipiente em torno da origem (0,0,0) e uma vez que é rodado, traduzimos a sua versão rodada para o canto inferior direito da tela. Lembre-se que a ordem de transformação real deve ser lida ao contrário: mesmo que no código que primeiro traduzir e depois girar, as transformações reais primeiro aplicar uma rotação e uma tradução. Compreender todas estas combinações de transformações e como se aplicam a objetos é difícil de entender. Experimente e experimentar transformações como estas e você vai rapidamente ter uma idéia dela.\nSe você fez as coisas direito, você deve obter o seguinte resultado:\n\nE aí está. Um recipiente traduzido desse girado vez mais, tudo feito por uma única matriz de transformação! Agora você pode ver porque matrizes são uma construção tão poderosa na terra gráficos. Podemos definir uma quantidade infinita de transformações e combiná-los todos em uma única matriz que podemos voltar a usar tão frequentemente como gostaríamos. Usando transformações como este no shader de vértice nos salva o esforço de definição de re-os dados de vértice e nos poupa algum tempo de processamento, bem como, uma vez que não tem que re-enviar os nossos dados o tempo todo (que é bastante lento); tudo o que precisamos fazer é atualizar o uniforme transformação.\nSe você não obter o resultado correto ou se você estiver em algum lugar preso outra coisa, dê uma olhada no código fonte ea classe shader atualizado.\n(/code_viewer_gh.php?code=src/1.getting_started/5.1.transformations/transformations.cpp)\n(https://learnopengl.com/code_viewer_gh.php?code=includes/learnopengl/shader_m.h)\nNo próximo capítulo vamos discutir como podemos usar matrizes para definir diferentes espaços de coordenadas para os nossos vértices. Este será o nosso primeiro passo para gráficos 3D!\nFurther reading Essência da Álgebra Linear: grande série de vídeo tutorial por Grant Sanderson sobre a matemática subjacente de transformações e álgebra linear.\n(https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)\nExercises Utilizando a última transformação no recipiente, tente trocar a ordem em torno de pelo primeiro rotativo e, em seguida, a tradução. Veja o que acontece e tentar razão por que isso acontece: solução.\n(/code_viewer_gh.php?code=src/1.getting_started/5.2.transformations_exercise1/transformations_exercise1.cpp)\nTente desenhar um segundo recipiente com outra chamada para glDrawElements mas colocá-lo em uma posição diferentes, utilizando apenas transformações. Certifique-se que este segundo recipiente é colocado no canto superior esquerdo da janela e, em vez de girar, escalá-lo ao longo do tempo (usando a função pecado é útil aqui, nota que o uso de pecado fará com que o objeto para invertido assim que uma escala negativa é aplicada): solução.\n(/code_viewer_gh.php?code=src/1.getting_started/5.2.transformations_exercise2/transformations_exercise2.cpp)\n"
},
{
	"uri": "https://filipecn.github.io/aprendaopengl/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://filipecn.github.io/aprendaopengl/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]